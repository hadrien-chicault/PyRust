{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#pyrust-documentation","title":"PyRust Documentation","text":"<p>Welcome to PyRust - a high-performance, Rust-based implementation of PySpark that delivers 10-50x faster data processing while maintaining API compatibility.</p>"},{"location":"#what-is-pyrust","title":"What is PyRust?","text":"<p>PyRust brings the power of Rust to Python data processing by leveraging:</p> <ul> <li>Apache DataFusion - High-performance query engine</li> <li>Apache Arrow - Columnar memory format</li> <li>PyO3 - Zero-copy Python bindings</li> </ul>"},{"location":"#key-features","title":"Key Features","text":""},{"location":"#high-performance","title":"\u26a1 High Performance","text":"<ul> <li>10-50x faster than PySpark for common operations</li> <li>Optimized columnar processing with vectorized operations</li> <li>Lazy evaluation with automatic query optimization</li> </ul>"},{"location":"#type-safe","title":"\ud83d\udd12 Type Safe","text":"<ul> <li>Leverages Rust's type system for safer data processing</li> <li>Compile-time guarantees prevent common runtime errors</li> <li>Strong schema validation</li> </ul>"},{"location":"#drop-in-replacement","title":"\ud83d\udd0c Drop-in Replacement","text":"<ul> <li>Familiar PySpark-like API</li> <li>Minimal code changes required</li> <li>Easy migration path</li> </ul>"},{"location":"#memory-efficient","title":"\ud83d\udcbe Memory Efficient","text":"<ul> <li>Apache Arrow's columnar format reduces memory usage</li> <li>Zero-copy data sharing between Python and Rust</li> <li>Efficient predicate pushdown and column pruning</li> </ul>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>from pyrust import SparkSession\n\n# Create a session\nspark = SparkSession.builder() \\\n    .appName(\"MyApp\") \\\n    .getOrCreate()\n\n# Read data\ndf = spark.read.csv(\"sales.csv\", header=True)\n\n# Transform and analyze\nresult = df.filter(\"amount &gt; 1000\") \\\n           .groupBy([\"region\"]) \\\n           .agg([(\"amount\", \"sum\"), (\"amount\", \"avg\")]) \\\n           .orderBy([\"sum(amount)\"])\n\n# Display results\nresult.show()\n</code></pre>"},{"location":"#performance-comparison","title":"Performance Comparison","text":"Operation PySpark PyRust Speedup CSV Read (1GB) 12.5s 1.2s 10.4x Filter + Select 8.3s 0.4s 20.8x GroupBy + Count 15.2s 0.8s 19.0x Complex Query 45.6s 2.1s 21.7x <p>Benchmarks run on: Intel Core i7, 16GB RAM, NVMe SSD</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Ready to try PyRust? Check out our guides:</p> <ul> <li>Installation Guide - Install PyRust on your system</li> <li>Quick Start Tutorial - Your first PyRust program</li> <li>Basic Operations - Common data operations</li> </ul>"},{"location":"#documentation-sections","title":"Documentation Sections","text":""},{"location":"#getting-started_1","title":"\ud83d\udcd6 Getting Started","text":"<p>Installation, setup, and your first PyRust program.</p>"},{"location":"#user-guide","title":"\ud83d\udcda User Guide","text":"<p>In-depth guides on DataFrames, transformations, and best practices.</p>"},{"location":"#api-reference","title":"\ud83d\udd27 API Reference","text":"<p>Complete API documentation for all classes and methods.</p>"},{"location":"#architecture","title":"\ud83c\udfd7\ufe0f Architecture","text":"<p>Deep dive into PyRust's internals and design decisions.</p>"},{"location":"#development","title":"\ud83d\udc68\u200d\ud83d\udcbb Development","text":"<p>Contributing guidelines, building from source, and testing.</p>"},{"location":"#community-support","title":"Community &amp; Support","text":"<ul> <li>GitHub: hadrien-chicault/PyRust</li> <li>Issues: Report bugs or request features</li> <li>\ud83e\udd80 Rust API Docs: Rust implementation documentation - Low-level internals</li> </ul>"},{"location":"#related-documentation","title":"Related Documentation","text":"<ul> <li>Rust API Reference - Detailed documentation of the Rust implementation</li> <li>Apache DataFusion - Query engine powering PyRust</li> <li>Apache Arrow - Columnar format for efficient data processing</li> </ul>"},{"location":"#license","title":"License","text":"<p>PyRust is open source software licensed under the Apache License 2.0.</p>"},{"location":"getting-started/basic-operations/","title":"Basic Operations","text":"<p>This guide covers the fundamental operations you'll use in PyRust.</p>"},{"location":"getting-started/basic-operations/#reading-data","title":"Reading Data","text":""},{"location":"getting-started/basic-operations/#csv-files","title":"CSV Files","text":"<pre><code># Basic CSV read\ndf = spark.read.csv(\"data.csv\")\n\n# With options\ndf = spark.read.csv(\n    \"data.csv\",\n    header=True,        # First row contains column names\n    infer_schema=True   # Automatically detect column types\n)\n</code></pre> <p>CSV Options: - <code>header</code> (bool): Whether first row is header (default: <code>True</code>) - <code>infer_schema</code> (bool): Automatically detect types (default: <code>True</code>)</p>"},{"location":"getting-started/basic-operations/#parquet-files","title":"Parquet Files","text":"<pre><code># Read Parquet file\ndf = spark.read.parquet(\"data.parquet\")\n\n# Read directory of Parquet files\ndf = spark.read.parquet(\"data_dir/\")\n</code></pre> <p>Why Parquet? - 5-10x smaller file size - 10-100x faster reads for selective queries - Schema preserved (no inference needed) - Built-in compression and encoding</p>"},{"location":"getting-started/basic-operations/#inspecting-data","title":"Inspecting Data","text":""},{"location":"getting-started/basic-operations/#view-schema","title":"View Schema","text":"<pre><code># Print schema in tree format\ndf.printSchema()\n\n# Get schema as string\nschema_str = df.schema()\nprint(schema_str)\n</code></pre>"},{"location":"getting-started/basic-operations/#display-rows","title":"Display Rows","text":"<pre><code># Show first 20 rows (default)\ndf.show()\n\n# Show specific number of rows\ndf.show(5)\ndf.show(100)\n\n# Count total rows\ntotal = df.count()\nprint(f\"Total rows: {total}\")\n</code></pre>"},{"location":"getting-started/basic-operations/#selecting-columns","title":"Selecting Columns","text":""},{"location":"getting-started/basic-operations/#select-specific-columns","title":"Select Specific Columns","text":"<pre><code># Select single column\nnames = df.select([\"name\"])\n\n# Select multiple columns\nsubset = df.select([\"name\", \"age\", \"city\"])\n\n# Reorder columns\nreordered = df.select([\"city\", \"name\", \"age\"])\n</code></pre>"},{"location":"getting-started/basic-operations/#column-operations","title":"Column Operations","text":"<pre><code># Select all columns (rarely needed)\nall_cols = df.select(df.columns)  # TODO: Not yet implemented\n\n# Drop columns\n# TODO: Not yet implemented in current version\n</code></pre>"},{"location":"getting-started/basic-operations/#filtering-rows","title":"Filtering Rows","text":""},{"location":"getting-started/basic-operations/#simple-filters","title":"Simple Filters","text":"<pre><code># Single condition\nadults = df.filter(\"age &gt;= 18\")\nseniors = df.filter(\"age &gt;= 65\")\n\n# Numeric comparisons\nhigh_earners = df.filter(\"salary &gt; 100000\")\nyoung_adults = df.filter(\"age &gt; 18\")\n</code></pre>"},{"location":"getting-started/basic-operations/#supported-operators","title":"Supported Operators","text":"Operator Description Example <code>&gt;</code> Greater than <code>age &gt; 18</code> <code>&lt;</code> Less than <code>age &lt; 65</code> <code>&gt;=</code> Greater or equal <code>salary &gt;= 50000</code> <code>&lt;=</code> Less or equal <code>age &lt;= 30</code> <code>==</code> Equal <code>status == 'active'</code> <code>!=</code> Not equal <code>city != 'Unknown'</code>"},{"location":"getting-started/basic-operations/#string-filters","title":"String Filters","text":"<pre><code># String equality (with quotes)\nvip_customers = df.filter(\"status == 'VIP'\")\nny_users = df.filter(\"city == 'New York'\")\n\n# String inequality\nactive = df.filter(\"status != 'inactive'\")\n</code></pre>"},{"location":"getting-started/basic-operations/#sorting-data","title":"Sorting Data","text":""},{"location":"getting-started/basic-operations/#sort-ascending","title":"Sort Ascending","text":"<pre><code># Sort by single column\nsorted_df = df.orderBy([\"age\"])\n\n# Sort by multiple columns (priority: left to right)\nsorted_df = df.orderBy([\"country\", \"city\", \"name\"])\n\n# Alternative: sort() method (identical)\nsorted_df = df.sort([\"age\"])\n</code></pre>"},{"location":"getting-started/basic-operations/#sorting-tips","title":"Sorting Tips","text":"<ul> <li>Nulls are sorted first by default</li> <li>Descending sort not yet implemented (future feature)</li> <li>Combine with <code>limit()</code> for \"top N\" queries</li> </ul>"},{"location":"getting-started/basic-operations/#limiting-results","title":"Limiting Results","text":""},{"location":"getting-started/basic-operations/#limit-rows","title":"Limit Rows","text":"<pre><code># First 10 rows\npreview = df.limit(10)\n\n# Top 5 after sorting\ntop_5 = df.orderBy([\"sales\"]).limit(5)\n\n# Sample for testing\ntest_df = df.limit(1000)\n</code></pre> <p>Use Cases: - Quick data preview - Top N queries - Testing transformations on small subset - Reducing output size</p>"},{"location":"getting-started/basic-operations/#grouping-and-aggregation","title":"Grouping and Aggregation","text":""},{"location":"getting-started/basic-operations/#simple-count","title":"Simple Count","text":"<pre><code># Count by single column\ncity_counts = df.groupBy([\"city\"]).count()\ncity_counts.show()\n</code></pre>"},{"location":"getting-started/basic-operations/#multiple-aggregations","title":"Multiple Aggregations","text":"<pre><code># Aggregate functions: count, sum, avg, min, max\nstats = df.groupBy([\"department\"]).agg([\n    (\"salary\", \"avg\"),\n    (\"salary\", \"min\"),\n    (\"salary\", \"max\"),\n    (\"employee_id\", \"count\")\n])\nstats.show()\n</code></pre>"},{"location":"getting-started/basic-operations/#group-by-multiple-columns","title":"Group by Multiple Columns","text":"<pre><code># Multi-level grouping\nsummary = df.groupBy([\"country\", \"city\"]).agg([\n    (\"population\", \"sum\"),\n    (\"age\", \"avg\")\n])\nsummary.show()\n</code></pre>"},{"location":"getting-started/basic-operations/#available-aggregation-functions","title":"Available Aggregation Functions","text":"Function Description Example <code>count</code> Count rows <code>(\"id\", \"count\")</code> <code>sum</code> Sum values <code>(\"amount\", \"sum\")</code> <code>avg</code> / <code>mean</code> Average <code>(\"age\", \"avg\")</code> <code>min</code> Minimum <code>(\"salary\", \"min\")</code> <code>max</code> Maximum <code>(\"salary\", \"max\")</code>"},{"location":"getting-started/basic-operations/#chaining-operations","title":"Chaining Operations","text":""},{"location":"getting-started/basic-operations/#transformation-chain","title":"Transformation Chain","text":"<p>Operations can be chained for complex workflows:</p> <pre><code>result = df.select([\"name\", \"age\", \"city\", \"salary\"]) \\\n           .filter(\"age &gt; 25\") \\\n           .filter(\"salary &gt; 50000\") \\\n           .orderBy([\"salary\"]) \\\n           .limit(100)\n</code></pre>"},{"location":"getting-started/basic-operations/#lazy-evaluation","title":"Lazy Evaluation","text":"<p>Transformations are lazy - they don't execute until an action is called:</p> <pre><code># These are lazy (just build query plan)\nfiltered = df.filter(\"age &gt; 18\")\nselected = filtered.select([\"name\", \"city\"])\nsorted_df = selected.orderBy([\"name\"])\n\n# This triggers execution\nsorted_df.show()  # Action!\n</code></pre> <p>Actions that trigger execution: - <code>show()</code> - Display rows - <code>count()</code> - Count rows - <code>collect()</code> - Future: return all data to Python</p>"},{"location":"getting-started/basic-operations/#complete-examples","title":"Complete Examples","text":""},{"location":"getting-started/basic-operations/#example-1-data-cleaning","title":"Example 1: Data Cleaning","text":"<pre><code># Remove invalid rows and select relevant columns\nclean_df = df.filter(\"age &gt; 0\") \\\n             .filter(\"age &lt; 120\") \\\n             .select([\"name\", \"age\", \"email\"]) \\\n             .orderBy([\"age\"])\n\nprint(f\"Valid records: {clean_df.count()}\")\nclean_df.show(10)\n</code></pre>"},{"location":"getting-started/basic-operations/#example-2-sales-analysis","title":"Example 2: Sales Analysis","text":"<pre><code># Analyze sales by region\nsales_summary = df.filter(\"amount &gt; 0\") \\\n                  .groupBy([\"region\"]) \\\n                  .agg([\n                      (\"amount\", \"sum\"),\n                      (\"amount\", \"avg\"),\n                      (\"transaction_id\", \"count\")\n                  ]) \\\n                  .orderBy([\"sum(amount)\"])\n\nprint(\"Sales Summary by Region:\")\nsales_summary.show()\n</code></pre>"},{"location":"getting-started/basic-operations/#example-3-user-segmentation","title":"Example 3: User Segmentation","text":"<pre><code># Find active users in top cities\nactive_users = df.filter(\"last_login_days &lt; 30\") \\\n                 .filter(\"age &gt;= 18\") \\\n                 .groupBy([\"city\"]) \\\n                 .count() \\\n                 .orderBy([\"count\"]) \\\n                 .limit(10)\n\nprint(\"Top 10 Cities by Active Users:\")\nactive_users.show()\n</code></pre>"},{"location":"getting-started/basic-operations/#best-practices","title":"Best Practices","text":""},{"location":"getting-started/basic-operations/#1-filter-early","title":"1. Filter Early","text":"<pre><code># Good\ndf.filter(\"age &gt; 18\").select([\"name\", \"city\"]).groupBy([\"city\"]).count()\n\n# Less efficient\ndf.groupBy([\"city\"]).count().filter(\"count &gt; 100\")\n</code></pre>"},{"location":"getting-started/basic-operations/#2-select-only-needed-columns","title":"2. Select Only Needed Columns","text":"<pre><code># Good\ndf.select([\"name\", \"age\"]).filter(\"age &gt; 18\")\n\n# Less efficient (processes all columns)\ndf.filter(\"age &gt; 18\")\n</code></pre>"},{"location":"getting-started/basic-operations/#3-use-limit-for-exploration","title":"3. Use Limit for Exploration","text":"<pre><code># Quick preview\ndf.limit(100).show()\n\n# Test on subset\ndf.limit(10000).filter(\"age &gt; 18\").groupBy([\"city\"]).count().show()\n</code></pre>"},{"location":"getting-started/basic-operations/#4-chain-related-operations","title":"4. Chain Related Operations","text":"<pre><code># Good - clear intent\nresult = df.filter(\"valid == true\") \\\n           .select([\"id\", \"value\"]) \\\n           .orderBy([\"value\"]) \\\n           .limit(100)\n\n# Avoid - unnecessary intermediate variables\nfiltered = df.filter(\"valid == true\")\nselected = filtered.select([\"id\", \"value\"])\nsorted_df = selected.orderBy([\"value\"])\nresult = sorted_df.limit(100)\n</code></pre>"},{"location":"getting-started/basic-operations/#next-steps","title":"Next Steps","text":"<ul> <li>DataFrame API Guide - Advanced DataFrame operations</li> <li>Performance Tips - Optimize your queries</li> <li>API Reference - Complete method documentation</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>This guide will help you install PyRust on your system.</p>"},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Python: 3.8 or higher</li> <li>Rust: 1.70 or higher (only for building from source)</li> <li>Operating System: Linux, macOS, or Windows</li> </ul>"},{"location":"getting-started/installation/#installation-methods","title":"Installation Methods","text":""},{"location":"getting-started/installation/#via-pip-recommended","title":"Via pip (Recommended)","text":"<p>Once PyRust is published to PyPI, installation is simple:</p> <pre><code>pip install pyrust\n</code></pre>"},{"location":"getting-started/installation/#from-github-release","title":"From GitHub Release","text":"<p>Download a pre-built wheel from the releases page:</p> <pre><code># Linux x86_64\npip install https://github.com/hadrien-chicault/PyRust/releases/download/v0.1.0/pyrust-0.1.0-cp312-cp312-linux_x86_64.whl\n\n# macOS x86_64\npip install https://github.com/hadrien-chicault/PyRust/releases/download/v0.1.0/pyrust-0.1.0-cp312-cp312-macosx_x86_64.whl\n\n# macOS ARM64 (Apple Silicon)\npip install https://github.com/hadrien-chicault/PyRust/releases/download/v0.1.0/pyrust-0.1.0-cp312-cp312-macosx_arm64.whl\n\n# Windows x86_64\npip install https://github.com/hadrien-chicault/PyRust/releases/download/v0.1.0/pyrust-0.1.0-cp312-cp312-win_amd64.whl\n</code></pre>"},{"location":"getting-started/installation/#from-source","title":"From Source","text":"<p>Building from source requires Rust and Maturin:</p> <pre><code># Clone the repository\ngit clone https://github.com/hadrien-chicault/PyRust.git\ncd PyRust\n\n# Install Rust (if not already installed)\ncurl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\n\n# Install maturin\npip install maturin\n\n# Build and install\nmaturin develop --release\n</code></pre>"},{"location":"getting-started/installation/#using-uv-fast-python-package-manager","title":"Using UV (Fast Python Package Manager)","text":"<p>UV is a fast Python package installer:</p> <pre><code># Install UV\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Install PyRust\nuv pip install pyrust\n\n# Or build from source\ngit clone https://github.com/hadrien-chicault/PyRust.git\ncd PyRust\nuv tool install maturin\nmaturin develop --release\n</code></pre>"},{"location":"getting-started/installation/#verify-installation","title":"Verify Installation","text":"<p>Test that PyRust is correctly installed:</p> <pre><code>from pyrust import SparkSession\n\n# Create a session\nspark = SparkSession.builder() \\\n    .appName(\"TestApp\") \\\n    .getOrCreate()\n\nprint(f\"PyRust installed successfully!\")\nprint(f\"App name: {spark.appName}\")\n</code></pre> <p>Expected output: <pre><code>PyRust installed successfully!\nApp name: TestApp\n</code></pre></p>"},{"location":"getting-started/installation/#optional-dependencies","title":"Optional Dependencies","text":""},{"location":"getting-started/installation/#for-development","title":"For Development","text":"<p>If you're contributing to PyRust, install development dependencies:</p> <pre><code>pip install pyrust[dev]\n</code></pre> <p>This includes: - <code>pytest</code> - Testing framework - <code>pytest-benchmark</code> - Performance benchmarking - <code>pandas</code> - For comparison tests - <code>maturin</code> - Build tool</p>"},{"location":"getting-started/installation/#for-data-analysis","title":"For Data Analysis","text":"<p>Additional libraries you might want:</p> <pre><code># Arrow for data interchange\npip install pyarrow\n\n# Pandas for comparison\npip install pandas\n\n# Jupyter for interactive analysis\npip install jupyter\n</code></pre>"},{"location":"getting-started/installation/#platform-specific-notes","title":"Platform-Specific Notes","text":""},{"location":"getting-started/installation/#linux","title":"Linux","text":"<p>On Linux, you may need to install additional system dependencies:</p> <pre><code># Ubuntu/Debian\nsudo apt-get update\nsudo apt-get install build-essential python3-dev\n\n# Fedora/RHEL\nsudo dnf install gcc python3-devel\n</code></pre>"},{"location":"getting-started/installation/#macos","title":"macOS","text":"<p>On macOS, ensure Xcode Command Line Tools are installed:</p> <pre><code>xcode-select --install\n</code></pre>"},{"location":"getting-started/installation/#windows","title":"Windows","text":"<p>On Windows, install the Visual C++ Build Tools:</p> <ol> <li>Download from Visual Studio Downloads</li> <li>Install \"Desktop development with C++\"</li> </ol>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/installation/#import-error","title":"Import Error","text":"<p>If you get <code>ImportError: No module named '_pyrust'</code>:</p> <pre><code># Reinstall with --force-reinstall\npip install --force-reinstall pyrust\n</code></pre>"},{"location":"getting-started/installation/#permission-denied","title":"Permission Denied","text":"<p>On Linux/macOS, if you get permission errors:</p> <pre><code># Install for current user only\npip install --user pyrust\n</code></pre>"},{"location":"getting-started/installation/#rust-version-too-old","title":"Rust Version Too Old","text":"<p>If building from source fails with Rust version errors:</p> <pre><code># Update Rust\nrustup update stable\n</code></pre>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<p>Now that PyRust is installed, check out:</p> <ul> <li>Quick Start Tutorial - Write your first PyRust program</li> <li>Basic Operations - Learn common data operations</li> <li>API Reference - Explore the full API</li> </ul>"},{"location":"getting-started/quickstart/","title":"Quick Start Tutorial","text":"<p>This tutorial will get you started with PyRust in 5 minutes.</p>"},{"location":"getting-started/quickstart/#your-first-pyrust-program","title":"Your First PyRust Program","text":"<p>Let's create a simple data analysis program:</p> <pre><code>from pyrust import SparkSession\n\n# 1. Create a SparkSession\nspark = SparkSession.builder() \\\n    .appName(\"MyFirstApp\") \\\n    .getOrCreate()\n\n# 2. Read a CSV file\ndf = spark.read.csv(\"data.csv\", header=True, infer_schema=True)\n\n# 3. Inspect the data\ndf.printSchema()\ndf.show(5)\n\n# 4. Perform transformations\nresult = df.select([\"name\", \"age\", \"city\"]) \\\n           .filter(\"age &gt; 18\") \\\n           .orderBy([\"age\"])\n\n# 5. Display results\nresult.show()\n</code></pre>"},{"location":"getting-started/quickstart/#step-by-step-breakdown","title":"Step-by-Step Breakdown","text":""},{"location":"getting-started/quickstart/#step-1-create-a-sparksession","title":"Step 1: Create a SparkSession","text":"<p>The <code>SparkSession</code> is your entry point to PyRust:</p> <pre><code>spark = SparkSession.builder() \\\n    .appName(\"MyFirstApp\") \\\n    .getOrCreate()\n</code></pre> <ul> <li><code>builder()</code> - Returns a builder for configuration</li> <li><code>appName()</code> - Sets your application name</li> <li><code>getOrCreate()</code> - Creates or retrieves the session</li> </ul>"},{"location":"getting-started/quickstart/#step-2-read-data","title":"Step 2: Read Data","text":"<p>Load data from various sources:</p> <pre><code># CSV with header\ndf = spark.read.csv(\"data.csv\", header=True, infer_schema=True)\n\n# Parquet (faster for large files)\ndf = spark.read.parquet(\"data.parquet\")\n</code></pre> <p>Supported formats: - CSV - Comma-separated values - Parquet - Columnar format (recommended for production)</p>"},{"location":"getting-started/quickstart/#step-3-inspect-data","title":"Step 3: Inspect Data","text":"<p>Understand your data structure:</p> <pre><code># Print schema (column types)\ndf.printSchema()\n# Output:\n# root\n#  |-- name: Utf8 (nullable = true)\n#  |-- age: Int64 (nullable = true)\n#  |-- city: Utf8 (nullable = true)\n\n# Show first rows\ndf.show(5)  # Show 5 rows\n\n# Count total rows\nprint(f\"Total rows: {df.count()}\")\n</code></pre>"},{"location":"getting-started/quickstart/#step-4-transform-data","title":"Step 4: Transform Data","text":"<p>Chain operations to transform your data:</p> <pre><code># Select specific columns\nsubset = df.select([\"name\", \"age\"])\n\n# Filter rows\nadults = df.filter(\"age &gt;= 18\")\n\n# Sort data\nsorted_df = df.orderBy([\"age\"])\n\n# Limit results\ntop_10 = df.limit(10)\n\n# Chain operations (lazy evaluation)\nresult = df.select([\"name\", \"age\"]) \\\n           .filter(\"age &gt; 18\") \\\n           .orderBy([\"age\"]) \\\n           .limit(100)\n</code></pre>"},{"location":"getting-started/quickstart/#step-5-aggregations","title":"Step 5: Aggregations","text":"<p>Compute statistics on grouped data:</p> <pre><code># Count by group\ncity_counts = df.groupBy([\"city\"]).count()\ncity_counts.show()\n\n# Multiple aggregations\nstats = df.groupBy([\"city\"]).agg([\n    (\"age\", \"avg\"),\n    (\"age\", \"min\"),\n    (\"age\", \"max\")\n])\nstats.show()\n</code></pre>"},{"location":"getting-started/quickstart/#complete-example","title":"Complete Example","text":"<p>Here's a complete data analysis workflow:</p> <pre><code>from pyrust import SparkSession\n\n# Initialize\nspark = SparkSession.builder().appName(\"Analysis\").getOrCreate()\n\n# Load sales data\nsales = spark.read.csv(\"sales.csv\", header=True, infer_schema=True)\n\n# Analyze high-value sales by region\nanalysis = sales.filter(\"amount &gt; 1000\") \\\n                .select([\"region\", \"amount\", \"product\"]) \\\n                .groupBy([\"region\"]) \\\n                .agg([\n                    (\"amount\", \"sum\"),\n                    (\"amount\", \"avg\"),\n                    (\"product\", \"count\")\n                ]) \\\n                .orderBy([\"sum(amount)\"])\n\n# Display results\nprint(\"High-Value Sales Analysis:\")\nanalysis.show()\n\n# Get summary statistics\nprint(f\"\\nTotal high-value transactions: {sales.filter('amount &gt; 1000').count()}\")\n</code></pre>"},{"location":"getting-started/quickstart/#performance-tips","title":"Performance Tips","text":""},{"location":"getting-started/quickstart/#1-filter-early","title":"1. Filter Early","text":"<p>Apply filters before other operations:</p> <pre><code># Good - filter first\ndf.filter(\"age &gt; 18\").select([\"name\", \"city\"]).groupBy([\"city\"]).count()\n\n# Less efficient - filter last\ndf.select([\"name\", \"city\"]).groupBy([\"city\"]).count().filter(\"count &gt; 100\")\n</code></pre>"},{"location":"getting-started/quickstart/#2-select-only-needed-columns","title":"2. Select Only Needed Columns","text":"<p>Reduce data size by selecting only required columns:</p> <pre><code># Good - select only needed columns\ndf.select([\"name\", \"age\"]).filter(\"age &gt; 18\")\n\n# Less efficient - process all columns\ndf.filter(\"age &gt; 18\")  # Still carries all columns\n</code></pre>"},{"location":"getting-started/quickstart/#3-use-parquet-for-large-files","title":"3. Use Parquet for Large Files","text":"<p>Parquet is much faster than CSV:</p> <pre><code># Save as Parquet for future use\ndf.write.parquet(\"data.parquet\")  # TODO: Not yet implemented\n\n# Read Parquet (10-100x faster than CSV)\ndf = spark.read.parquet(\"data.parquet\")\n</code></pre>"},{"location":"getting-started/quickstart/#4-limit-when-exploring","title":"4. Limit When Exploring","text":"<p>Use <code>limit()</code> for quick data exploration:</p> <pre><code># Fast preview\ndf.limit(1000).show()\n\n# Analyze small subset first\ndf.limit(10000).groupBy([\"category\"]).count().show()\n</code></pre>"},{"location":"getting-started/quickstart/#common-patterns","title":"Common Patterns","text":""},{"location":"getting-started/quickstart/#pattern-1-filter-and-aggregate","title":"Pattern 1: Filter and Aggregate","text":"<pre><code># Count adult users by city\nadult_by_city = df.filter(\"age &gt;= 18\") \\\n                  .groupBy([\"city\"]) \\\n                  .count() \\\n                  .orderBy([\"count\"])\nadult_by_city.show()\n</code></pre>"},{"location":"getting-started/quickstart/#pattern-2-top-n-analysis","title":"Pattern 2: Top N Analysis","text":"<pre><code># Top 10 highest sales\ntop_sales = df.orderBy([\"amount\"]) \\\n              .limit(10) \\\n              .select([\"customer\", \"amount\", \"date\"])\ntop_sales.show()\n</code></pre>"},{"location":"getting-started/quickstart/#pattern-3-multi-column-grouping","title":"Pattern 3: Multi-Column Grouping","text":"<pre><code># Sales by country and product\nsummary = df.groupBy([\"country\", \"product\"]) \\\n            .agg([(\"amount\", \"sum\"), (\"order_id\", \"count\")])\nsummary.show()\n</code></pre>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<p>You're now ready to use PyRust! Continue learning:</p> <ul> <li>Basic Operations - More detailed operation guides</li> <li>DataFrame API - Complete DataFrame guide</li> <li>Performance Tips - Optimize your queries</li> <li>API Reference - Full API documentation</li> </ul>"},{"location":"getting-started/quickstart/#sample-data","title":"Sample Data","text":"<p>Need sample data to practice? Create a test CSV file:</p> <pre><code>name,age,city,salary\nAlice,25,New York,75000\nBob,30,London,85000\nCharlie,35,New York,95000\nDiana,28,Paris,80000\nEve,42,London,110000\nFrank,19,Paris,45000\nGrace,55,New York,125000\nHenry,31,London,88000\n</code></pre> <p>Save this as <code>test.csv</code> and try the examples above!</p>"},{"location":"user-guide/data-loading/","title":"Data Loading Guide","text":"<p>Learn how to efficiently load data into PyRust DataFrames.</p>"},{"location":"user-guide/data-loading/#supported-formats","title":"Supported Formats","text":"<p>PyRust currently supports: - CSV - Comma-separated values - Parquet - Columnar format (recommended)</p>"},{"location":"user-guide/data-loading/#csv-files","title":"CSV Files","text":""},{"location":"user-guide/data-loading/#basic-csv-loading","title":"Basic CSV Loading","text":"<pre><code># Simple read\ndf = spark.read.csv(\"data.csv\")\n\n# With header\ndf = spark.read.csv(\"data.csv\", header=True)\n\n# With schema inference\ndf = spark.read.csv(\"data.csv\", header=True, infer_schema=True)\n</code></pre>"},{"location":"user-guide/data-loading/#csv-options","title":"CSV Options","text":"Option Type Default Description <code>header</code> bool <code>True</code> First row contains column names <code>infer_schema</code> bool <code>True</code> Automatically detect column types"},{"location":"user-guide/data-loading/#schema-inference","title":"Schema Inference","text":"<p>When <code>infer_schema=True</code>, PyRust samples the first 1000 rows to detect types:</p> <pre><code>df = spark.read.csv(\"data.csv\", header=True, infer_schema=True)\ndf.printSchema()\n# Output:\n# root\n#  |-- name: Utf8\n#  |-- age: Int64\n#  |-- salary: Float64\n</code></pre> <p>Inference Rules: - Pure integers \u2192 <code>Int64</code> - Numbers with decimals \u2192 <code>Float64</code> - Text \u2192 <code>Utf8</code> (string) - Dates (future) \u2192 <code>Date</code> or <code>Timestamp</code></p>"},{"location":"user-guide/data-loading/#performance-tips-for-csv","title":"Performance Tips for CSV","text":"<ol> <li> <p>Use Schema Inference Carefully <pre><code># Fast: skip inference for known schema\ndf = spark.read.csv(\"data.csv\", header=True, infer_schema=False)\n</code></pre></p> </li> <li> <p>Convert to Parquet <pre><code># Read CSV once\ndf = spark.read.csv(\"large_file.csv\", header=True)\n\n# TODO: Write to Parquet (not yet implemented)\n# df.write.parquet(\"large_file.parquet\")\n\n# Future reads will be much faster\n# df = spark.read.parquet(\"large_file.parquet\")\n</code></pre></p> </li> </ol>"},{"location":"user-guide/data-loading/#parquet-files","title":"Parquet Files","text":""},{"location":"user-guide/data-loading/#basic-parquet-loading","title":"Basic Parquet Loading","text":"<pre><code># Read single file\ndf = spark.read.parquet(\"data.parquet\")\n\n# Read directory of Parquet files\ndf = spark.read.parquet(\"data_dir/\")\n</code></pre>"},{"location":"user-guide/data-loading/#why-parquet","title":"Why Parquet?","text":"<p>Advantages over CSV: - 10-100x faster for selective queries - 5-10x smaller file size - Schema preserved - no inference needed - Columnar format - only read needed columns - Built-in compression - automatic - Predicate pushdown - filters applied during read</p> <p>Comparison Example:</p> Metric CSV (1GB) Parquet (200MB) File Size 1.0 GB 0.2 GB (5x smaller) Full Read 12.5s 1.2s (10x faster) Select 2 cols 11.8s 0.3s (39x faster) Filter + Select 8.3s 0.2s (41x faster)"},{"location":"user-guide/data-loading/#parquet-partitioning","title":"Parquet Partitioning","text":"<p>Parquet files can be partitioned for even better performance:</p> <pre><code>data_dir/\n  year=2023/\n    month=01/\n      data.parquet\n    month=02/\n      data.parquet\n  year=2024/\n    month=01/\n      data.parquet\n</code></pre> <pre><code># Read all partitions\ndf = spark.read.parquet(\"data_dir/\")\n\n# Filter automatically uses partitions\n# Only reads data from 2024\ndf_2024 = df.filter(\"year == 2024\")\n</code></pre>"},{"location":"user-guide/data-loading/#file-path-patterns","title":"File Path Patterns","text":""},{"location":"user-guide/data-loading/#local-files","title":"Local Files","text":"<pre><code># Absolute path\ndf = spark.read.csv(\"/home/user/data.csv\")\n\n# Relative path\ndf = spark.read.csv(\"./data/input.csv\")\ndf = spark.read.csv(\"data.csv\")\n</code></pre>"},{"location":"user-guide/data-loading/#multiple-files-future","title":"Multiple Files (Future)","text":"<pre><code># Future: Read multiple files\n# df = spark.read.csv(\"data/*.csv\")\n# df = spark.read.csv([\"file1.csv\", \"file2.csv\"])\n</code></pre>"},{"location":"user-guide/data-loading/#data-sources-best-practices","title":"Data Sources Best Practices","text":""},{"location":"user-guide/data-loading/#1-choose-the-right-format","title":"1. Choose the Right Format","text":"Use Case Recommended Format Ad-hoc analysis CSV (easy to create/view) Production pipelines Parquet (fast, compressed) One-time import CSV (then convert to Parquet) Archival storage Parquet (compressed) Human-readable CSV Machine-only Parquet"},{"location":"user-guide/data-loading/#2-optimize-csv-reading","title":"2. Optimize CSV Reading","text":"<pre><code># For large CSVs, consider:\n# 1. Enable schema inference\ndf = spark.read.csv(\"large.csv\", header=True, infer_schema=True)\n\n# 2. Save to Parquet for future use\n# df.write.parquet(\"large.parquet\")  # TODO\n</code></pre>"},{"location":"user-guide/data-loading/#3-leverage-parquet-features","title":"3. Leverage Parquet Features","text":"<pre><code># Parquet automatically optimizes these operations:\n\n# Only reads 'name' and 'age' columns\ndf = spark.read.parquet(\"data.parquet\").select([\"name\", \"age\"])\n\n# Filter is pushed down to file read\ndf = spark.read.parquet(\"data.parquet\").filter(\"age &gt; 18\")\n\n# Combines both optimizations\ndf = spark.read.parquet(\"data.parquet\") \\\n       .select([\"name\", \"age\"]) \\\n       .filter(\"age &gt; 18\")\n</code></pre>"},{"location":"user-guide/data-loading/#common-issues","title":"Common Issues","text":""},{"location":"user-guide/data-loading/#issue-1-file-not-found","title":"Issue 1: File Not Found","text":"<pre><code># Error: file doesn't exist\ndf = spark.read.csv(\"missing.csv\")\n# RuntimeError: Failed to read CSV: ...\n\n# Solution: Check path\nimport os\nprint(os.path.exists(\"data.csv\"))  # True\ndf = spark.read.csv(\"data.csv\")\n</code></pre>"},{"location":"user-guide/data-loading/#issue-2-encoding-problems","title":"Issue 2: Encoding Problems","text":"<pre><code># CSV with special characters may fail\n# Future: add encoding parameter\n# df = spark.read.csv(\"data.csv\", encoding=\"utf-8\")\n</code></pre>"},{"location":"user-guide/data-loading/#issue-3-large-file-memory","title":"Issue 3: Large File Memory","text":"<pre><code># If file is too large, filter early\ndf = spark.read.csv(\"huge.csv\", header=True)\nsubset = df.limit(1000)  # Work with subset first\nsubset.show()\n</code></pre>"},{"location":"user-guide/data-loading/#next-steps","title":"Next Steps","text":"<ul> <li>DataFrame Guide - Working with loaded data</li> <li>Performance Tips - Optimize data loading</li> <li>API Reference - Complete API documentation</li> </ul>"},{"location":"user-guide/dataframe/","title":"DataFrame Guide","text":"<p>A comprehensive guide to working with PyRust DataFrames.</p>"},{"location":"user-guide/dataframe/#what-is-a-dataframe","title":"What is a DataFrame?","text":"<p>A DataFrame is a distributed collection of data organized into named columns. It's conceptually similar to: - A table in a relational database - A Pandas DataFrame (but distributed) - A spreadsheet with named columns</p>"},{"location":"user-guide/dataframe/#creating-dataframes","title":"Creating DataFrames","text":""},{"location":"user-guide/dataframe/#from-files","title":"From Files","text":"<pre><code># CSV\ndf = spark.read.csv(\"data.csv\", header=True, infer_schema=True)\n\n# Parquet\ndf = spark.read.parquet(\"data.parquet\")\n</code></pre>"},{"location":"user-guide/dataframe/#from-memory-future-feature","title":"From Memory (Future Feature)","text":"<pre><code># Not yet implemented\ndata = [[\"Alice\", 25], [\"Bob\", 30]]\ndf = spark.createDataFrame(data, [\"name\", \"age\"])\n</code></pre>"},{"location":"user-guide/dataframe/#dataframe-properties","title":"DataFrame Properties","text":""},{"location":"user-guide/dataframe/#schema","title":"Schema","text":"<p>Every DataFrame has a schema defining column names and types:</p> <pre><code># View schema\ndf.printSchema()\n\n# Output:\n# root\n#  |-- name: Utf8 (nullable = true)\n#  |-- age: Int64 (nullable = true)\n#  |-- salary: Float64 (nullable = true)\n</code></pre> <p>Supported Types: - <code>Utf8</code> - String/text - <code>Int64</code> - 64-bit integer - <code>Float64</code> - 64-bit float - <code>Boolean</code> - true/false - <code>Date</code>, <code>Timestamp</code> - Date/time values</p>"},{"location":"user-guide/dataframe/#row-count","title":"Row Count","text":"<pre><code>count = df.count()  # Triggers execution\nprint(f\"Total rows: {count}\")\n</code></pre>"},{"location":"user-guide/dataframe/#transformations","title":"Transformations","text":"<p>Transformations are lazy - they build a query plan but don't execute until an action is called.</p>"},{"location":"user-guide/dataframe/#select","title":"Select","text":"<p>Project specific columns:</p> <pre><code># Select columns\ndf.select([\"name\", \"age\"])\n\n# Reorder columns\ndf.select([\"age\", \"name\", \"city\"])\n\n# Select single column\ndf.select([\"name\"])\n</code></pre>"},{"location":"user-guide/dataframe/#filter-where","title":"Filter / Where","text":"<p>Keep rows matching a condition:</p> <pre><code># Numeric filters\ndf.filter(\"age &gt; 18\")\ndf.filter(\"salary &gt;= 50000\")\ndf.filter(\"score &lt; 100\")\n\n# String filters\ndf.filter(\"city == 'New York'\")\ndf.filter(\"status != 'inactive'\")\n\n# where() is an alias for filter()\ndf.where(\"age &gt; 18\")\n</code></pre> <p>Limitation: Currently only supports simple conditions (<code>column op value</code>). Complex conditions with AND/OR coming in future versions.</p>"},{"location":"user-guide/dataframe/#sort-orderby","title":"Sort / OrderBy","text":"<p>Sort rows by column values:</p> <pre><code># Sort by single column\ndf.orderBy([\"age\"])\n\n# Sort by multiple columns\ndf.orderBy([\"country\", \"city\", \"name\"])\n\n# sort() is an alias for orderBy()\ndf.sort([\"age\"])\n</code></pre> <p>Note: Currently only ascending order supported. Descending order coming in future versions.</p>"},{"location":"user-guide/dataframe/#limit","title":"Limit","text":"<p>Restrict number of rows:</p> <pre><code># First 10 rows\ndf.limit(10)\n\n# Top 5 after sorting\ndf.orderBy([\"sales\"]).limit(5)\n</code></pre>"},{"location":"user-guide/dataframe/#groupby","title":"GroupBy","text":"<p>Group rows for aggregation:</p> <pre><code># Count by group\ndf.groupBy([\"city\"]).count()\n\n# Multiple aggregations\ndf.groupBy([\"department\"]).agg([\n    (\"salary\", \"avg\"),\n    (\"salary\", \"max\"),\n    (\"age\", \"min\")\n])\n\n# Multi-level grouping\ndf.groupBy([\"country\", \"city\"]).count()\n</code></pre>"},{"location":"user-guide/dataframe/#actions","title":"Actions","text":"<p>Actions trigger execution and return results to Python.</p>"},{"location":"user-guide/dataframe/#show","title":"Show","text":"<p>Display rows in formatted table:</p> <pre><code># Default: 20 rows\ndf.show()\n\n# Custom number\ndf.show(5)\ndf.show(100)\n</code></pre>"},{"location":"user-guide/dataframe/#count","title":"Count","text":"<p>Return row count:</p> <pre><code>total = df.count()\nfiltered_count = df.filter(\"age &gt; 18\").count()\n</code></pre>"},{"location":"user-guide/dataframe/#collect-future","title":"Collect (Future)","text":"<p>Return all data to Python (not yet implemented):</p> <pre><code># Future feature\nrows = df.collect()  # Returns list of rows\n</code></pre>"},{"location":"user-guide/dataframe/#complex-workflows","title":"Complex Workflows","text":""},{"location":"user-guide/dataframe/#example-1-etl-pipeline","title":"Example 1: ETL Pipeline","text":"<pre><code># Extract\nraw_df = spark.read.csv(\"raw_data.csv\", header=True)\n\n# Transform\nclean_df = raw_df.filter(\"valid == true\") \\\n                 .select([\"id\", \"name\", \"amount\", \"date\"]) \\\n                 .filter(\"amount &gt; 0\") \\\n                 .orderBy([\"date\"])\n\n# Load (Future: write to Parquet)\nprint(f\"Processed {clean_df.count()} records\")\nclean_df.show(10)\n</code></pre>"},{"location":"user-guide/dataframe/#example-2-aggregation-report","title":"Example 2: Aggregation Report","text":"<pre><code># Load data\nsales = spark.read.parquet(\"sales.parquet\")\n\n# Multi-level aggregation\nreport = sales.filter(\"year == 2024\") \\\n              .groupBy([\"region\", \"product\"]) \\\n              .agg([\n                  (\"amount\", \"sum\"),\n                  (\"amount\", \"avg\"),\n                  (\"transaction_id\", \"count\")\n              ]) \\\n              .orderBy([\"region\", \"sum(amount)\"])\n\n# Display\nprint(\"2024 Sales Report:\")\nreport.show(50)\n</code></pre>"},{"location":"user-guide/dataframe/#example-3-data-quality-check","title":"Example 3: Data Quality Check","text":"<pre><code># Load data\ndf = spark.read.csv(\"user_data.csv\", header=True)\n\n# Check for data issues\nprint(\"=== Data Quality Report ===\\n\")\n\n# Total records\nprint(f\"Total records: {df.count()}\")\n\n# Age validation\ninvalid_age = df.filter(\"age &lt; 0\")\nprint(f\"Invalid ages: {invalid_age.count()}\")\n\n# Missing cities (assuming empty strings)\nmissing_city = df.filter(\"city == ''\")\nprint(f\"Missing cities: {missing_city.count()}\")\n\n# Show problematic records\nprint(\"\\nProblematic Records:\")\ndf.filter(\"age &lt; 0\").show(10)\n</code></pre>"},{"location":"user-guide/dataframe/#performance-optimization","title":"Performance Optimization","text":""},{"location":"user-guide/dataframe/#filter-pushdown","title":"Filter Pushdown","text":"<p>Apply filters early to reduce data volume:</p> <pre><code># Good - filter early\ndf.filter(\"year == 2024\") \\\n  .filter(\"valid == true\") \\\n  .select([\"name\", \"amount\"]) \\\n  .groupBy([\"name\"]) \\\n  .count()\n\n# Less efficient - filter late\ndf.groupBy([\"name\", \"year\"]) \\\n  .count() \\\n  .filter(\"year == 2024\")\n</code></pre>"},{"location":"user-guide/dataframe/#column-pruning","title":"Column Pruning","text":"<p>Select only needed columns:</p> <pre><code># Good - select early\ndf.select([\"id\", \"amount\"]) \\\n  .filter(\"amount &gt; 1000\") \\\n  .orderBy([\"amount\"])\n\n# Less efficient - carries all columns\ndf.filter(\"amount &gt; 1000\") \\\n  .orderBy([\"amount\"])\n</code></pre>"},{"location":"user-guide/dataframe/#limit-for-exploration","title":"Limit for Exploration","text":"<p>Use limit() when exploring large datasets:</p> <pre><code># Fast preview\ndf.limit(1000).show()\n\n# Test transformations on subset\ndf.limit(10000) \\\n  .filter(\"category == 'electronics'\") \\\n  .groupBy([\"brand\"]) \\\n  .count() \\\n  .show()\n</code></pre>"},{"location":"user-guide/dataframe/#common-patterns","title":"Common Patterns","text":""},{"location":"user-guide/dataframe/#top-n-query","title":"Top N Query","text":"<pre><code># Top 10 highest values\ntop_10 = df.orderBy([\"sales\"]) \\\n           .limit(10) \\\n           .select([\"product\", \"sales\"])\ntop_10.show()\n</code></pre>"},{"location":"user-guide/dataframe/#distinct-count","title":"Distinct Count","text":"<pre><code># Count unique cities\nunique_cities = df.select([\"city\"]) \\\n                  .distinct() \\  # TODO: Not yet implemented\n                  .count()\n</code></pre>"},{"location":"user-guide/dataframe/#summary-statistics","title":"Summary Statistics","text":"<pre><code># Statistics by group\nstats = df.groupBy([\"category\"]).agg([\n    (\"price\", \"min\"),\n    (\"price\", \"max\"),\n    (\"price\", \"avg\"),\n    (\"product_id\", \"count\")\n])\nstats.show()\n</code></pre>"},{"location":"user-guide/dataframe/#working-with-nulls","title":"Working with Nulls","text":""},{"location":"user-guide/dataframe/#handling-nulls","title":"Handling Nulls","text":"<pre><code># Filter out nulls (future feature)\n# df.filter(\"age IS NOT NULL\")\n\n# For now, use value checks\ndf.filter(\"age &gt; 0\")  # Assuming positive ages are valid\n</code></pre>"},{"location":"user-guide/dataframe/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/dataframe/#1-cache-intermediate-results-future","title":"1. Cache Intermediate Results (Future)","text":"<pre><code># Future feature\n# expensive_df = df.filter(...).select(...)\n# expensive_df.cache()  # Reuse in memory\n</code></pre>"},{"location":"user-guide/dataframe/#2-understand-lazy-evaluation","title":"2. Understand Lazy Evaluation","text":"<pre><code># These DON'T execute immediately\nfiltered = df.filter(\"age &gt; 18\")  # Lazy\nselected = filtered.select([\"name\"])  # Lazy\nsorted_df = selected.orderBy([\"name\"])  # Lazy\n\n# This triggers execution of entire chain\nsorted_df.show()  # Action!\n</code></pre>"},{"location":"user-guide/dataframe/#3-avoid-repeated-actions","title":"3. Avoid Repeated Actions","text":"<pre><code># Bad - counts twice (executes query twice)\ncount1 = df.filter(\"age &gt; 18\").count()\ncount2 = df.filter(\"age &gt; 18\").count()\n\n# Good - store result\nfiltered = df.filter(\"age &gt; 18\")\ncount = filtered.count()\n</code></pre>"},{"location":"user-guide/dataframe/#4-use-parquet-for-production","title":"4. Use Parquet for Production","text":"<pre><code># CSV: Good for ad-hoc analysis\ndf = spark.read.csv(\"data.csv\")\n\n# Parquet: Better for production (faster, smaller)\ndf = spark.read.parquet(\"data.parquet\")\n</code></pre>"},{"location":"user-guide/dataframe/#debugging","title":"Debugging","text":""},{"location":"user-guide/dataframe/#view-query-plan-future","title":"View Query Plan (Future)","text":"<pre><code># Future feature: df.explain()\n</code></pre>"},{"location":"user-guide/dataframe/#print-intermediate-results","title":"Print Intermediate Results","text":"<pre><code># Check data at each step\nstep1 = df.filter(\"age &gt; 18\")\nprint(f\"After filter: {step1.count()}\")\n\nstep2 = step1.select([\"name\", \"city\"])\nstep2.show(5)\n\nstep3 = step2.groupBy([\"city\"]).count()\nstep3.show()\n</code></pre>"},{"location":"user-guide/dataframe/#next-steps","title":"Next Steps","text":"<ul> <li>Data Loading Guide - More on reading data</li> <li>Performance Tips - Optimize your queries</li> <li>API Reference - Complete DataFrame API</li> </ul>"},{"location":"user-guide/performance/","title":"Performance Tips","text":"<p>Optimize your PyRust workflows for maximum performance.</p>"},{"location":"user-guide/performance/#pyrust-performance-profile","title":"PyRust Performance Profile","text":""},{"location":"user-guide/performance/#speed-advantages","title":"Speed Advantages","text":"<p>PyRust is typically 10-50x faster than PySpark:</p> Operation PySpark PyRust Speedup CSV Read 12.5s 1.2s 10.4x Filter + Select 8.3s 0.4s 20.8x GroupBy + Aggregate 15.2s 0.8s 19.0x Multi-stage Pipeline 45.6s 2.1s 21.7x"},{"location":"user-guide/performance/#why-so-fast","title":"Why So Fast?","text":"<ol> <li>Rust Performance - Compiled, not interpreted</li> <li>Arrow Columnar Format - Vectorized operations</li> <li>DataFusion Optimizer - Query plan optimization</li> <li>Zero-Copy - No Python/Rust boundary overhead</li> <li>Parallel Execution - Automatic parallelization</li> </ol>"},{"location":"user-guide/performance/#general-optimization-strategies","title":"General Optimization Strategies","text":""},{"location":"user-guide/performance/#1-choose-the-right-file-format","title":"1. Choose the Right File Format","text":""},{"location":"user-guide/performance/#parquet-vs-csv","title":"Parquet vs CSV","text":"<p>Use Parquet for: - Production pipelines - Large datasets (&gt;100MB) - Repeated queries - Column-selective queries</p> <p>Parquet advantages: <pre><code># Read Parquet: 10-100x faster\ndf = spark.read.parquet(\"data.parquet\")\n\n# Only reads needed columns (column pruning)\ndf.select([\"name\", \"age\"]).show()\n\n# Filters applied during read (predicate pushdown)\ndf.filter(\"age &gt; 18\").show()\n</code></pre></p> <p>Use CSV for: - Ad-hoc analysis - Human-readable data - One-time imports - Small files (&lt;10MB)</p>"},{"location":"user-guide/performance/#2-filter-early-and-often","title":"2. Filter Early and Often","text":"<p>Push filters as early as possible in your transformation chain:</p> <pre><code># \u2705 GOOD - Filter early\ndf.filter(\"year == 2024\") \\\n  .filter(\"status == 'active'\") \\\n  .select([\"name\", \"amount\"]) \\\n  .groupBy([\"name\"]) \\\n  .count()\n\n# \u274c BAD - Filter late\ndf.groupBy([\"name\", \"year\"]) \\\n  .count() \\\n  .filter(\"year == 2024\")\n</code></pre> <p>Impact: Filtering early can reduce execution time by 5-20x.</p>"},{"location":"user-guide/performance/#3-select-only-needed-columns","title":"3. Select Only Needed Columns","text":"<p>Column pruning reduces I/O and memory:</p> <pre><code># \u2705 GOOD - Select early\ndf.select([\"id\", \"amount\"]) \\\n  .filter(\"amount &gt; 1000\") \\\n  .groupBy([\"id\"]) \\\n  .count()\n\n# \u274c BAD - Carries unnecessary columns\ndf.filter(\"amount &gt; 1000\") \\\n  .groupBy([\"id\"]) \\\n  .count()\n</code></pre> <p>Impact: 2-5x faster with Parquet files.</p>"},{"location":"user-guide/performance/#4-minimize-sorting","title":"4. Minimize Sorting","text":"<p>Sorting is expensive - only sort when necessary:</p> <pre><code># \u2705 GOOD - Sort only final result\ndf.filter(\"active == true\") \\\n  .groupBy([\"category\"]) \\\n  .count() \\\n  .orderBy([\"count\"])  # Sort small result\n\n# \u274c BAD - Unnecessary sort\ndf.orderBy([\"name\"]) \\  # Sorts all data\n  .filter(\"active == true\") \\\n  .groupBy([\"category\"]) \\\n  .count()\n</code></pre>"},{"location":"user-guide/performance/#5-use-limit-for-exploration","title":"5. Use Limit for Exploration","text":"<p>When exploring data, use <code>limit()</code> liberally:</p> <pre><code># Quick preview\ndf.limit(1000).show()\n\n# Test transformations on subset\ndf.limit(10000) \\\n  .filter(\"category == 'electronics'\") \\\n  .groupBy([\"brand\"]) \\\n  .count() \\\n  .show()\n</code></pre>"},{"location":"user-guide/performance/#operation-specific-optimizations","title":"Operation-Specific Optimizations","text":""},{"location":"user-guide/performance/#reading-data","title":"Reading Data","text":"<pre><code># \u2705 GOOD - Parquet with schema\ndf = spark.read.parquet(\"data.parquet\")\n\n# \u26a0\ufe0f OK - CSV with schema inference\ndf = spark.read.csv(\"data.csv\", header=True, infer_schema=True)\n\n# \u274c SLOW - CSV without schema\ndf = spark.read.csv(\"data.csv\", header=False, infer_schema=False)\n</code></pre>"},{"location":"user-guide/performance/#filtering","title":"Filtering","text":"<pre><code># \u2705 GOOD - Simple conditions\ndf.filter(\"age &gt; 18\")\ndf.filter(\"salary &gt;= 50000\")\n\n# \u26a0\ufe0f Future - Complex conditions\n# df.filter(\"age &gt; 18 AND salary &gt;= 50000\")\n</code></pre>"},{"location":"user-guide/performance/#grouping","title":"Grouping","text":"<pre><code># \u2705 GOOD - Single aggregation\ndf.groupBy([\"city\"]).count()\n\n# \u2705 BETTER - Multiple aggregations in one pass\ndf.groupBy([\"city\"]).agg([\n    (\"age\", \"avg\"),\n    (\"salary\", \"avg\"),\n    (\"id\", \"count\")\n])\n\n# \u274c BAD - Separate aggregations\navg_age = df.groupBy([\"city\"]).agg([(\"age\", \"avg\")])\navg_salary = df.groupBy([\"city\"]).agg([(\"salary\", \"avg\")])\ncount_records = df.groupBy([\"city\"]).count()\n</code></pre>"},{"location":"user-guide/performance/#sorting","title":"Sorting","text":"<pre><code># \u2705 GOOD - Sort + Limit (Top N)\ndf.orderBy([\"sales\"]).limit(10)\n\n# \u274c BAD - Sort all data unnecessarily\ndf.orderBy([\"name\"])  # If order doesn't matter downstream\n</code></pre>"},{"location":"user-guide/performance/#memory-optimization","title":"Memory Optimization","text":""},{"location":"user-guide/performance/#understanding-memory-usage","title":"Understanding Memory Usage","text":"<p>PyRust uses Apache Arrow's columnar format, which is very memory-efficient:</p> <ul> <li>Columnar layout reduces memory fragmentation</li> <li>Compression reduces memory footprint</li> <li>Zero-copy operations avoid duplication</li> </ul>"},{"location":"user-guide/performance/#tips-for-large-datasets","title":"Tips for Large Datasets","text":"<ol> <li> <p>Stream Processing <pre><code># Process in chunks with limit\nfor offset in range(0, total, 100000):\n    chunk = df.limit(100000).offset(offset)  # Future: offset\n    process_chunk(chunk)\n</code></pre></p> </li> <li> <p>Select Early <pre><code># Don't: Load all columns\ndf = spark.read.parquet(\"huge.parquet\")\n\n# Do: Select needed columns\ndf = spark.read.parquet(\"huge.parquet\").select([\"id\", \"amount\"])\n</code></pre></p> </li> <li> <p>Filter Early <pre><code># Don't: Filter after loading\ndf = spark.read.parquet(\"huge.parquet\")\nfiltered = df.filter(\"year == 2024\")\n\n# Do: Filter during read (predicate pushdown)\ndf = spark.read.parquet(\"huge.parquet\")\n# Filter applied at read time automatically\nfiltered = df.filter(\"year == 2024\")\n</code></pre></p> </li> </ol>"},{"location":"user-guide/performance/#query-optimization-patterns","title":"Query Optimization Patterns","text":""},{"location":"user-guide/performance/#pattern-1-etl-pipeline","title":"Pattern 1: ETL Pipeline","text":"<pre><code># Optimized ETL\nclean_df = spark.read.parquet(\"raw.parquet\") \\  # Fast read\n             .filter(\"valid == true\") \\           # Early filter\n             .select([\"id\", \"amount\", \"date\"]) \\  # Column pruning\n             .filter(\"amount &gt; 0\") \\              # More filtering\n             .orderBy([\"date\"]) \\                 # Sort if needed\n             .limit(10000)                        # Limit output\n\nprint(f\"Processed: {clean_df.count()}\")\n</code></pre>"},{"location":"user-guide/performance/#pattern-2-aggregation-report","title":"Pattern 2: Aggregation Report","text":"<pre><code># Optimized aggregation\nreport = spark.read.parquet(\"sales.parquet\") \\\n              .filter(\"year == 2024\") \\              # Early filter\n              .select([\"region\", \"product\", \"amount\"]) \\  # Select needed\n              .groupBy([\"region\", \"product\"]) \\      # Group\n              .agg([(\"amount\", \"sum\"), (\"amount\", \"avg\")]) \\  # Multi-agg\n              .orderBy([\"sum(amount)\"])              # Sort result\n\nreport.show(50)\n</code></pre>"},{"location":"user-guide/performance/#pattern-3-data-quality","title":"Pattern 3: Data Quality","text":"<pre><code># Efficient validation\nvalidation = spark.read.parquet(\"data.parquet\") \\\n                  .select([\"id\", \"age\", \"email\"]) \\  # Select validators\n                  .filter(\"age &gt; 0\") \\                # Quick filter\n                  .filter(\"age &lt; 150\")                # Range check\n\nprint(f\"Valid records: {validation.count()}\")\n</code></pre>"},{"location":"user-guide/performance/#benchmarking-your-code","title":"Benchmarking Your Code","text":""},{"location":"user-guide/performance/#measure-performance","title":"Measure Performance","text":"<pre><code>import time\n\n# Time a query\nstart = time.time()\nresult = df.filter(\"age &gt; 18\").groupBy([\"city\"]).count()\nresult.show()\nelapsed = time.time() - start\nprint(f\"Query took: {elapsed:.2f}s\")\n</code></pre>"},{"location":"user-guide/performance/#compare-approaches","title":"Compare Approaches","text":"<pre><code># Approach 1: Filter late\nstart = time.time()\ndf.groupBy([\"city\"]).count().filter(\"count &gt; 100\").show()\ntime1 = time.time() - start\n\n# Approach 2: Filter early\nstart = time.time()\ndf.filter(\"active == true\").groupBy([\"city\"]).count().show()\ntime2 = time.time() - start\n\nprint(f\"Approach 1: {time1:.2f}s\")\nprint(f\"Approach 2: {time2:.2f}s ({time1/time2:.1f}x faster)\")\n</code></pre>"},{"location":"user-guide/performance/#performance-checklist","title":"Performance Checklist","text":"<p>Before running queries on large datasets:</p> <ul> <li> Using Parquet format? (vs CSV)</li> <li> Filtering as early as possible?</li> <li> Selecting only needed columns?</li> <li> Combining multiple aggregations in one <code>agg()</code> call?</li> <li> Avoiding unnecessary sorts?</li> <li> Using <code>limit()</code> for exploration?</li> <li> Testing on small subset first?</li> </ul>"},{"location":"user-guide/performance/#common-performance-pitfalls","title":"Common Performance Pitfalls","text":""},{"location":"user-guide/performance/#pitfall-1-late-filtering","title":"Pitfall 1: Late Filtering","text":"<pre><code># \u274c Processes all data before filtering\ndf.groupBy([\"category\"]).count().filter(\"count &gt; 1000\")\n\n# \u2705 Filters early\ndf.filter(\"active == true\").groupBy([\"category\"]).count()\n</code></pre>"},{"location":"user-guide/performance/#pitfall-2-multiple-aggregations","title":"Pitfall 2: Multiple Aggregations","text":"<pre><code># \u274c Three separate queries\navg1 = df.groupBy([\"dept\"]).agg([(\"salary\", \"avg\")])\nmax1 = df.groupBy([\"dept\"]).agg([(\"salary\", \"max\")])\ncount1 = df.groupBy([\"dept\"]).count()\n\n# \u2705 Single query\nstats = df.groupBy([\"dept\"]).agg([\n    (\"salary\", \"avg\"),\n    (\"salary\", \"max\"),\n    (\"id\", \"count\")\n])\n</code></pre>"},{"location":"user-guide/performance/#pitfall-3-unnecessary-sorts","title":"Pitfall 3: Unnecessary Sorts","text":"<pre><code># \u274c Sorts before filter\ndf.orderBy([\"name\"]).filter(\"age &gt; 18\").show()\n\n# \u2705 Filter first (might make sort unnecessary)\ndf.filter(\"age &gt; 18\").show()  # No sort needed for show()\n</code></pre>"},{"location":"user-guide/performance/#pitfall-4-reading-all-columns","title":"Pitfall 4: Reading All Columns","text":"<pre><code># \u274c Reads all columns from Parquet\ndf = spark.read.parquet(\"wide_table.parquet\")\ndf.select([\"id\", \"name\"]).show()\n\n# \u2705 Column pruning automatically applied\ndf = spark.read.parquet(\"wide_table.parquet\") \\\n          .select([\"id\", \"name\"])  # Only reads 2 columns\ndf.show()\n</code></pre>"},{"location":"user-guide/performance/#next-steps","title":"Next Steps","text":"<ul> <li>DataFrame Guide - Master DataFrame operations</li> <li>Transformations - Learn transformation patterns</li> <li>Architecture - Understand internals</li> </ul>"},{"location":"user-guide/set-operations/","title":"Data Operations","text":"<p>PyRust provides comprehensive data manipulation operations for working with DataFrames. This guide covers operations for removing duplicates, combining DataFrames, and performing set operations.</p>"},{"location":"user-guide/set-operations/#removing-duplicates","title":"Removing Duplicates","text":""},{"location":"user-guide/set-operations/#distinct","title":"distinct()","text":"<p>Remove all duplicate rows from a DataFrame:</p> <pre><code># Remove duplicate rows (considers all columns)\nunique_df = df.distinct()\n</code></pre> <p>The <code>distinct()</code> method: - Compares all columns when determining duplicates - Returns a new DataFrame with only unique rows - Order of rows is not guaranteed</p>"},{"location":"user-guide/set-operations/#dropduplicates","title":"dropDuplicates()","text":"<p>Remove duplicates based on specific columns:</p> <pre><code># Remove duplicates based on specific columns\ndf.dropDuplicates(['name'])           # By single column\ndf.dropDuplicates(['name', 'city'])   # By multiple columns\ndf.dropDuplicates()                   # All columns (same as distinct)\n</code></pre> <p>Alias: You can also use <code>drop_duplicates()</code> (snake_case) for compatibility.</p> <p>Parameters: - <code>subset</code>: List of column names to consider for deduplication - If <code>None</code> or empty, all columns are used</p> <p>Examples:</p> <pre><code>from pyrust import SparkSession\n\nspark = SparkSession.builder.appName(\"DedupeExample\").getOrCreate()\n\n# Sample data with duplicates\ndf = spark.read.csv(\"users.csv\", header=True)\n\n# Keep only one row per unique name\nunique_names = df.dropDuplicates(['name'])\n\n# Keep only one row per unique (name, city) combination\nunique_pairs = df.dropDuplicates(['name', 'city'])\n\n# Remove all duplicate rows\ncompletely_unique = df.dropDuplicates()\n</code></pre>"},{"location":"user-guide/set-operations/#combining-dataframes","title":"Combining DataFrames","text":""},{"location":"user-guide/set-operations/#union-unionall","title":"union() / unionAll()","text":"<p>Combine two DataFrames vertically (stack rows):</p> <pre><code># Combine DataFrames (keeps all rows including duplicates)\ncombined = df1.union(df2)\n\n# unionAll() is an alias for union()\ncombined = df1.unionAll(df2)\n</code></pre> <p>Requirements: - Both DataFrames must have the same schema (column names and types) - Column order must match</p> <p>Behavior: - All rows from both DataFrames are included - Duplicates are kept (equivalent to SQL <code>UNION ALL</code>) - To remove duplicates after union, use <code>.distinct()</code></p> <p>Examples:</p> <pre><code># Combine two datasets\ndf1 = spark.read.csv(\"data_2023.csv\", header=True)\ndf2 = spark.read.csv(\"data_2024.csv\", header=True)\n\n# Combine keeping all rows\nall_data = df1.union(df2)\nprint(f\"Total rows: {all_data.count()}\")\n\n# Combine and remove duplicates\nunique_data = df1.union(df2).distinct()\nprint(f\"Unique rows: {unique_data.count()}\")\n</code></pre>"},{"location":"user-guide/set-operations/#set-operations","title":"Set Operations","text":""},{"location":"user-guide/set-operations/#intersect","title":"intersect()","text":"<p>Find rows that exist in both DataFrames:</p> <pre><code># Get rows common to both DataFrames\ncommon_rows = df1.intersect(df2)\n</code></pre> <p>Behavior: - Returns rows that appear in both DataFrames - Similar to SQL <code>INTERSECT</code> - May keep some duplicates (use <code>.distinct()</code> for truly unique results)</p> <p>Examples:</p> <pre><code># Find common users between two datasets\nusers_2023 = spark.read.csv(\"users_2023.csv\", header=True)\nusers_2024 = spark.read.csv(\"users_2024.csv\", header=True)\n\n# Users in both years\nreturning_users = users_2023.intersect(users_2024)\nreturning_users.show()\n\n# Ensure unique results\nunique_returning = users_2023.intersect(users_2024).distinct()\n</code></pre>"},{"location":"user-guide/set-operations/#exceptall-subtract","title":"exceptAll() / subtract()","text":"<p>Find rows in the first DataFrame that don't exist in the second:</p> <pre><code># Get rows in df1 but not in df2\ndifference = df1.exceptAll(df2)\n\n# subtract() is an alias\ndifference = df1.subtract(df2)\n</code></pre> <p>Behavior: - Returns rows from the first DataFrame that don't appear in the second - Equivalent to SQL <code>EXCEPT</code> - Duplicates are removed from the result</p> <p>Examples:</p> <pre><code># Find users who left\nall_users = spark.read.csv(\"all_users.csv\", header=True)\nactive_users = spark.read.csv(\"active_users.csv\", header=True)\n\n# Users who are no longer active\ninactive_users = all_users.exceptAll(active_users)\ninactive_users.show()\n\n# Also works with subtract()\ninactive_users = all_users.subtract(active_users)\n</code></pre>"},{"location":"user-guide/set-operations/#chaining-operations","title":"Chaining Operations","text":"<p>All operations can be chained together:</p> <pre><code># Complex chain of operations\nresult = (\n    df1.union(df2)           # Combine DataFrames\n    .distinct()               # Remove duplicates\n    .filter(\"age &gt; 25\")       # Filter rows\n    .orderBy(\"name\")          # Sort\n    .limit(100)               # Limit results\n)\n\n# Another example: data comparison\nhigh_earners = df.filter(\"salary &gt; 100000\")\nlong_tenure = df.filter(\"years_employed &gt; 5\")\n\n# People with high salary but short tenure\nnew_high_earners = high_earners.exceptAll(long_tenure)\n\n# People with both high salary and long tenure\nsenior_high_earners = high_earners.intersect(long_tenure).distinct()\n</code></pre>"},{"location":"user-guide/set-operations/#performance-tips","title":"Performance Tips","text":"<ol> <li> <p>Use distinct() early: Apply deduplication before expensive operations    <pre><code># Good\ndf.distinct().filter(\"complex_condition\").join(other)\n\n# Less efficient\ndf.filter(\"complex_condition\").join(other).distinct()\n</code></pre></p> </li> <li> <p>Use dropDuplicates() with specific columns: More efficient than distinct()    <pre><code># More efficient if you only care about name uniqueness\ndf.dropDuplicates(['name'])\n\n# Less efficient - checks all columns\ndf.distinct()\n</code></pre></p> </li> <li> <p>Union with distinct only when needed: union() is cheap, distinct() is expensive    <pre><code># If you know there are no duplicates\ndf1.union(df2)  # Fast\n\n# Only if you need unique rows\ndf1.union(df2).distinct()  # Slower\n</code></pre></p> </li> <li> <p>Order matters for except(): <code>df1.exceptAll(df2)</code> \u2260 <code>df2.exceptAll(df1)</code> <pre><code># Rows in A but not in B\na_only = df_a.exceptAll(df_b)\n\n# Rows in B but not in A\nb_only = df_b.exceptAll(df_a)\n</code></pre></p> </li> </ol>"},{"location":"user-guide/set-operations/#common-patterns","title":"Common Patterns","text":""},{"location":"user-guide/set-operations/#deduplication","title":"Deduplication","text":"<pre><code># Keep first occurrence based on timestamp\ndf.orderBy(\"timestamp\").dropDuplicates(['user_id'])\n\n# Remove exact duplicate rows\ndf.distinct()\n\n# Remove duplicates by key columns only\ndf.dropDuplicates(['id', 'date'])\n</code></pre>"},{"location":"user-guide/set-operations/#combining-multiple-sources","title":"Combining Multiple Sources","text":"<pre><code># Combine data from multiple files\ndf_list = [\n    spark.read.csv(\"data_jan.csv\", header=True),\n    spark.read.csv(\"data_feb.csv\", header=True),\n    spark.read.csv(\"data_mar.csv\", header=True),\n]\n\n# Union all\nfrom functools import reduce\ncombined = reduce(lambda a, b: a.union(b), df_list)\n\n# Remove any duplicates across sources\nfinal = combined.distinct()\n</code></pre>"},{"location":"user-guide/set-operations/#set-analysis","title":"Set Analysis","text":"<pre><code># Venn diagram analysis\nset_a = df.filter(\"category = 'A'\").select(\"user_id\")\nset_b = df.filter(\"category = 'B'\").select(\"user_id\")\n\n# Only in A\na_only = set_a.exceptAll(set_b)\n\n# Only in B\nb_only = set_b.exceptAll(set_a)\n\n# In both A and B\nboth = set_a.intersect(set_b).distinct()\n\n# In A or B (or both)\neither = set_a.union(set_b).distinct()\n</code></pre>"},{"location":"user-guide/set-operations/#data-quality-checks","title":"Data Quality Checks","text":"<pre><code># Find duplicate IDs (should be unique)\ndf.groupBy(\"id\").count().filter(\"count &gt; 1\")\n\n# Compare two datasets for differences\nexpected = spark.read.csv(\"expected.csv\", header=True)\nactual = spark.read.csv(\"actual.csv\", header=True)\n\n# Rows in expected but not in actual (missing data)\nmissing = expected.exceptAll(actual)\n\n# Rows in actual but not in expected (extra data)\nextra = actual.exceptAll(expected)\n\n# Rows in both (correct data)\ncorrect = expected.intersect(actual)\n</code></pre>"},{"location":"user-guide/set-operations/#examples","title":"Examples","text":"<p>See the complete examples in <code>examples/data_operations.py</code>:</p> <pre><code>python examples/data_operations.py\n</code></pre> <p>This example demonstrates: - Removing duplicates with distinct() and dropDuplicates() - Combining DataFrames with union() - Finding common rows with intersect() - Finding differences with exceptAll() - Chaining multiple operations - Practical use cases</p>"},{"location":"user-guide/set-operations/#method-summary","title":"Method Summary","text":"Method Description Removes Duplicates <code>distinct()</code> Remove duplicate rows (all columns) Yes <code>dropDuplicates(cols)</code> Remove duplicates by columns Yes <code>union(other)</code> Combine DataFrames vertically No <code>unionAll(other)</code> Alias for union() No <code>intersect(other)</code> Rows in both DataFrames Partial* <code>exceptAll(other)</code> Rows in first but not second Yes <code>subtract(other)</code> Alias for exceptAll() Yes <p>*Note: <code>intersect()</code> may keep some duplicates. Use <code>.distinct()</code> after for fully unique results.</p>"},{"location":"user-guide/set-operations/#api-aliases","title":"API Aliases","text":"<p>For compatibility, several methods have snake_case aliases:</p> <pre><code>df.dropDuplicates(['name'])  # camelCase\ndf.drop_duplicates(['name'])  # snake_case (alias)\n\ndf.unionAll(other)    # camelCase\n# (snake_case alias not provided as union() is already simple)\n\ndf.exceptAll(other)   # camelCase\ndf.subtract(other)    # alternative name\n</code></pre>"},{"location":"user-guide/sql-queries/","title":"SQL Queries","text":"<p>PyRust provides full SQL query support through DataFusion's SQL engine. You can execute standard SQL queries on your DataFrames using temporary views.</p>"},{"location":"user-guide/sql-queries/#quick-start","title":"Quick Start","text":"<pre><code>from pyrust import SparkSession\n\n# Create a SparkSession\nspark = SparkSession.builder.appName(\"SQLExample\").getOrCreate()\n\n# Load data\ndf = spark.read.csv(\"users.csv\", header=True)\n\n# Register as temporary view\ndf.createOrReplaceTempView(\"users\")\n\n# Execute SQL query\nresult = spark.sql(\"SELECT name, age FROM users WHERE age &gt; 25\")\nresult.show()\n</code></pre>"},{"location":"user-guide/sql-queries/#creating-temporary-views","title":"Creating Temporary Views","text":"<p>Before you can query a DataFrame with SQL, you need to register it as a temporary view:</p> <pre><code># Register DataFrame as a view\ndf.createOrReplaceTempView(\"users\")\n\n# Now you can reference it in SQL queries\nresult = spark.sql(\"SELECT * FROM users\")\n</code></pre>"},{"location":"user-guide/sql-queries/#replacing-views","title":"Replacing Views","text":"<p>The <code>createOrReplaceTempView()</code> method will replace any existing view with the same name:</p> <pre><code># Create initial view\ndf.createOrReplaceTempView(\"my_data\")\n\n# Later, replace it with a filtered version\nfiltered_df = df.filter(\"age &gt; 30\")\nfiltered_df.createOrReplaceTempView(\"my_data\")  # Replaces the old view\n</code></pre>"},{"location":"user-guide/sql-queries/#basic-sql-queries","title":"Basic SQL Queries","text":""},{"location":"user-guide/sql-queries/#select","title":"SELECT","text":"<p>Select specific columns:</p> <pre><code>result = spark.sql(\"SELECT name, age, city FROM users\")\n</code></pre> <p>Select all columns:</p> <pre><code>result = spark.sql(\"SELECT * FROM users\")\n</code></pre>"},{"location":"user-guide/sql-queries/#where-clause","title":"WHERE Clause","text":"<p>Filter rows based on conditions:</p> <pre><code>result = spark.sql(\"SELECT * FROM users WHERE age &gt; 25\")\nresult = spark.sql(\"SELECT * FROM users WHERE city = 'New York'\")\nresult = spark.sql(\"SELECT * FROM users WHERE age &gt; 25 AND salary &gt; 70000\")\n</code></pre>"},{"location":"user-guide/sql-queries/#order-by","title":"ORDER BY","text":"<p>Sort results:</p> <pre><code># Ascending order (default)\nresult = spark.sql(\"SELECT * FROM users ORDER BY age\")\n\n# Descending order\nresult = spark.sql(\"SELECT * FROM users ORDER BY salary DESC\")\n\n# Multiple columns\nresult = spark.sql(\"SELECT * FROM users ORDER BY city, age DESC\")\n</code></pre>"},{"location":"user-guide/sql-queries/#limit","title":"LIMIT","text":"<p>Limit the number of rows:</p> <pre><code>result = spark.sql(\"SELECT * FROM users LIMIT 10\")\n</code></pre>"},{"location":"user-guide/sql-queries/#aggregations","title":"Aggregations","text":""},{"location":"user-guide/sql-queries/#count","title":"COUNT","text":"<p>Count rows:</p> <pre><code>result = spark.sql(\"SELECT COUNT(*) as total FROM users\")\n</code></pre>"},{"location":"user-guide/sql-queries/#group-by","title":"GROUP BY","text":"<p>Group and aggregate:</p> <pre><code>result = spark.sql(\"\"\"\n    SELECT city, COUNT(*) as count\n    FROM users\n    GROUP BY city\n\"\"\")\n</code></pre>"},{"location":"user-guide/sql-queries/#multiple-aggregations","title":"Multiple Aggregations","text":"<p>Use multiple aggregate functions:</p> <pre><code>result = spark.sql(\"\"\"\n    SELECT\n        city,\n        COUNT(*) as total,\n        AVG(age) as avg_age,\n        MIN(salary) as min_salary,\n        MAX(salary) as max_salary,\n        SUM(salary) as total_salary\n    FROM users\n    GROUP BY city\n\"\"\")\n</code></pre>"},{"location":"user-guide/sql-queries/#having-clause","title":"HAVING Clause","text":"<p>Filter groups after aggregation:</p> <pre><code>result = spark.sql(\"\"\"\n    SELECT city, COUNT(*) as count\n    FROM users\n    GROUP BY city\n    HAVING COUNT(*) &gt; 5\n\"\"\")\n</code></pre>"},{"location":"user-guide/sql-queries/#joins","title":"Joins","text":"<p>You can perform joins using SQL syntax:</p>"},{"location":"user-guide/sql-queries/#inner-join","title":"INNER JOIN","text":"<pre><code># Register both DataFrames\nemployees.createOrReplaceTempView(\"employees\")\ndepartments.createOrReplaceTempView(\"departments\")\n\n# Join them\nresult = spark.sql(\"\"\"\n    SELECT e.name, e.age, d.dept_name\n    FROM employees e\n    INNER JOIN departments d ON e.dept_id = d.dept_id\n\"\"\")\n</code></pre>"},{"location":"user-guide/sql-queries/#left-join","title":"LEFT JOIN","text":"<pre><code>result = spark.sql(\"\"\"\n    SELECT e.name, d.dept_name\n    FROM employees e\n    LEFT JOIN departments d ON e.dept_id = d.dept_id\n\"\"\")\n</code></pre>"},{"location":"user-guide/sql-queries/#other-join-types","title":"Other Join Types","text":"<p>PyRust supports all standard SQL join types: - <code>INNER JOIN</code> - <code>LEFT JOIN</code> (or <code>LEFT OUTER JOIN</code>) - <code>RIGHT JOIN</code> (or <code>RIGHT OUTER JOIN</code>) - <code>FULL OUTER JOIN</code></p>"},{"location":"user-guide/sql-queries/#subqueries","title":"Subqueries","text":""},{"location":"user-guide/sql-queries/#subquery-in-where","title":"Subquery in WHERE","text":"<p>Filter based on a subquery result:</p> <pre><code>result = spark.sql(\"\"\"\n    SELECT name, age\n    FROM users\n    WHERE age &gt; (SELECT AVG(age) FROM users)\n\"\"\")\n</code></pre>"},{"location":"user-guide/sql-queries/#subquery-in-from","title":"Subquery in FROM","text":"<p>Use a subquery as a data source:</p> <pre><code>result = spark.sql(\"\"\"\n    SELECT city, avg_salary\n    FROM (\n        SELECT city, AVG(salary) as avg_salary\n        FROM users\n        GROUP BY city\n    ) AS city_stats\n    WHERE avg_salary &gt; 70000\n\"\"\")\n</code></pre>"},{"location":"user-guide/sql-queries/#sql-functions","title":"SQL Functions","text":""},{"location":"user-guide/sql-queries/#string-functions","title":"String Functions","text":"<pre><code>result = spark.sql(\"\"\"\n    SELECT\n        name,\n        UPPER(name) as upper_name,\n        LOWER(city) as lower_city\n    FROM users\n\"\"\")\n</code></pre> <p>Common string functions: - <code>UPPER(col)</code> - Convert to uppercase - <code>LOWER(col)</code> - Convert to lowercase - <code>LENGTH(col)</code> - Get string length - <code>TRIM(col)</code> - Remove whitespace</p>"},{"location":"user-guide/sql-queries/#math-functions","title":"Math Functions","text":"<pre><code>result = spark.sql(\"\"\"\n    SELECT\n        age,\n        age * 2 as double_age,\n        age + 10 as age_plus_10\n    FROM users\n\"\"\")\n</code></pre>"},{"location":"user-guide/sql-queries/#case-expressions","title":"CASE Expressions","text":"<p>Conditional logic in SQL:</p> <pre><code>result = spark.sql(\"\"\"\n    SELECT\n        name,\n        age,\n        CASE\n            WHEN age &lt; 25 THEN 'Young'\n            WHEN age &lt; 35 THEN 'Middle'\n            ELSE 'Senior'\n        END as age_category\n    FROM users\n\"\"\")\n</code></pre>"},{"location":"user-guide/sql-queries/#mixing-sql-and-dataframe-operations","title":"Mixing SQL and DataFrame Operations","text":"<p>You can seamlessly mix SQL queries with DataFrame operations:</p>"},{"location":"user-guide/sql-queries/#sql-dataframe-operations","title":"SQL \u2192 DataFrame Operations","text":"<pre><code># Start with SQL\nresult = spark.sql(\"SELECT * FROM users WHERE age &gt; 25\")\n\n# Continue with DataFrame operations\nresult = result.filter(\"salary &gt; 70000\") \\\n               .orderBy(\"salary\") \\\n               .limit(10)\n</code></pre>"},{"location":"user-guide/sql-queries/#dataframe-operations-sql","title":"DataFrame Operations \u2192 SQL","text":"<pre><code># Start with DataFrame operations\nfiltered = df.filter(\"age &gt; 25\")\n\n# Register and query with SQL\nfiltered.createOrReplaceTempView(\"filtered_users\")\nresult = spark.sql(\"SELECT city, COUNT(*) FROM filtered_users GROUP BY city\")\n</code></pre>"},{"location":"user-guide/sql-queries/#error-handling","title":"Error Handling","text":"<p>PyRust raises <code>RuntimeError</code> for SQL errors:</p> <pre><code>try:\n    result = spark.sql(\"SELECT * FROM nonexistent_table\")\nexcept RuntimeError as e:\n    print(f\"SQL error: {e}\")\n</code></pre> <p>Common errors: - Table not found: Make sure you've called <code>createOrReplaceTempView()</code> first - Column not found: Check column names in your query - Syntax error: Verify your SQL syntax is correct</p>"},{"location":"user-guide/sql-queries/#performance-tips","title":"Performance Tips","text":"<ol> <li> <p>Filter early: Use WHERE clauses to reduce data before joins    <pre><code># Good\nspark.sql(\"SELECT * FROM users WHERE age &gt; 25\")\n\n# Less efficient for large datasets\nspark.sql(\"SELECT * FROM users\").filter(\"age &gt; 25\")\n</code></pre></p> </li> <li> <p>Use column selection: Select only needed columns    <pre><code># Good\nspark.sql(\"SELECT name, age FROM users\")\n\n# Unnecessary if you only need two columns\nspark.sql(\"SELECT * FROM users\")\n</code></pre></p> </li> <li> <p>Aggregate before joining: Reduce data size before expensive operations    <pre><code>spark.sql(\"\"\"\n    SELECT u.city, s.avg_salary\n    FROM users u\n    JOIN (\n        SELECT city, AVG(salary) as avg_salary\n        FROM salaries\n        GROUP BY city\n    ) s ON u.city = s.city\n\"\"\")\n</code></pre></p> </li> </ol>"},{"location":"user-guide/sql-queries/#examples","title":"Examples","text":"<p>See the complete SQL examples in <code>examples/sql_queries.py</code>:</p> <pre><code>python examples/sql_queries.py\n</code></pre> <p>This example demonstrates: - Basic queries (SELECT, WHERE, ORDER BY) - Aggregations (GROUP BY, HAVING) - CASE expressions - Subqueries - String and math functions - Mixing SQL with DataFrame operations - Replacing temporary views</p>"},{"location":"user-guide/sql-queries/#supported-sql-features","title":"Supported SQL Features","text":"<p>PyRust leverages DataFusion's SQL engine, which supports:</p> <ul> <li>\u2705 SELECT, WHERE, ORDER BY, LIMIT</li> <li>\u2705 GROUP BY, HAVING</li> <li>\u2705 Aggregations: COUNT, SUM, AVG, MIN, MAX</li> <li>\u2705 Joins: INNER, LEFT, RIGHT, FULL OUTER</li> <li>\u2705 Subqueries in SELECT, FROM, and WHERE</li> <li>\u2705 CASE expressions</li> <li>\u2705 String functions: UPPER, LOWER, TRIM, LENGTH</li> <li>\u2705 Math operators: +, -, *, /</li> <li>\u2705 Comparison operators: =, !=, &lt;, &gt;, &lt;=, &gt;=</li> <li>\u2705 Logical operators: AND, OR, NOT</li> <li>\u2705 Common SQL functions</li> </ul> <p>For advanced SQL features, refer to DataFusion's SQL documentation.</p>"},{"location":"user-guide/transformations/","title":"Transformations Guide","text":"<p>Master PyRust's transformation operations for powerful data manipulation.</p>"},{"location":"user-guide/transformations/#understanding-transformations","title":"Understanding Transformations","text":""},{"location":"user-guide/transformations/#lazy-evaluation","title":"Lazy Evaluation","text":"<p>Transformations don't execute immediately - they build a query plan:</p> <pre><code># None of these execute yet\ndf1 = df.filter(\"age &gt; 18\")        # Lazy\ndf2 = df1.select([\"name\", \"city\"]) # Lazy\ndf3 = df2.orderBy([\"name\"])        # Lazy\n\n# This triggers execution\ndf3.show()  # Action! Executes entire chain\n</code></pre> <p>Benefits: - Query optimization - Efficient execution planning - Reduced memory usage</p>"},{"location":"user-guide/transformations/#transformations-vs-actions","title":"Transformations vs Actions","text":"Transformations (Lazy) Actions (Eager) <code>select()</code> <code>show()</code> <code>filter()</code> / <code>where_()</code> <code>count()</code> <code>orderBy()</code> / <code>sort()</code> <code>collect()</code> (future) <code>limit()</code> <code>groupBy()</code>"},{"location":"user-guide/transformations/#column-operations","title":"Column Operations","text":""},{"location":"user-guide/transformations/#selecting-columns","title":"Selecting Columns","text":"<pre><code># Select specific columns\ndf.select([\"name\", \"age\"])\n\n# Reorder columns\ndf.select([\"age\", \"name\", \"city\"])\n\n# Select all except (future feature)\n# df.select(df.columns.except([\"id\"]))\n</code></pre>"},{"location":"user-guide/transformations/#renaming-columns-future","title":"Renaming Columns (Future)","text":"<pre><code># Future: Column renaming\n# df.withColumnRenamed(\"old_name\", \"new_name\")\n</code></pre>"},{"location":"user-guide/transformations/#adding-columns-future","title":"Adding Columns (Future)","text":"<pre><code># Future: Computed columns\n# df.withColumn(\"age_plus_10\", col(\"age\") + 10)\n</code></pre>"},{"location":"user-guide/transformations/#row-operations","title":"Row Operations","text":""},{"location":"user-guide/transformations/#filtering","title":"Filtering","text":""},{"location":"user-guide/transformations/#simple-conditions","title":"Simple Conditions","text":"<pre><code># Numeric comparisons\ndf.filter(\"age &gt; 18\")\ndf.filter(\"salary &gt;= 50000\")\ndf.filter(\"score &lt; 100\")\n\n# String equality\ndf.filter(\"city == 'New York'\")\ndf.filter(\"status == 'active'\")\n\n# String inequality\ndf.filter(\"country != 'USA'\")\n</code></pre>"},{"location":"user-guide/transformations/#multiple-filters","title":"Multiple Filters","text":"<pre><code># Chain filters (implicit AND)\ndf.filter(\"age &gt; 18\") \\\n  .filter(\"age &lt; 65\") \\\n  .filter(\"salary &gt; 30000\")\n\n# Future: Combined conditions\n# df.filter(\"age &gt; 18 AND salary &gt; 30000\")\n# df.filter(\"city == 'NYC' OR city == 'LA'\")\n</code></pre>"},{"location":"user-guide/transformations/#sorting","title":"Sorting","text":"<pre><code># Sort ascending\ndf.orderBy([\"age\"])\n\n# Multiple columns (priority: left to right)\ndf.orderBy([\"country\", \"city\", \"name\"])\n\n# Future: Descending order\n# df.orderBy(col(\"age\").desc())\n</code></pre>"},{"location":"user-guide/transformations/#limiting","title":"Limiting","text":"<pre><code># First N rows\ndf.limit(10)\n\n# Top N (combine with sort)\ndf.orderBy([\"sales\"]).limit(5)\n</code></pre>"},{"location":"user-guide/transformations/#deduplication-future","title":"Deduplication (Future)","text":"<pre><code># Future: Remove duplicates\n# df.distinct()\n# df.dropDuplicates([\"email\"])\n</code></pre>"},{"location":"user-guide/transformations/#aggregations","title":"Aggregations","text":""},{"location":"user-guide/transformations/#simple-count","title":"Simple Count","text":"<pre><code># Count by group\ndf.groupBy([\"city\"]).count()\n\n# Multiple group columns\ndf.groupBy([\"country\", \"city\"]).count()\n</code></pre>"},{"location":"user-guide/transformations/#multiple-aggregations","title":"Multiple Aggregations","text":"<pre><code># Various aggregate functions\ndf.groupBy([\"department\"]).agg([\n    (\"salary\", \"sum\"),\n    (\"salary\", \"avg\"),\n    (\"salary\", \"min\"),\n    (\"salary\", \"max\"),\n    (\"employee_id\", \"count\")\n])\n</code></pre>"},{"location":"user-guide/transformations/#aggregation-functions","title":"Aggregation Functions","text":"Function Description Example Output Column <code>count</code> Count rows <code>count(column)</code> <code>sum</code> Sum values <code>sum(column)</code> <code>avg</code> Average <code>avg(column)</code> <code>mean</code> Average (alias) <code>avg(column)</code> <code>min</code> Minimum <code>min(column)</code> <code>max</code> Maximum <code>max(column)</code>"},{"location":"user-guide/transformations/#transformation-patterns","title":"Transformation Patterns","text":""},{"location":"user-guide/transformations/#pattern-1-filter-select-sort","title":"Pattern 1: Filter-Select-Sort","text":"<pre><code># Extract specific data\nresult = df.filter(\"status == 'active'\") \\\n           .select([\"name\", \"email\", \"signup_date\"]) \\\n           .orderBy([\"signup_date\"])\nresult.show()\n</code></pre>"},{"location":"user-guide/transformations/#pattern-2-aggregate-filter","title":"Pattern 2: Aggregate-Filter","text":"<pre><code># Group and filter groups\nsummary = df.groupBy([\"product\"]) \\\n            .agg([(\"quantity\", \"sum\")]) \\\n            .filter(\"sum(quantity) &gt; 1000\")\nsummary.show()\n</code></pre>"},{"location":"user-guide/transformations/#pattern-3-multiple-groupings","title":"Pattern 3: Multiple Groupings","text":"<pre><code># Hierarchical aggregation\nby_region = df.groupBy([\"region\"]).agg([(\"sales\", \"sum\")])\nby_city = df.groupBy([\"region\", \"city\"]).agg([(\"sales\", \"sum\")])\nby_store = df.groupBy([\"region\", \"city\", \"store\"]).agg([(\"sales\", \"sum\")])\n</code></pre>"},{"location":"user-guide/transformations/#pattern-4-top-n-per-group","title":"Pattern 4: Top N per Group","text":"<pre><code># Top 5 highest sales\ntop_sales = df.orderBy([\"amount\"]) \\\n              .limit(5)\n\n# Future: Top N per category\n# window = Window.partitionBy(\"category\").orderBy(col(\"sales\").desc())\n# df.withColumn(\"rank\", row_number().over(window)) \\\n#   .filter(\"rank &lt;= 5\")\n</code></pre>"},{"location":"user-guide/transformations/#optimization-strategies","title":"Optimization Strategies","text":""},{"location":"user-guide/transformations/#1-filter-pushdown","title":"1. Filter Pushdown","text":"<p>Push filters as early as possible:</p> <pre><code># Good - filter early\ndf.filter(\"year == 2024\") \\\n  .select([\"name\", \"amount\"]) \\\n  .groupBy([\"name\"]) \\\n  .agg([(\"amount\", \"sum\")])\n\n# Less efficient - filter late\ndf.groupBy([\"name\", \"year\"]) \\\n  .agg([(\"amount\", \"sum\")]) \\\n  .filter(\"year == 2024\")\n</code></pre>"},{"location":"user-guide/transformations/#2-column-pruning","title":"2. Column Pruning","text":"<p>Select only needed columns early:</p> <pre><code># Good - select early\ndf.select([\"id\", \"amount\", \"date\"]) \\\n  .filter(\"amount &gt; 1000\") \\\n  .groupBy([\"date\"]) \\\n  .agg([(\"amount\", \"sum\")])\n\n# Less efficient - all columns carried through\ndf.filter(\"amount &gt; 1000\") \\\n  .groupBy([\"date\"]) \\\n  .agg([(\"amount\", \"sum\")])\n</code></pre>"},{"location":"user-guide/transformations/#3-minimize-sorts","title":"3. Minimize Sorts","text":"<p>Sorting is expensive - only sort when necessary:</p> <pre><code># Good - sort only final result\ndf.filter(\"age &gt; 18\") \\\n  .groupBy([\"city\"]) \\\n  .count() \\\n  .orderBy([\"count\"])\n\n# Less efficient - unnecessary intermediate sort\ndf.orderBy([\"name\"]) \\\n  .filter(\"age &gt; 18\") \\\n  .groupBy([\"city\"]) \\\n  .count()\n</code></pre>"},{"location":"user-guide/transformations/#4-limit-early-for-exploration","title":"4. Limit Early for Exploration","text":"<pre><code># Quick exploration\ndf.limit(1000) \\\n  .filter(\"category == 'electronics'\") \\\n  .groupBy([\"brand\"]) \\\n  .count() \\\n  .show()\n</code></pre>"},{"location":"user-guide/transformations/#advanced-patterns","title":"Advanced Patterns","text":""},{"location":"user-guide/transformations/#conditional-aggregation-future","title":"Conditional Aggregation (Future)","text":"<pre><code># Future: Conditional sums\n# df.groupBy([\"product\"]).agg(\n#     sum(when(col(\"status\") == \"sold\", col(\"amount\"))).alias(\"sold_amount\"),\n#     sum(when(col(\"status\") == \"returned\", col(\"amount\"))).alias(\"returned_amount\")\n# )\n</code></pre>"},{"location":"user-guide/transformations/#window-functions-future","title":"Window Functions (Future)","text":"<pre><code># Future: Running totals, rankings\n# window = Window.orderBy(\"date\")\n# df.withColumn(\"running_total\", sum(\"amount\").over(window))\n</code></pre>"},{"location":"user-guide/transformations/#joins-future","title":"Joins (Future)","text":"<pre><code># Future: Join DataFrames\n# df1.join(df2, on=\"user_id\", how=\"inner\")\n# df1.join(df2, df1.id == df2.user_id, how=\"left\")\n</code></pre>"},{"location":"user-guide/transformations/#best-practices","title":"Best Practices","text":"<ol> <li>Filter Early, Often</li> <li>Apply filters before expensive operations</li> <li> <p>Reduces data volume throughout pipeline</p> </li> <li> <p>Select Only Needed Columns</p> </li> <li>Reduces memory usage and I/O</li> <li> <p>Especially important with wide tables</p> </li> <li> <p>Understand Lazy Evaluation</p> </li> <li>Transformations build a plan</li> <li>Actions trigger execution</li> <li> <p>Multiple actions re-execute the plan</p> </li> <li> <p>Use Appropriate Aggregations</p> </li> <li><code>count()</code> for counting</li> <li><code>sum()</code> for totals</li> <li><code>avg()</code> for means</li> <li> <p>Combine multiple in single <code>agg()</code> call</p> </li> <li> <p>Test on Subsets</p> </li> <li>Use <code>limit()</code> for quick validation</li> <li>Test transformations on small data first</li> </ol>"},{"location":"user-guide/transformations/#next-steps","title":"Next Steps","text":"<ul> <li>Performance Guide - Optimize transformations</li> <li>DataFrame API - Complete DataFrame guide</li> <li>API Reference - Method documentation</li> </ul>"}]}