{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#pyrust-documentation","title":"PyRust Documentation","text":"<p>Welcome to PyRust - a high-performance, Rust-based implementation of PySpark that delivers 10-50x faster data processing while maintaining API compatibility.</p>"},{"location":"#what-is-pyrust","title":"What is PyRust?","text":"<p>PyRust brings the power of Rust to Python data processing by leveraging:</p> <ul> <li>Apache DataFusion - High-performance query engine</li> <li>Apache Arrow - Columnar memory format</li> <li>PyO3 - Zero-copy Python bindings</li> </ul>"},{"location":"#key-features","title":"Key Features","text":""},{"location":"#high-performance","title":"\u26a1 High Performance","text":"<ul> <li>10-50x faster than PySpark for common operations</li> <li>Optimized columnar processing with vectorized operations</li> <li>Lazy evaluation with automatic query optimization</li> </ul>"},{"location":"#type-safe","title":"\ud83d\udd12 Type Safe","text":"<ul> <li>Leverages Rust's type system for safer data processing</li> <li>Compile-time guarantees prevent common runtime errors</li> <li>Strong schema validation</li> </ul>"},{"location":"#drop-in-replacement","title":"\ud83d\udd0c Drop-in Replacement","text":"<ul> <li>Familiar PySpark-like API</li> <li>Minimal code changes required</li> <li>Easy migration path</li> </ul>"},{"location":"#memory-efficient","title":"\ud83d\udcbe Memory Efficient","text":"<ul> <li>Apache Arrow's columnar format reduces memory usage</li> <li>Zero-copy data sharing between Python and Rust</li> <li>Efficient predicate pushdown and column pruning</li> </ul>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>from pyrust import SparkSession\n\n# Create a session\nspark = SparkSession.builder() \\\n    .appName(\"MyApp\") \\\n    .getOrCreate()\n\n# Read data\ndf = spark.read.csv(\"sales.csv\", header=True)\n\n# Transform and analyze\nresult = df.filter(\"amount &gt; 1000\") \\\n           .groupBy([\"region\"]) \\\n           .agg([(\"amount\", \"sum\"), (\"amount\", \"avg\")]) \\\n           .orderBy([\"sum(amount)\"])\n\n# Display results\nresult.show()\n</code></pre>"},{"location":"#performance-comparison","title":"Performance Comparison","text":"Operation PySpark PyRust Speedup CSV Read (1GB) 12.5s 1.2s 10.4x Filter + Select 8.3s 0.4s 20.8x GroupBy + Count 15.2s 0.8s 19.0x Complex Query 45.6s 2.1s 21.7x <p>Benchmarks run on: Intel Core i7, 16GB RAM, NVMe SSD</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Ready to try PyRust? Check out our guides:</p> <ul> <li>Installation Guide - Install PyRust on your system</li> <li>Quick Start Tutorial - Your first PyRust program</li> <li>Basic Operations - Common data operations</li> </ul>"},{"location":"#documentation-sections","title":"Documentation Sections","text":""},{"location":"#getting-started_1","title":"\ud83d\udcd6 Getting Started","text":"<p>Installation, setup, and your first PyRust program.</p>"},{"location":"#user-guide","title":"\ud83d\udcda User Guide","text":"<p>In-depth guides on DataFrames, transformations, and best practices.</p>"},{"location":"#api-reference","title":"\ud83d\udd27 API Reference","text":"<p>Complete API documentation for all classes and methods.</p>"},{"location":"#architecture","title":"\ud83c\udfd7\ufe0f Architecture","text":"<p>Deep dive into PyRust's internals and design decisions.</p>"},{"location":"#development","title":"\ud83d\udc68\u200d\ud83d\udcbb Development","text":"<p>Contributing guidelines, building from source, and testing.</p>"},{"location":"#community-support","title":"Community &amp; Support","text":"<ul> <li>GitHub: hadrien-chicault/PyRust</li> <li>Issues: Report bugs or request features</li> <li>\ud83e\udd80 Rust API Docs: Rust implementation documentation - Low-level internals</li> </ul>"},{"location":"#related-documentation","title":"Related Documentation","text":"<ul> <li>Rust API Reference - Detailed documentation of the Rust implementation</li> <li>Apache DataFusion - Query engine powering PyRust</li> <li>Apache Arrow - Columnar format for efficient data processing</li> </ul>"},{"location":"#license","title":"License","text":"<p>PyRust is open source software licensed under the Apache License 2.0.</p>"},{"location":"limitations/","title":"Current Limitations","text":"<p>PyRust is under active development. While it provides core DataFrame functionality, some features are not yet implemented.</p>"},{"location":"limitations/#overview","title":"Overview","text":"<p>This page documents known limitations and provides workarounds. For a complete roadmap of planned features, see ROADMAP.md.</p>"},{"location":"limitations/#major-limitations","title":"Major Limitations","text":""},{"location":"limitations/#1-column-expressions-not-supported","title":"1. Column Expressions Not Supported","text":"<p>What's missing: - Cannot use expressions in <code>select()</code> - No column arithmetic or transformations - No function calls on columns</p> <p>Examples that DON'T work: <pre><code>from pyrust.functions import col, mean  # Module doesn't exist yet\n\n# This will fail\ndf.select(col(\"price\") * col(\"quantity\"))\n\n# This will fail\ndf.select(mean(\"amount\").alias(\"avg_amount\"))\n\n# This will fail\ndf.select(col(\"name\").upper())\n</code></pre></p> <p>Workaround - Use SQL: <pre><code># Instead, use SQL\ndf.createOrReplaceTempView(\"data\")\nresult = spark.sql(\"SELECT price * quantity as total FROM data\")\n\n# Or for aggregations\nresult = spark.sql(\"SELECT AVG(amount) as avg_amount FROM data\")\n</code></pre></p> <p>Workaround - Use groupBy for aggregations: <pre><code># For simple aggregations\ndf.groupBy().agg((\"amount\", \"avg\"))\n</code></pre></p>"},{"location":"limitations/#2-no-withcolumn","title":"2. No withColumn()","text":"<p>What's missing: - Cannot add new columns with expressions - Cannot modify existing columns with transformations</p> <p>Examples that DON'T work: <pre><code># This will fail\ndf = df.withColumn(\"total\", col(\"price\") * col(\"quantity\"))\n\n# This will fail\ndf = df.withColumn(\"upper_name\", col(\"name\").upper())\n</code></pre></p> <p>Workaround - Use SQL: <pre><code>df.createOrReplaceTempView(\"sales\")\nresult = spark.sql(\"SELECT *, price * quantity as total FROM sales\")\n</code></pre></p> <p>Workaround - Use withColumnRenamed for simple cases: <pre><code># If you just need to rename\ndf = df.withColumnRenamed(\"old_name\", \"new_name\")\n</code></pre></p>"},{"location":"limitations/#3-no-window-functions","title":"3. No Window Functions","text":"<p>What's missing: - No <code>Window.partitionBy()</code> or <code>Window.orderBy()</code> - No ranking functions: <code>row_number()</code>, <code>rank()</code>, <code>dense_rank()</code> - No offset functions: <code>lag()</code>, <code>lead()</code> - No cumulative aggregations</p> <p>Examples that DON'T work: <pre><code>from pyrust.window import Window  # Doesn't exist\n\n# This will fail\nwindow = Window.partitionBy(\"category\").orderBy(\"date\")\ndf = df.select(\"*\", row_number().over(window).alias(\"row_num\"))\n</code></pre></p> <p>Workaround - Use SQL: <pre><code>df.createOrReplaceTempView(\"orders\")\nresult = spark.sql(\"\"\"\n    SELECT *,\n           ROW_NUMBER() OVER (PARTITION BY customer_id ORDER BY order_date) as row_num,\n           LAG(amount, 1) OVER (PARTITION BY customer_id ORDER BY order_date) as prev_amount\n    FROM orders\n\"\"\")\n</code></pre></p>"},{"location":"limitations/#4-no-type-casting","title":"4. No Type Casting","text":"<p>What's missing: - No <code>cast()</code> function - Cannot change column data types - Schema is determined at read time only</p> <p>Examples that DON'T work: <pre><code># This will fail\ndf.select(col(\"age\").cast(\"string\"))\n</code></pre></p> <p>Workaround - Use SQL: <pre><code>df.createOrReplaceTempView(\"data\")\nresult = spark.sql(\"SELECT CAST(age AS STRING) as age_str FROM data\")\n</code></pre></p>"},{"location":"limitations/#5-limited-aggregation-functions","title":"5. Limited Aggregation Functions","text":"<p>What's available: - \u2705 count, sum, avg, min, max</p> <p>What's missing: - stddev, variance - percentile, median - first, last - collect_list, collect_set</p> <p>Workaround - Use SQL: <pre><code>result = spark.sql(\"\"\"\n    SELECT category,\n           STDDEV(price) as price_stddev,\n           PERCENTILE(price, 0.5) as price_median\n    FROM products\n    GROUP BY category\n\"\"\")\n</code></pre></p>"},{"location":"limitations/#6-no-stringdate-functions","title":"6. No String/Date Functions","text":"<p>What's missing: - String: <code>upper()</code>, <code>lower()</code>, <code>substring()</code>, <code>concat()</code>, <code>trim()</code> - Date: <code>year()</code>, <code>month()</code>, <code>day()</code>, <code>date_format()</code> - Math: <code>round()</code>, <code>ceil()</code>, <code>floor()</code>, <code>abs()</code></p> <p>Workaround - Use SQL: <pre><code>result = spark.sql(\"\"\"\n    SELECT UPPER(name) as name_upper,\n           SUBSTRING(description, 1, 10) as desc_short,\n           YEAR(date) as year\n    FROM data\n\"\"\")\n</code></pre></p>"},{"location":"limitations/#7-no-write-operations","title":"7. No Write Operations","text":"<p>What's missing: - Cannot save DataFrames to files - No <code>df.write.csv()</code> or <code>df.write.parquet()</code></p> <p>Workaround: <pre><code># Collect to Python and save manually\nimport csv\n\ndata = []\n# Note: Collecting large datasets not recommended\n# This is just for small results\nresult.show()  # Display instead\n</code></pre></p>"},{"location":"limitations/#8-no-cachingpersistence","title":"8. No Caching/Persistence","text":"<p>What's missing: - No <code>cache()</code> or <code>persist()</code> - DataFrames are recomputed each time</p> <p>Workaround: - Keep queries simple - Avoid reusing expensive computations - Consider using SQL CTEs for complex queries</p>"},{"location":"limitations/#9-no-complex-join-conditions","title":"9. No Complex Join Conditions","text":"<p>What's available: - \u2705 Equality joins on column names</p> <p>What's missing: - Non-equality joins - Complex join expressions</p> <p>Examples that DON'T work: <pre><code># This will fail\ndf1.join(df2, df1[\"price\"] &gt; df2[\"min_price\"])\n</code></pre></p> <p>Workaround - Use SQL: <pre><code>df1.createOrReplaceTempView(\"products\")\ndf2.createOrReplaceTempView(\"ranges\")\nresult = spark.sql(\"\"\"\n    SELECT p.*, r.category\n    FROM products p\n    JOIN ranges r ON p.price &gt; r.min_price AND p.price &lt;= r.max_price\n\"\"\")\n</code></pre></p>"},{"location":"limitations/#10-select-only-accepts-strings","title":"10. Select Only Accepts Strings","text":"<p>What's available: <pre><code># This works\ndf.select(\"name\", \"age\", \"city\")\n</code></pre></p> <p>What doesn't work: <pre><code># This fails\ndf.select(col(\"name\"), col(\"age\"))\n\n# This fails\ndf.select([col(\"price\") * col(\"qty\")])\n</code></pre></p> <p>Workaround: <pre><code># Use SQL for transformations\ndf.createOrReplaceTempView(\"data\")\nresult = spark.sql(\"SELECT name, age, price * qty as total FROM data\")\n\n# Or use separate operations\ndf.select(\"name\", \"age\").show()\n</code></pre></p>"},{"location":"limitations/#general-workaround-use-sql","title":"General Workaround: Use SQL","text":"<p>For most limitations, SQL provides a powerful workaround:</p> <pre><code>from pyrust import SparkSession\n\nspark = SparkSession.builder.appName(\"SQLWorkaround\").getOrCreate()\n\n# Load data\ndf = spark.read.csv(\"data.csv\", header=True)\n\n# Register as temp view\ndf.createOrReplaceTempView(\"mydata\")\n\n# Use SQL for complex operations\nresult = spark.sql(\"\"\"\n    SELECT\n        customer_id,\n        UPPER(name) as name_upper,\n        price * quantity as total,\n        AVG(price) OVER (PARTITION BY category ORDER BY date) as running_avg,\n        ROW_NUMBER() OVER (PARTITION BY customer_id ORDER BY date DESC) as recent_order\n    FROM mydata\n    WHERE price &gt; 100\n\"\"\")\n\nresult.show()\n</code></pre>"},{"location":"limitations/#feature-priorities","title":"Feature Priorities","text":"<p>See ROADMAP.md for: - Complete list of planned features - Timeline estimates - How to contribute</p> <p>High Priority: 1. Expression system (<code>col()</code>, <code>lit()</code>, operators) 2. <code>withColumn()</code> for column transformations 3. Type casting (<code>cast()</code>) 4. Basic column functions (string, math, date)</p> <p>Medium Priority: 5. Window functions 6. Write operations 7. Advanced aggregations (pivot, rollup) 8. UDFs</p>"},{"location":"limitations/#contributing","title":"Contributing","text":"<p>Want to help implement these features? See CONTRIBUTING.md.</p>"},{"location":"limitations/#questions","title":"Questions?","text":"<ul> <li>Check existing Issues</li> <li>Start a Discussion</li> <li>Read the User Guide</li> </ul>"},{"location":"api/context/","title":"SparkSession API Reference","text":"<p>API reference for SparkSession (context) operations.</p>"},{"location":"api/context/#sparksession","title":"SparkSession","text":"<p>Main entry point for PyRust applications.</p>"},{"location":"api/context/#creating-a-session","title":"Creating a Session","text":""},{"location":"api/context/#builder","title":"builder","text":"<p>Create a new SparkSession builder.</p> <p>Returns: SparkSession.Builder</p> <p>Example: <pre><code>from pyrust import SparkSession\n\nspark = SparkSession.builder \\\n    .appName(\"MyApp\") \\\n    .getOrCreate()\n</code></pre></p>"},{"location":"api/context/#builder-methods","title":"Builder Methods","text":""},{"location":"api/context/#appnamename","title":"appName(name)","text":"<p>Set application name.</p> <p>Parameters: - <code>name</code> (str): Application name</p> <p>Returns: Builder</p> <p>Example: <pre><code>builder.appName(\"DataProcessing\")\n</code></pre></p>"},{"location":"api/context/#mastermaster_url","title":"master(master_url)","text":"<p>Set master URL (for future distributed support).</p> <p>Parameters: - <code>master_url</code> (str): Master URL</p> <p>Returns: Builder</p> <p>Example: <pre><code>builder.master(\"local[*]\")\n</code></pre></p>"},{"location":"api/context/#getorcreate","title":"getOrCreate()","text":"<p>Get existing session or create new one.</p> <p>Returns: SparkSession</p> <p>Example: <pre><code>spark = SparkSession.builder.appName(\"App\").getOrCreate()\n</code></pre></p>"},{"location":"api/context/#reading-data","title":"Reading Data","text":""},{"location":"api/context/#read","title":"read","text":"<p>Access DataFrameReader for loading data.</p> <p>Returns: DataFrameReader</p> <p>Example: <pre><code>df = spark.read.csv(\"data.csv\", header=True)\n</code></pre></p>"},{"location":"api/context/#dataframereader-methods","title":"DataFrameReader Methods","text":""},{"location":"api/context/#csvpath-headerfalse","title":"csv(path, header=False)","text":"<p>Read CSV file.</p> <p>Parameters: - <code>path</code> (str): File path - <code>header</code> (bool): Whether first row is header</p> <p>Returns: DataFrame</p> <p>Example: <pre><code>df = spark.read.csv(\"users.csv\", header=True)\n</code></pre></p>"},{"location":"api/context/#parquetpath","title":"parquet(path)","text":"<p>Read Parquet file.</p> <p>Parameters: - <code>path</code> (str): File path</p> <p>Returns: DataFrame</p> <p>Example: <pre><code>df = spark.read.parquet(\"data.parquet\")\n</code></pre></p>"},{"location":"api/context/#sql-operations","title":"SQL Operations","text":""},{"location":"api/context/#sqlquery","title":"sql(query)","text":"<p>Execute SQL query.</p> <p>Parameters: - <code>query</code> (str): SQL query string</p> <p>Returns: DataFrame</p> <p>Example: <pre><code>result = spark.sql(\"SELECT * FROM users WHERE age &gt; 18\")\n</code></pre></p>"},{"location":"api/context/#register_temp_viewdf-name","title":"register_temp_view(df, name)","text":"<p>Register DataFrame as temporary view (internal method).</p> <p>Parameters: - <code>df</code> (DataFrame): DataFrame to register - <code>name</code> (str): View name</p> <p>Returns: None</p>"},{"location":"api/context/#session-management","title":"Session Management","text":""},{"location":"api/context/#stop","title":"stop()","text":"<p>Stop the SparkSession.</p> <p>Returns: None</p> <p>Example: <pre><code>spark.stop()\n</code></pre></p>"},{"location":"api/context/#complete-example","title":"Complete Example","text":"<pre><code>from pyrust import SparkSession\n\n# Create session\nspark = SparkSession.builder \\\n    .appName(\"DataAnalysis\") \\\n    .getOrCreate()\n\n# Read data\ndf = spark.read.csv(\"sales.csv\", header=True)\n\n# Register for SQL\ndf.createOrReplaceTempView(\"sales\")\n\n# Execute SQL\nresult = spark.sql(\"\"\"\n    SELECT product, SUM(amount) as total\n    FROM sales\n    GROUP BY product\n\"\"\")\n\nresult.show()\n\n# Clean up\nspark.stop()\n</code></pre>"},{"location":"api/context/#see-also","title":"See Also","text":"<ul> <li>User Guide - Data Loading</li> <li>User Guide - SQL Queries</li> <li>DataFrame API</li> </ul>"},{"location":"api/dataframe/","title":"DataFrame API Reference","text":"<p>Complete API reference for PyRust DataFrame operations.</p>"},{"location":"api/dataframe/#core-methods","title":"Core Methods","text":""},{"location":"api/dataframe/#creation-loading","title":"Creation &amp; Loading","text":"<p>DataFrames are typically created through <code>SparkSession.read</code>:</p> <pre><code>df = spark.read.csv(\"data.csv\", header=True)\ndf = spark.read.parquet(\"data.parquet\")\n</code></pre>"},{"location":"api/dataframe/#selectcols","title":"select(*cols)","text":"<p>Select columns from the DataFrame.</p> <p>Parameters: - <code>*cols</code> (str): Column names to select</p> <p>Returns: DataFrame</p> <p>Example: <pre><code>df.select(\"name\", \"age\", \"city\")\n</code></pre></p>"},{"location":"api/dataframe/#filtercondition-wherecondition","title":"filter(condition) / where(condition)","text":"<p>Filter rows based on a condition.</p> <p>Parameters: - <code>condition</code> (str): SQL-like filter expression</p> <p>Returns: DataFrame</p> <p>Example: <pre><code>df.filter(\"age &gt; 18\")\ndf.where(\"salary &gt;= 50000\")\n</code></pre></p>"},{"location":"api/dataframe/#groupbycols","title":"groupBy(*cols)","text":"<p>Group DataFrame by specified columns.</p> <p>Parameters: - <code>*cols</code> (str): Column names to group by</p> <p>Returns: GroupedData</p> <p>Example: <pre><code>df.groupBy(\"city\").count()\ndf.groupBy(\"department\", \"year\").agg((\"salary\", \"avg\"))\n</code></pre></p>"},{"location":"api/dataframe/#joinother-on-howinner","title":"join(other, on, how=\"inner\")","text":"<p>Join with another DataFrame.</p> <p>Parameters: - <code>other</code> (DataFrame): DataFrame to join with - <code>on</code> (str or list): Column name(s) to join on - <code>how</code> (str): Join type - \"inner\", \"left\", \"right\", \"outer\", \"semi\", \"anti\"</p> <p>Returns: DataFrame</p> <p>Example: <pre><code>df1.join(df2, on=\"user_id\")\ndf1.join(df2, on=[\"country\", \"city\"], how=\"left\")\n</code></pre></p> <p>See Joins Guide for detailed documentation.</p>"},{"location":"api/dataframe/#orderbycols-sortcols","title":"orderBy(cols) / sort(cols)","text":"<p>Sort DataFrame by specified columns.</p> <p>Parameters: - <code>*cols</code> (str): Column names to sort by</p> <p>Returns: DataFrame</p> <p>Example: <pre><code>df.orderBy(\"age\")\ndf.sort(\"department\", \"salary\")\n</code></pre></p>"},{"location":"api/dataframe/#distinct","title":"distinct()","text":"<p>Remove duplicate rows.</p> <p>Returns: DataFrame</p> <p>Example: <pre><code>df.distinct()\n</code></pre></p>"},{"location":"api/dataframe/#dropduplicatessubsetnone","title":"dropDuplicates(subset=None)","text":"<p>Remove duplicates based on specified columns.</p> <p>Parameters: - <code>subset</code> (list, optional): Column names to consider. If None, uses all columns.</p> <p>Returns: DataFrame</p> <p>Aliases: <code>drop_duplicates()</code></p> <p>Example: <pre><code>df.dropDuplicates([\"user_id\"])\ndf.drop_duplicates([\"email\"])\n</code></pre></p>"},{"location":"api/dataframe/#unionother-unionallother","title":"union(other) / unionAll(other)","text":"<p>Combine DataFrames vertically (append rows).</p> <p>Parameters: - <code>other</code> (DataFrame): DataFrame to union with</p> <p>Returns: DataFrame</p> <p>Example: <pre><code>df1.union(df2)\ndf1.unionAll(df2)  # Same as union()\n</code></pre></p>"},{"location":"api/dataframe/#intersectother","title":"intersect(other)","text":"<p>Return rows that exist in both DataFrames.</p> <p>Parameters: - <code>other</code> (DataFrame): DataFrame to intersect with</p> <p>Returns: DataFrame</p> <p>Example: <pre><code>df1.intersect(df2)\n</code></pre></p>"},{"location":"api/dataframe/#exceptallother-subtractother","title":"exceptAll(other) / subtract(other)","text":"<p>Return rows in this DataFrame but not in other.</p> <p>Parameters: - <code>other</code> (DataFrame): DataFrame to subtract</p> <p>Returns: DataFrame</p> <p>Example: <pre><code>df1.exceptAll(df2)\ndf1.subtract(df2)  # Alias\n</code></pre></p>"},{"location":"api/dataframe/#withcolumnrenamedexisting-new","title":"withColumnRenamed(existing, new)","text":"<p>Rename a column.</p> <p>Parameters: - <code>existing</code> (str): Current column name - <code>new</code> (str): New column name</p> <p>Returns: DataFrame</p> <p>Aliases: <code>with_column_renamed()</code></p> <p>Example: <pre><code>df.withColumnRenamed(\"old_name\", \"new_name\")\n</code></pre></p>"},{"location":"api/dataframe/#limitn","title":"limit(n)","text":"<p>Limit number of rows returned.</p> <p>Parameters: - <code>n</code> (int): Maximum number of rows</p> <p>Returns: DataFrame</p> <p>Example: <pre><code>df.limit(10)\n</code></pre></p>"},{"location":"api/dataframe/#count","title":"count()","text":"<p>Count total number of rows.</p> <p>Returns: int</p> <p>Example: <pre><code>total = df.count()\n</code></pre></p>"},{"location":"api/dataframe/#shown20-truncatetrue","title":"show(n=20, truncate=True)","text":"<p>Display DataFrame contents.</p> <p>Parameters: - <code>n</code> (int): Number of rows to show - <code>truncate</code> (bool): Whether to truncate long strings</p> <p>Returns: None</p> <p>Example: <pre><code>df.show()\ndf.show(5)\n</code></pre></p>"},{"location":"api/dataframe/#printschema","title":"printSchema()","text":"<p>Print DataFrame schema.</p> <p>Returns: None</p> <p>Example: <pre><code>df.printSchema()\n</code></pre></p>"},{"location":"api/dataframe/#createorreplacetempviewname","title":"createOrReplaceTempView(name)","text":"<p>Register DataFrame as a temporary SQL view.</p> <p>Parameters: - <code>name</code> (str): View name</p> <p>Returns: None</p> <p>Example: <pre><code>df.createOrReplaceTempView(\"users\")\nspark.sql(\"SELECT * FROM users WHERE age &gt; 18\")\n</code></pre></p>"},{"location":"api/dataframe/#groupeddata-methods","title":"GroupedData Methods","text":"<p>Returned by <code>groupBy()</code>.</p>"},{"location":"api/dataframe/#count_1","title":"count()","text":"<p>Count rows in each group.</p> <p>Returns: DataFrame</p> <p>Example: <pre><code>df.groupBy(\"city\").count()\n</code></pre></p>"},{"location":"api/dataframe/#aggexprs","title":"agg(*exprs)","text":"<p>Perform aggregations on grouped data.</p> <p>Parameters: - <code>*exprs</code> (tuple): Tuples of (column, function) - \"sum\", \"avg\", \"min\", \"max\", \"count\"</p> <p>Returns: DataFrame</p> <p>Example: <pre><code>df.groupBy(\"department\").agg((\"salary\", \"avg\"), (\"age\", \"max\"))\n</code></pre></p>"},{"location":"api/dataframe/#sumcols","title":"sum(*cols)","text":"<p>Sum of specified columns per group.</p> <p>Parameters: - <code>*cols</code> (str): Column names</p> <p>Returns: DataFrame</p> <p>Example: <pre><code>df.groupBy(\"category\").sum(\"price\", \"quantity\")\n</code></pre></p>"},{"location":"api/dataframe/#avgcols","title":"avg(*cols)","text":"<p>Average of specified columns per group.</p> <p>Parameters: - <code>*cols</code> (str): Column names</p> <p>Returns: DataFrame</p> <p>Example: <pre><code>df.groupBy(\"department\").avg(\"salary\")\n</code></pre></p>"},{"location":"api/dataframe/#mincols","title":"min(*cols)","text":"<p>Minimum of specified columns per group.</p> <p>Parameters: - <code>*cols</code> (str): Column names</p> <p>Returns: DataFrame</p>"},{"location":"api/dataframe/#maxcols","title":"max(*cols)","text":"<p>Maximum of specified columns per group.</p> <p>Parameters: - <code>*cols</code> (str): Column names</p> <p>Returns: DataFrame</p>"},{"location":"api/dataframe/#see-also","title":"See Also","text":"<ul> <li>User Guide - DataFrame</li> <li>User Guide - Joins</li> <li>User Guide - Data Operations</li> <li>Limitations</li> </ul>"},{"location":"api/operations/","title":"Operations Reference","text":"<p>Quick reference for common DataFrame operations.</p>"},{"location":"api/operations/#transformations-lazy","title":"Transformations (Lazy)","text":"<p>Transformations create new DataFrames without executing immediately.</p> Operation Method Example Select columns <code>select()</code> <code>df.select(\"name\", \"age\")</code> Filter rows <code>filter()</code>, <code>where()</code> <code>df.filter(\"age &gt; 18\")</code> Sort <code>orderBy()</code>, <code>sort()</code> <code>df.orderBy(\"name\")</code> Limit rows <code>limit()</code> <code>df.limit(10)</code> Remove duplicates <code>distinct()</code> <code>df.distinct()</code> Drop duplicates by cols <code>dropDuplicates()</code> <code>df.dropDuplicates([\"id\"])</code> Rename column <code>withColumnRenamed()</code> <code>df.withColumnRenamed(\"old\", \"new\")</code>"},{"location":"api/operations/#actions-eager","title":"Actions (Eager)","text":"<p>Actions trigger execution and return results.</p> Operation Method Returns Example Count rows <code>count()</code> int <code>df.count()</code> Display data <code>show()</code> None <code>df.show(10)</code> Show schema <code>printSchema()</code> None <code>df.printSchema()</code>"},{"location":"api/operations/#aggregations","title":"Aggregations","text":"<p>Group and aggregate data.</p> Operation Method Example Group by <code>groupBy()</code> <code>df.groupBy(\"city\")</code> Count per group <code>.count()</code> <code>df.groupBy(\"city\").count()</code> Multiple aggs <code>.agg()</code> <code>df.groupBy(\"dept\").agg((\"salary\", \"avg\"))</code> Sum <code>.sum()</code> <code>df.groupBy(\"category\").sum(\"amount\")</code> Average <code>.avg()</code> <code>df.groupBy(\"dept\").avg(\"salary\")</code> Min/Max <code>.min()</code>, <code>.max()</code> <code>df.groupBy(\"product\").max(\"price\")</code>"},{"location":"api/operations/#joins","title":"Joins","text":"<p>Combine DataFrames.</p> Join Type Method Example Inner <code>join()</code> <code>df1.join(df2, on=\"id\")</code> Left outer <code>join(how=\"left\")</code> <code>df1.join(df2, on=\"id\", how=\"left\")</code> Right outer <code>join(how=\"right\")</code> <code>df1.join(df2, on=\"id\", how=\"right\")</code> Full outer <code>join(how=\"outer\")</code> <code>df1.join(df2, on=\"id\", how=\"outer\")</code> Semi <code>join(how=\"semi\")</code> <code>df1.join(df2, on=\"id\", how=\"semi\")</code> Anti <code>join(how=\"anti\")</code> <code>df1.join(df2, on=\"id\", how=\"anti\")</code>"},{"location":"api/operations/#set-operations","title":"Set Operations","text":"<p>Combine DataFrames as sets.</p> Operation Method Example Union <code>union()</code>, <code>unionAll()</code> <code>df1.union(df2)</code> Intersection <code>intersect()</code> <code>df1.intersect(df2)</code> Difference <code>exceptAll()</code>, <code>subtract()</code> <code>df1.exceptAll(df2)</code>"},{"location":"api/operations/#sql-operations","title":"SQL Operations","text":"<p>Execute SQL queries.</p> Operation Method Example Register view <code>createOrReplaceTempView()</code> <code>df.createOrReplaceTempView(\"users\")</code> Execute SQL <code>spark.sql()</code> <code>spark.sql(\"SELECT * FROM users\")</code>"},{"location":"api/operations/#common-patterns","title":"Common Patterns","text":""},{"location":"api/operations/#filter-and-aggregate","title":"Filter and Aggregate","text":"<pre><code>df.filter(\"age &gt; 25\") \\\n  .groupBy(\"city\") \\\n  .agg((\"salary\", \"avg\")) \\\n  .orderBy(\"city\")\n</code></pre>"},{"location":"api/operations/#join-and-select","title":"Join and Select","text":"<pre><code>df1.join(df2, on=\"user_id\") \\\n   .select(\"name\", \"age\", \"order_total\")\n</code></pre>"},{"location":"api/operations/#complex-chain","title":"Complex Chain","text":"<pre><code>df.filter(\"status = 'active'\") \\\n  .join(details, on=\"id\", how=\"left\") \\\n  .groupBy(\"category\") \\\n  .count() \\\n  .orderBy(\"count\") \\\n  .limit(10) \\\n  .show()\n</code></pre>"},{"location":"api/operations/#union-and-deduplicate","title":"Union and Deduplicate","text":"<pre><code>df1.union(df2) \\\n   .distinct() \\\n   .orderBy(\"date\")\n</code></pre>"},{"location":"api/operations/#see-also","title":"See Also","text":"<ul> <li>DataFrame API - Detailed API reference</li> <li>SparkSession API - Session and reader methods</li> <li>User Guide - Complete tutorials</li> </ul>"},{"location":"architecture/overview/","title":"Architecture Overview","text":"<p>PyRust is built on a modern, high-performance architecture leveraging the best of Rust and Python ecosystems.</p>"},{"location":"architecture/overview/#high-level-architecture","title":"High-Level Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502          Python Application Layer           \u2502\n\u2502         (PySpark-compatible API)            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                  \u2502 PyO3 Bindings\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502           Rust Execution Layer              \u2502\n\u2502                                             \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502   PyRust     \u2502      \u2502   DataFusion    \u2502 \u2502\n\u2502  \u2502   Wrappers   \u2502\u2500\u2500\u2500\u2500\u2500\u25b6\u2502  Query Engine   \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                  \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502          Apache Arrow Layer                 \u2502\n\u2502       (Columnar Memory Format)              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/overview/#key-components","title":"Key Components","text":""},{"location":"architecture/overview/#1-python-api-layer","title":"1. Python API Layer","text":"<p>Location: <code>python/pyrust/</code></p> <p>The Python API provides a PySpark-compatible interface:</p> <ul> <li><code>SparkSession</code> - Session management</li> <li><code>DataFrame</code> - Main data structure</li> <li><code>GroupedData</code> - Aggregation interface</li> <li><code>DataFrameReader</code> - Data loading</li> </ul> <p>Technology: Pure Python with PyO3 bindings</p>"},{"location":"architecture/overview/#2-rust-core","title":"2. Rust Core","text":"<p>Location: <code>src/</code></p> <p>The Rust core handles all computation:</p> <ul> <li>PyO3 Wrappers (<code>src/dataframe/mod.rs</code>) - Python\u2194Rust bridge</li> <li>Query Execution - Powered by DataFusion</li> <li>Memory Management - Safe, zero-copy operations</li> </ul> <p>Technology: Rust, DataFusion, Arrow</p>"},{"location":"architecture/overview/#3-apache-arrow","title":"3. Apache Arrow","text":"<p>DataFusion uses Apache Arrow for:</p> <ul> <li>Columnar Storage - Efficient memory layout</li> <li>Zero-Copy - Data shared between Python and Rust</li> <li>Vectorization - SIMD operations for speed</li> </ul>"},{"location":"architecture/overview/#data-flow","title":"Data Flow","text":""},{"location":"architecture/overview/#query-execution","title":"Query Execution","text":"<pre><code>1. Python API Call\n   df.filter(\"age &gt; 18\").select(\"name\")\n        \u2193\n2. PyO3 Binding\n   Rust receives DataFrame + operations\n        \u2193\n3. DataFusion Query Plan\n   Builds logical plan \u2192 optimizes\n        \u2193\n4. Physical Execution\n   Executes on Arrow data\n        \u2193\n5. Return to Python\n   Arrow \u2192 Python via zero-copy\n</code></pre>"},{"location":"architecture/overview/#example-flow","title":"Example Flow","text":"<pre><code># 1. Python call\ndf = spark.read.csv(\"data.csv\")\nresult = df.filter(\"age &gt; 18\").select(\"name\", \"city\")\n\n# 2. Rust receives:\n# - CSV path\n# - Filter expression: \"age &gt; 18\"\n# - Columns: [\"name\", \"city\"]\n\n# 3. DataFusion:\n# - Reads CSV to Arrow\n# - Applies filter (predicate pushdown)\n# - Projects columns (column pruning)\n# - Returns Arrow batches\n\n# 4. Python receives Arrow data (zero-copy)\n</code></pre>"},{"location":"architecture/overview/#design-principles","title":"Design Principles","text":""},{"location":"architecture/overview/#1-lazy-evaluation","title":"1. Lazy Evaluation","text":"<p>Transformations are lazy - they build a plan without executing:</p> <pre><code># These don't execute yet\ndf2 = df.filter(\"age &gt; 18\")\ndf3 = df2.select(\"name\")\n\n# Action triggers execution\ndf3.show()  # Now it executes\n</code></pre>"},{"location":"architecture/overview/#2-immutability","title":"2. Immutability","text":"<p>DataFrames are immutable - operations return new DataFrames:</p> <pre><code>df1 = spark.read.csv(\"data.csv\")\ndf2 = df1.filter(\"age &gt; 18\")  # df1 unchanged\n</code></pre>"},{"location":"architecture/overview/#3-zero-copy","title":"3. Zero-Copy","text":"<p>Data is shared between Python and Rust without copying:</p> <ul> <li>Arrow format in Rust</li> <li>Same Arrow format in Python</li> <li>No serialization overhead</li> </ul>"},{"location":"architecture/overview/#4-type-safety","title":"4. Type Safety","text":"<p>Rust's type system prevents many runtime errors:</p> <ul> <li>Schema validation at compile time</li> <li>Safe memory management</li> <li>No null pointer errors</li> </ul>"},{"location":"architecture/overview/#performance-characteristics","title":"Performance Characteristics","text":""},{"location":"architecture/overview/#strengths","title":"Strengths","text":"<ol> <li>Columnar Processing - Efficient cache usage</li> <li>Vectorization - SIMD operations</li> <li>No GC Pauses - Rust's ownership model</li> <li>Predicate Pushdown - Filter early</li> <li>Column Pruning - Read only needed columns</li> </ol>"},{"location":"architecture/overview/#comparison","title":"Comparison","text":"Operation PySpark PyRust Speedup CSV Read 1.0x 2-3x 2-3x faster Filter 1.0x 3-5x 3-5x faster GroupBy 1.0x 2-4x 2-4x faster Join 1.0x 2-3x 2-3x faster <p>Benchmarks on CPU-bound operations with medium datasets</p>"},{"location":"architecture/overview/#memory-model","title":"Memory Model","text":""},{"location":"architecture/overview/#arrow-columnar-format","title":"Arrow Columnar Format","text":"<pre><code>Traditional Row Format:\n[{id:1, name:\"Alice\", age:25}, {id:2, name:\"Bob\", age:30}, ...]\n\nArrow Column Format:\nids:   [1, 2, 3, ...]\nnames: [\"Alice\", \"Bob\", \"Charlie\", ...]\nages:  [25, 30, 35, ...]\n</code></pre> <p>Benefits: - Better cache locality - Vectorized operations - Compression-friendly</p>"},{"location":"architecture/overview/#reference-counting","title":"Reference Counting","text":"<p>DataFrames use <code>Arc&lt;&gt;</code> for cheap cloning:</p> <pre><code>pub struct PyDataFrame {\n    df: Arc&lt;DataFrame&gt;,  // Shared reference\n}\n</code></pre> <p>Multiple Python DataFrames can reference same data.</p>"},{"location":"architecture/overview/#see-also","title":"See Also","text":"<ul> <li>Rust Core - Rust implementation details</li> <li>Python Bindings - PyO3 integration</li> <li>Performance Tips</li> </ul>"},{"location":"architecture/python-bindings/","title":"Python Bindings","text":"<p>How PyRust bridges Python and Rust using PyO3.</p>"},{"location":"architecture/python-bindings/#pyo3-framework","title":"PyO3 Framework","text":"<p>PyRust uses PyO3 for Python bindings:</p> <ul> <li>Zero-cost abstractions</li> <li>Type-safe conversions</li> <li>Automatic reference counting</li> <li>Native Python types</li> </ul>"},{"location":"architecture/python-bindings/#type-conversions","title":"Type Conversions","text":""},{"location":"architecture/python-bindings/#rust-python","title":"Rust \u2192 Python","text":"<pre><code>// String\nlet s: String = \"hello\".to_string();\n// Becomes Python str\n\n// Numbers\nlet n: i64 = 42;\n// Becomes Python int\n\n// DataFrame\nlet df: PyDataFrame = ...;\n// Becomes Python object\n</code></pre>"},{"location":"architecture/python-bindings/#python-rust","title":"Python \u2192 Rust","text":"<pre><code>fn select(&amp;self, cols: Vec&lt;&amp;str&gt;) -&gt; PyResult&lt;Self&gt;\n//              \u2191 Python list \u2192 Rust Vec&lt;&amp;str&gt;\n</code></pre>"},{"location":"architecture/python-bindings/#class-wrapping","title":"Class Wrapping","text":""},{"location":"architecture/python-bindings/#rust-struct","title":"Rust Struct","text":"<pre><code>#[pyclass]\npub struct PyDataFrame {\n    df: Arc&lt;DataFrame&gt;,\n}\n</code></pre>"},{"location":"architecture/python-bindings/#python-class","title":"Python Class","text":"<pre><code># In Python, this becomes:\ndf = DataFrame(...)\n</code></pre>"},{"location":"architecture/python-bindings/#method-binding","title":"Method Binding","text":""},{"location":"architecture/python-bindings/#rust-methods","title":"Rust Methods","text":"<pre><code>#[pymethods]\nimpl PyDataFrame {\n    fn count(&amp;self) -&gt; PyResult&lt;i64&gt; {\n        // Implementation\n    }\n}\n</code></pre>"},{"location":"architecture/python-bindings/#python-methods","title":"Python Methods","text":"<pre><code># Callable from Python as:\ntotal = df.count()\n</code></pre>"},{"location":"architecture/python-bindings/#error-handling","title":"Error Handling","text":""},{"location":"architecture/python-bindings/#rust-error-types","title":"Rust Error Types","text":"<pre><code>PyResult&lt;T&gt;  // Result that can cross Python boundary\nPyErr        // Python exception\n</code></pre>"},{"location":"architecture/python-bindings/#example","title":"Example","text":"<pre><code>fn operation(&amp;self) -&gt; PyResult&lt;DataFrame&gt; {\n    self.df.do_something()\n        .map_err(|e| PyRuntimeError::new_err(\n            format!(\"Operation failed: {}\", e)\n        ))\n}\n</code></pre>"},{"location":"architecture/python-bindings/#in-python","title":"In Python","text":"<pre><code>try:\n    df.operation()\nexcept RuntimeError as e:\n    print(f\"Error: {e}\")\n</code></pre>"},{"location":"architecture/python-bindings/#memory-management","title":"Memory Management","text":""},{"location":"architecture/python-bindings/#reference-counting","title":"Reference Counting","text":"<p>PyO3 uses Python's reference counting:</p> <pre><code>#[pyclass]\nstruct PyDataFrame {\n    df: Arc&lt;DataFrame&gt;,  // Shared ownership in Rust\n}\n</code></pre> <p>When Python GC collects, Rust's Arc&lt;&gt; decrements.</p>"},{"location":"architecture/python-bindings/#zero-copy-arrow","title":"Zero-Copy Arrow","text":"<p>Arrow arrays shared without copying:</p> <pre><code>// Arrow batch created in Rust\nlet batch: RecordBatch = ...;\n\n// Sent to Python via Arrow C API\n// No data copying!\n</code></pre>"},{"location":"architecture/python-bindings/#performance","title":"Performance","text":""},{"location":"architecture/python-bindings/#function-call-overhead","title":"Function Call Overhead","text":"<ul> <li>PyO3 calls: ~10-50ns</li> <li>Negligible for DataFrame operations</li> <li>Dominated by actual computation</li> </ul>"},{"location":"architecture/python-bindings/#data-transfer","title":"Data Transfer","text":"<ul> <li>Column data: Zero-copy via Arrow</li> <li>Metadata: Small overhead</li> <li>Overall: Very efficient</li> </ul>"},{"location":"architecture/python-bindings/#see-also","title":"See Also","text":"<ul> <li>Architecture Overview</li> <li>Rust Core</li> </ul>"},{"location":"architecture/rust-core/","title":"Rust Core","text":"<p>Implementation details of PyRust's Rust core.</p>"},{"location":"architecture/rust-core/#module-structure","title":"Module Structure","text":"<pre><code>src/\n\u251c\u2500\u2500 lib.rs              # PyO3 module definition\n\u251c\u2500\u2500 dataframe/\n\u2502   \u2514\u2500\u2500 mod.rs         # DataFrame implementation\n\u2514\u2500\u2500 session/\n    \u2514\u2500\u2500 mod.rs         # SparkSession implementation\n</code></pre>"},{"location":"architecture/rust-core/#key-types","title":"Key Types","text":""},{"location":"architecture/rust-core/#pydataframe","title":"PyDataFrame","text":"<p>Main DataFrame wrapper around DataFusion's DataFrame.</p> <pre><code>#[pyclass]\npub struct PyDataFrame {\n    df: Arc&lt;DataFrame&gt;,  // Shared reference to DataFusion DataFrame\n}\n</code></pre> <p>Why Arc? - Cheap cloning for transformations - Thread-safe sharing - Immutability</p>"},{"location":"architecture/rust-core/#pysparksession","title":"PySparkSession","text":"<p>Session management and DataFrame creation.</p> <pre><code>#[pyclass]\npub struct PySparkSession {\n    ctx: Arc&lt;SessionContext&gt;,  // DataFusion context\n}\n</code></pre>"},{"location":"architecture/rust-core/#datafusion-integration","title":"DataFusion Integration","text":"<p>PyRust uses Apache DataFusion for query execution:</p> <pre><code>use datafusion::prelude::*;\nuse datafusion::arrow::util::pretty;\n</code></pre>"},{"location":"architecture/rust-core/#query-execution-pattern","title":"Query Execution Pattern","text":"<pre><code>fn filter(&amp;self, condition: &amp;str) -&gt; PyResult&lt;Self&gt; {\n    let rt = Self::runtime()?;  // Tokio runtime\n\n    let df = rt.block_on(async {\n        self.clone_df()\n            .filter(parse_condition(condition))?\n            .await\n    })?;\n\n    Ok(PyDataFrame::new(df))\n}\n</code></pre>"},{"location":"architecture/rust-core/#async-runtime","title":"Async Runtime","text":"<p>DataFusion is async, PyRust bridges to sync Python:</p> <pre><code>fn runtime() -&gt; PyResult&lt;Runtime&gt; {\n    Runtime::new()\n        .map_err(|e| PyRuntimeError::new_err(\n            format!(\"Failed to create runtime: {}\", e)\n        ))\n}\n</code></pre> <p>Pattern: 1. Create Tokio runtime 2. <code>block_on()</code> async DataFusion calls 3. Return result to Python</p>"},{"location":"architecture/rust-core/#error-handling","title":"Error Handling","text":"<p>Rust errors \u2192 Python exceptions:</p> <pre><code>.map_err(|e| PyRuntimeError::new_err(\n    format!(\"Failed to execute: {}\", e)\n))?\n</code></pre>"},{"location":"architecture/rust-core/#pyo3-bindings","title":"PyO3 Bindings","text":""},{"location":"architecture/rust-core/#method-decoration","title":"Method Decoration","text":"<pre><code>#[pymethods]\nimpl PyDataFrame {\n    fn select(&amp;self, cols: Vec&lt;&amp;str&gt;) -&gt; PyResult&lt;Self&gt; {\n        // Implementation\n    }\n}\n</code></pre>"},{"location":"architecture/rust-core/#python-module","title":"Python Module","text":"<pre><code>#[pymodule]\nfn pyrust(_py: Python, m: &amp;PyModule) -&gt; PyResult&lt;()&gt; {\n    m.add_class::&lt;PyDataFrame&gt;()?;\n    m.add_class::&lt;PySparkSession&gt;()?;\n    Ok(())\n}\n</code></pre>"},{"location":"architecture/rust-core/#see-also","title":"See Also","text":"<ul> <li>Architecture Overview</li> <li>Python Bindings</li> </ul>"},{"location":"development/building/","title":"Building PyRust","text":"<p>Instructions for building PyRust from source.</p>"},{"location":"development/building/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.8+</li> <li>Rust 1.70+</li> <li>Git</li> </ul>"},{"location":"development/building/#installation","title":"Installation","text":""},{"location":"development/building/#1-clone-repository","title":"1. Clone Repository","text":"<pre><code>git clone https://github.com/hadrien-chicault/PyRust.git\ncd PyRust\n</code></pre>"},{"location":"development/building/#2-install-rust-if-needed","title":"2. Install Rust (if needed)","text":"<pre><code>curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\nsource $HOME/.cargo/env\n</code></pre>"},{"location":"development/building/#3-create-virtual-environment","title":"3. Create Virtual Environment","text":"<pre><code>python3 -m venv .venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n</code></pre>"},{"location":"development/building/#4-install-dependencies","title":"4. Install Dependencies","text":"<pre><code>pip install maturin pytest ruff\n</code></pre>"},{"location":"development/building/#5-build-pyrust","title":"5. Build PyRust","text":""},{"location":"development/building/#development-build","title":"Development Build","text":"<pre><code>maturin develop\n</code></pre>"},{"location":"development/building/#release-build-faster","title":"Release Build (faster)","text":"<pre><code>maturin develop --release\n</code></pre>"},{"location":"development/building/#running-tests","title":"Running Tests","text":"<pre><code># All tests\npytest python/tests/ -v\n\n# Specific test file\npytest python/tests/test_dataframe.py\n\n# With coverage\npytest python/tests/ --cov=pyrust\n</code></pre>"},{"location":"development/building/#code-quality","title":"Code Quality","text":""},{"location":"development/building/#format-code","title":"Format Code","text":"<pre><code># Rust\ncargo fmt\n\n# Python\nruff format python/\n</code></pre>"},{"location":"development/building/#lint-code","title":"Lint Code","text":"<pre><code># Rust\ncargo clippy\n\n# Python\nruff check python/\n</code></pre>"},{"location":"development/building/#run-all-checks","title":"Run All Checks","text":"<pre><code>pre-commit run --all-files\n</code></pre>"},{"location":"development/building/#documentation","title":"Documentation","text":""},{"location":"development/building/#build-docs-locally","title":"Build Docs Locally","text":"<pre><code>pip install mkdocs mkdocs-material mkdocstrings[python]\nmkdocs serve\n</code></pre> <p>View at: http://127.0.0.1:8000</p>"},{"location":"development/building/#generate-rust-docs","title":"Generate Rust Docs","text":"<pre><code>cargo doc --open\n</code></pre>"},{"location":"development/building/#development-workflow","title":"Development Workflow","text":"<ol> <li>Make changes to Rust code in <code>src/</code></li> <li>Rebuild: <code>maturin develop</code></li> <li>Test: <code>pytest python/tests/</code></li> <li>Repeat</li> </ol>"},{"location":"development/building/#troubleshooting","title":"Troubleshooting","text":""},{"location":"development/building/#no-module-named-pyrust","title":"\"No module named 'pyrust'\"","text":"<p>Run <code>maturin develop</code> to build and install.</p>"},{"location":"development/building/#rust-compilation-errors","title":"Rust compilation errors","text":"<p>Check Rust version: <pre><code>rustc --version  # Should be 1.70+\n</code></pre></p> <p>Update if needed: <pre><code>rustup update\n</code></pre></p>"},{"location":"development/building/#import-errors-after-changes","title":"Import errors after changes","text":"<p>Rebuild and reinstall: <pre><code>maturin develop --release\n</code></pre></p>"},{"location":"development/building/#tests-fail-after-changes","title":"Tests fail after changes","text":"<ol> <li>Check if code compiles: <code>cargo build</code></li> <li>Rebuild Python module: <code>maturin develop</code></li> <li>Run specific failing test: <code>pytest path/to/test.py::test_name -v</code></li> </ol>"},{"location":"development/building/#ide-setup","title":"IDE Setup","text":""},{"location":"development/building/#vs-code","title":"VS Code","text":"<p>Recommended extensions: - rust-analyzer - Python - Ruff</p>"},{"location":"development/building/#pycharm","title":"PyCharm","text":"<ul> <li>Enable Rust plugin</li> <li>Configure Python interpreter to use <code>.venv</code></li> </ul>"},{"location":"development/building/#see-also","title":"See Also","text":"<ul> <li>Contributing Guide</li> <li>Testing Guide</li> <li>Local CI</li> </ul>"},{"location":"development/contributing/","title":"Contributing to PyRust","text":"<p>Thank you for your interest in contributing to PyRust!</p>"},{"location":"development/contributing/#getting-started","title":"Getting Started","text":"<ol> <li>Fork the repository on GitHub</li> <li>Clone your fork locally</li> <li>Set up development environment (see Building)</li> <li>Create a branch for your changes</li> <li>Make your changes with tests</li> <li>Submit a pull request</li> </ol>"},{"location":"development/contributing/#development-setup","title":"Development Setup","text":"<p>See Building for detailed setup instructions.</p> <p>Quick start: <pre><code>git clone https://github.com/YOUR_USERNAME/PyRust.git\ncd PyRust\npython -m venv .venv\nsource .venv/bin/activate\npip install maturin pytest\nmaturin develop\n</code></pre></p>"},{"location":"development/contributing/#code-style","title":"Code Style","text":""},{"location":"development/contributing/#rust","title":"Rust","text":"<ul> <li>Run <code>cargo fmt</code> before committing</li> <li>Run <code>cargo clippy</code> and fix warnings</li> <li>Follow Rust naming conventions</li> </ul>"},{"location":"development/contributing/#python","title":"Python","text":"<ul> <li>Use <code>ruff format</code> for formatting</li> <li>Use <code>ruff check</code> for linting</li> <li>Follow PEP 8 style guide</li> </ul>"},{"location":"development/contributing/#testing","title":"Testing","text":""},{"location":"development/contributing/#run-all-tests","title":"Run All Tests","text":"<pre><code>pytest python/tests/\n</code></pre>"},{"location":"development/contributing/#run-specific-test-file","title":"Run Specific Test File","text":"<pre><code>pytest python/tests/test_dataframe.py -v\n</code></pre>"},{"location":"development/contributing/#add-tests","title":"Add Tests","text":"<ul> <li>Add test files in <code>python/tests/</code></li> <li>Test new features thoroughly</li> <li>Include edge cases</li> </ul>"},{"location":"development/contributing/#pre-commit-hooks","title":"Pre-commit Hooks","text":"<p>PyRust uses pre-commit hooks for quality:</p> <pre><code># Install hooks\npre-commit install\n\n# Run manually\npre-commit run --all-files\n</code></pre> <p>Hooks check: - Rust formatting (cargo fmt) - Rust linting (clippy) - Python formatting (ruff) - Python linting (ruff) - YAML syntax - Trailing whitespace</p>"},{"location":"development/contributing/#pull-request-process","title":"Pull Request Process","text":"<ol> <li>Create an issue describing the change</li> <li>Reference the issue in your PR</li> <li>Include tests for new features</li> <li>Update documentation if needed</li> <li>Ensure CI passes before requesting review</li> </ol>"},{"location":"development/contributing/#pr-checklist","title":"PR Checklist","text":"<ul> <li> Tests added/updated</li> <li> Documentation updated</li> <li> Pre-commit hooks pass</li> <li> All tests pass</li> <li> No clippy warnings</li> <li> Commit messages are clear</li> </ul>"},{"location":"development/contributing/#areas-to-contribute","title":"Areas to Contribute","text":"<p>See ROADMAP.md for priorities.</p> <p>High Impact: - Expression system (col(), lit()) - withColumn() implementation - Column functions (string, math, date) - Window functions</p> <p>Good First Issues: - Documentation improvements - Example scripts - Test coverage - Bug fixes</p>"},{"location":"development/contributing/#code-review","title":"Code Review","text":"<p>Maintainers will review your PR and may: - Request changes - Ask questions - Suggest improvements</p> <p>Please be patient and responsive to feedback.</p>"},{"location":"development/contributing/#license","title":"License","text":"<p>By contributing, you agree that your contributions will be licensed under Apache 2.0.</p>"},{"location":"development/contributing/#questions","title":"Questions?","text":"<ul> <li>Open an Issue</li> <li>Start a Discussion</li> </ul> <p>Thank you for contributing! \ud83c\udf89</p>"},{"location":"development/local-ci/","title":"Local CI Checks","text":"<p>Run the same checks locally that run in CI/CD.</p>"},{"location":"development/local-ci/#quick-start","title":"Quick Start","text":"<pre><code># Install pre-commit hooks\npre-commit install\n\n# Run all checks\npre-commit run --all-files\n</code></pre>"},{"location":"development/local-ci/#individual-checks","title":"Individual Checks","text":""},{"location":"development/local-ci/#rust-format","title":"Rust Format","text":"<pre><code>cargo fmt --check\n</code></pre> <p>Fix issues: <pre><code>cargo fmt\n</code></pre></p>"},{"location":"development/local-ci/#rust-lint-clippy","title":"Rust Lint (Clippy)","text":"<pre><code>cargo clippy -- -D warnings\n</code></pre>"},{"location":"development/local-ci/#rust-compilation","title":"Rust Compilation","text":"<pre><code>cargo build --release\n</code></pre>"},{"location":"development/local-ci/#python-format","title":"Python Format","text":"<pre><code>ruff format --check python/\n</code></pre> <p>Fix issues: <pre><code>ruff format python/\n</code></pre></p>"},{"location":"development/local-ci/#python-lint","title":"Python Lint","text":"<pre><code>ruff check python/\n</code></pre>"},{"location":"development/local-ci/#run-tests","title":"Run Tests","text":"<pre><code>pytest python/tests/ -v\n</code></pre>"},{"location":"development/local-ci/#pre-commit-hooks","title":"Pre-commit Hooks","text":"<p>The <code>.pre-commit-config.yaml</code> file defines hooks that run automatically.</p>"},{"location":"development/local-ci/#what-gets-checked","title":"What Gets Checked","text":"<ul> <li>\u2705 Rust formatting (cargo fmt)</li> <li>\u2705 Rust linting (clippy)</li> <li>\u2705 Rust compilation</li> <li>\u2705 Python formatting (ruff)</li> <li>\u2705 Python linting (ruff)</li> <li>\u2705 Trailing whitespace</li> <li>\u2705 End of file newlines</li> <li>\u2705 YAML syntax</li> <li>\u2705 Large files check</li> <li>\u2705 Mixed line endings</li> <li>\u2705 Markdown linting</li> </ul>"},{"location":"development/local-ci/#manual-run","title":"Manual Run","text":"<pre><code># All files\npre-commit run --all-files\n\n# Only staged files\npre-commit run\n\n# Specific hook\npre-commit run cargo-fmt\n</code></pre>"},{"location":"development/local-ci/#full-ci-simulation","title":"Full CI Simulation","text":"<p>Run everything CI runs:</p> <pre><code># Format check\ncargo fmt --check\nruff format --check python/\n\n# Lint\ncargo clippy -- -D warnings\nruff check python/\n\n# Build\ncargo build --release\n\n# Test\npytest python/tests/ -v\n\n# Documentation\nmkdocs build\n</code></pre>"},{"location":"development/local-ci/#cost-savings","title":"Cost Savings","text":"<p>Running checks locally before pushing: - \u2705 Faster feedback (seconds vs minutes) - \u2705 Reduces CI/CD costs - \u2705 Catches issues early</p>"},{"location":"development/local-ci/#troubleshooting","title":"Troubleshooting","text":""},{"location":"development/local-ci/#pre-commit-fails","title":"Pre-commit fails","text":"<ol> <li>Check error message</li> <li>Fix the issue</li> <li>Run again: <code>pre-commit run --all-files</code></li> </ol>"},{"location":"development/local-ci/#hooks-not-running","title":"Hooks not running","text":"<pre><code># Reinstall hooks\npre-commit uninstall\npre-commit install\n</code></pre>"},{"location":"development/local-ci/#slow-hooks","title":"Slow hooks","text":"<pre><code># Update hooks to latest version\npre-commit autoupdate\n</code></pre>"},{"location":"development/local-ci/#see-also","title":"See Also","text":"<ul> <li>Building Guide</li> <li>Testing Guide</li> <li>Contributing Guide</li> </ul>"},{"location":"development/testing/","title":"Testing Guide","text":"<p>Guide for writing and running tests in PyRust.</p>"},{"location":"development/testing/#test-structure","title":"Test Structure","text":"<pre><code>python/tests/\n\u251c\u2500\u2500 test_dataframe.py       # Core DataFrame tests\n\u251c\u2500\u2500 test_joins.py           # Join operation tests\n\u251c\u2500\u2500 test_data_operations.py # Distinct, union, etc.\n\u251c\u2500\u2500 test_column_operations.py # Column manipulation\n\u251c\u2500\u2500 test_sql.py             # SQL query tests\n\u2514\u2500\u2500 test_compatibility.py   # PySpark compatibility\n</code></pre>"},{"location":"development/testing/#running-tests","title":"Running Tests","text":""},{"location":"development/testing/#all-tests","title":"All Tests","text":"<pre><code>pytest python/tests/ -v\n</code></pre>"},{"location":"development/testing/#specific-file","title":"Specific File","text":"<pre><code>pytest python/tests/test_dataframe.py -v\n</code></pre>"},{"location":"development/testing/#specific-test","title":"Specific Test","text":"<pre><code>pytest python/tests/test_dataframe.py::TestDataFrameOperations::test_filter_numeric -v\n</code></pre>"},{"location":"development/testing/#with-coverage","title":"With Coverage","text":"<pre><code>pytest python/tests/ --cov=pyrust --cov-report=html\n</code></pre>"},{"location":"development/testing/#writing-tests","title":"Writing Tests","text":""},{"location":"development/testing/#test-structure_1","title":"Test Structure","text":"<pre><code>import pytest\nfrom pyrust import SparkSession\n\n@pytest.fixture\ndef spark():\n    \"\"\"Create SparkSession for tests.\"\"\"\n    return SparkSession.builder.appName(\"Test\").getOrCreate()\n\n@pytest.fixture\ndef sample_df(spark, tmp_path):\n    \"\"\"Create sample DataFrame.\"\"\"\n    csv_file = tmp_path / \"data.csv\"\n    csv_file.write_text(\"name,age\\nAlice,25\\nBob,30\\n\")\n    return spark.read.csv(str(csv_file), header=True)\n\nclass TestFeature:\n    \"\"\"Tests for feature X.\"\"\"\n\n    def test_basic_case(self, sample_df):\n        \"\"\"Test basic functionality.\"\"\"\n        result = sample_df.operation()\n        assert result.count() == 2\n\n    def test_edge_case(self, sample_df):\n        \"\"\"Test edge case.\"\"\"\n        # Test implementation\n</code></pre>"},{"location":"development/testing/#test-fixtures","title":"Test Fixtures","text":"<p>Common fixtures: - <code>spark</code> - SparkSession instance - <code>tmp_path</code> - Temporary directory (pytest built-in) - Sample DataFrames created in fixtures</p>"},{"location":"development/testing/#assertions","title":"Assertions","text":"<pre><code># Count checks\nassert df.count() == 10\n\n# Content checks (use show for debugging)\nresult.show()\nassert \"expected_value\" in str(result)\n\n# Error checks\nwith pytest.raises(RuntimeError, match=\"error message\"):\n    df.invalid_operation()\n</code></pre>"},{"location":"development/testing/#test-categories","title":"Test Categories","text":""},{"location":"development/testing/#unit-tests","title":"Unit Tests","text":"<p>Test individual operations:</p> <pre><code>def test_select(self, sample_df):\n    result = sample_df.select(\"name\")\n    assert result.count() == sample_df.count()\n</code></pre>"},{"location":"development/testing/#integration-tests","title":"Integration Tests","text":"<p>Test multiple operations:</p> <pre><code>def test_filter_and_aggregate(self, sample_df):\n    result = sample_df.filter(\"age &gt; 25\") \\\n                     .groupBy(\"city\") \\\n                     .count()\n    assert result.count() &gt; 0\n</code></pre>"},{"location":"development/testing/#compatibility-tests","title":"Compatibility Tests","text":"<p>Verify PySpark compatibility:</p> <pre><code>def test_api_compatibility(self):\n    \"\"\"Test that API matches PySpark.\"\"\"\n    # Verify methods exist and have correct signatures\n</code></pre>"},{"location":"development/testing/#best-practices","title":"Best Practices","text":""},{"location":"development/testing/#1-use-fixtures","title":"1. Use Fixtures","text":"<pre><code>@pytest.fixture\ndef users_df(spark, tmp_path):\n    csv_file = tmp_path / \"users.csv\"\n    csv_file.write_text(\"name,age\\nAlice,25\\n\")\n    return spark.read.csv(str(csv_file), header=True)\n</code></pre>"},{"location":"development/testing/#2-test-edge-cases","title":"2. Test Edge Cases","text":"<pre><code>def test_empty_dataframe(self, spark):\n    \"\"\"Test operations on empty DataFrame.\"\"\"\n    # Create empty DataFrame\n    # Verify behavior\n</code></pre>"},{"location":"development/testing/#3-test-error-conditions","title":"3. Test Error Conditions","text":"<pre><code>def test_invalid_column_raises_error(self, sample_df):\n    with pytest.raises(ValueError):\n        sample_df.select(\"nonexistent_column\")\n</code></pre>"},{"location":"development/testing/#4-keep-tests-fast","title":"4. Keep Tests Fast","text":"<ul> <li>Use small datasets</li> <li>Avoid unnecessary I/O</li> <li>Use tmp_path for temp files</li> </ul>"},{"location":"development/testing/#5-test-names-should-be-descriptive","title":"5. Test Names Should Be Descriptive","text":"<pre><code>def test_join_on_single_column_returns_matching_rows(self):\n    # Clear what's being tested\n</code></pre>"},{"location":"development/testing/#cicd","title":"CI/CD","text":"<p>Tests run automatically on: - Every push to GitHub - Every pull request</p> <p>See <code>.github/workflows/ci.yml</code> for CI configuration.</p>"},{"location":"development/testing/#see-also","title":"See Also","text":"<ul> <li>Building Guide</li> <li>Contributing Guide</li> <li>Local CI</li> </ul>"},{"location":"getting-started/basic-operations/","title":"Basic Operations","text":"<p>This guide covers the fundamental operations you'll use in PyRust.</p>"},{"location":"getting-started/basic-operations/#reading-data","title":"Reading Data","text":""},{"location":"getting-started/basic-operations/#csv-files","title":"CSV Files","text":"<pre><code># Basic CSV read\ndf = spark.read.csv(\"data.csv\")\n\n# With options\ndf = spark.read.csv(\n    \"data.csv\",\n    header=True,        # First row contains column names\n    infer_schema=True   # Automatically detect column types\n)\n</code></pre> <p>CSV Options: - <code>header</code> (bool): Whether first row is header (default: <code>True</code>) - <code>infer_schema</code> (bool): Automatically detect types (default: <code>True</code>)</p>"},{"location":"getting-started/basic-operations/#parquet-files","title":"Parquet Files","text":"<pre><code># Read Parquet file\ndf = spark.read.parquet(\"data.parquet\")\n\n# Read directory of Parquet files\ndf = spark.read.parquet(\"data_dir/\")\n</code></pre> <p>Why Parquet? - 5-10x smaller file size - 10-100x faster reads for selective queries - Schema preserved (no inference needed) - Built-in compression and encoding</p>"},{"location":"getting-started/basic-operations/#inspecting-data","title":"Inspecting Data","text":""},{"location":"getting-started/basic-operations/#view-schema","title":"View Schema","text":"<pre><code># Print schema in tree format\ndf.printSchema()\n\n# Get schema as string\nschema_str = df.schema()\nprint(schema_str)\n</code></pre>"},{"location":"getting-started/basic-operations/#display-rows","title":"Display Rows","text":"<pre><code># Show first 20 rows (default)\ndf.show()\n\n# Show specific number of rows\ndf.show(5)\ndf.show(100)\n\n# Count total rows\ntotal = df.count()\nprint(f\"Total rows: {total}\")\n</code></pre>"},{"location":"getting-started/basic-operations/#selecting-columns","title":"Selecting Columns","text":""},{"location":"getting-started/basic-operations/#select-specific-columns","title":"Select Specific Columns","text":"<pre><code># Select single column\nnames = df.select([\"name\"])\n\n# Select multiple columns\nsubset = df.select([\"name\", \"age\", \"city\"])\n\n# Reorder columns\nreordered = df.select([\"city\", \"name\", \"age\"])\n</code></pre>"},{"location":"getting-started/basic-operations/#column-operations","title":"Column Operations","text":"<pre><code># Select all columns (rarely needed)\nall_cols = df.select(df.columns)  # TODO: Not yet implemented\n\n# Drop columns\n# TODO: Not yet implemented in current version\n</code></pre>"},{"location":"getting-started/basic-operations/#filtering-rows","title":"Filtering Rows","text":""},{"location":"getting-started/basic-operations/#simple-filters","title":"Simple Filters","text":"<pre><code># Single condition\nadults = df.filter(\"age &gt;= 18\")\nseniors = df.filter(\"age &gt;= 65\")\n\n# Numeric comparisons\nhigh_earners = df.filter(\"salary &gt; 100000\")\nyoung_adults = df.filter(\"age &gt; 18\")\n</code></pre>"},{"location":"getting-started/basic-operations/#supported-operators","title":"Supported Operators","text":"Operator Description Example <code>&gt;</code> Greater than <code>age &gt; 18</code> <code>&lt;</code> Less than <code>age &lt; 65</code> <code>&gt;=</code> Greater or equal <code>salary &gt;= 50000</code> <code>&lt;=</code> Less or equal <code>age &lt;= 30</code> <code>==</code> Equal <code>status == 'active'</code> <code>!=</code> Not equal <code>city != 'Unknown'</code>"},{"location":"getting-started/basic-operations/#string-filters","title":"String Filters","text":"<pre><code># String equality (with quotes)\nvip_customers = df.filter(\"status == 'VIP'\")\nny_users = df.filter(\"city == 'New York'\")\n\n# String inequality\nactive = df.filter(\"status != 'inactive'\")\n</code></pre>"},{"location":"getting-started/basic-operations/#sorting-data","title":"Sorting Data","text":""},{"location":"getting-started/basic-operations/#sort-ascending","title":"Sort Ascending","text":"<pre><code># Sort by single column\nsorted_df = df.orderBy([\"age\"])\n\n# Sort by multiple columns (priority: left to right)\nsorted_df = df.orderBy([\"country\", \"city\", \"name\"])\n\n# Alternative: sort() method (identical)\nsorted_df = df.sort([\"age\"])\n</code></pre>"},{"location":"getting-started/basic-operations/#sorting-tips","title":"Sorting Tips","text":"<ul> <li>Nulls are sorted first by default</li> <li>Descending sort not yet implemented (future feature)</li> <li>Combine with <code>limit()</code> for \"top N\" queries</li> </ul>"},{"location":"getting-started/basic-operations/#limiting-results","title":"Limiting Results","text":""},{"location":"getting-started/basic-operations/#limit-rows","title":"Limit Rows","text":"<pre><code># First 10 rows\npreview = df.limit(10)\n\n# Top 5 after sorting\ntop_5 = df.orderBy([\"sales\"]).limit(5)\n\n# Sample for testing\ntest_df = df.limit(1000)\n</code></pre> <p>Use Cases: - Quick data preview - Top N queries - Testing transformations on small subset - Reducing output size</p>"},{"location":"getting-started/basic-operations/#grouping-and-aggregation","title":"Grouping and Aggregation","text":""},{"location":"getting-started/basic-operations/#simple-count","title":"Simple Count","text":"<pre><code># Count by single column\ncity_counts = df.groupBy([\"city\"]).count()\ncity_counts.show()\n</code></pre>"},{"location":"getting-started/basic-operations/#multiple-aggregations","title":"Multiple Aggregations","text":"<pre><code># Aggregate functions: count, sum, avg, min, max\nstats = df.groupBy([\"department\"]).agg([\n    (\"salary\", \"avg\"),\n    (\"salary\", \"min\"),\n    (\"salary\", \"max\"),\n    (\"employee_id\", \"count\")\n])\nstats.show()\n</code></pre>"},{"location":"getting-started/basic-operations/#group-by-multiple-columns","title":"Group by Multiple Columns","text":"<pre><code># Multi-level grouping\nsummary = df.groupBy([\"country\", \"city\"]).agg([\n    (\"population\", \"sum\"),\n    (\"age\", \"avg\")\n])\nsummary.show()\n</code></pre>"},{"location":"getting-started/basic-operations/#available-aggregation-functions","title":"Available Aggregation Functions","text":"Function Description Example <code>count</code> Count rows <code>(\"id\", \"count\")</code> <code>sum</code> Sum values <code>(\"amount\", \"sum\")</code> <code>avg</code> / <code>mean</code> Average <code>(\"age\", \"avg\")</code> <code>min</code> Minimum <code>(\"salary\", \"min\")</code> <code>max</code> Maximum <code>(\"salary\", \"max\")</code>"},{"location":"getting-started/basic-operations/#chaining-operations","title":"Chaining Operations","text":""},{"location":"getting-started/basic-operations/#transformation-chain","title":"Transformation Chain","text":"<p>Operations can be chained for complex workflows:</p> <pre><code>result = df.select([\"name\", \"age\", \"city\", \"salary\"]) \\\n           .filter(\"age &gt; 25\") \\\n           .filter(\"salary &gt; 50000\") \\\n           .orderBy([\"salary\"]) \\\n           .limit(100)\n</code></pre>"},{"location":"getting-started/basic-operations/#lazy-evaluation","title":"Lazy Evaluation","text":"<p>Transformations are lazy - they don't execute until an action is called:</p> <pre><code># These are lazy (just build query plan)\nfiltered = df.filter(\"age &gt; 18\")\nselected = filtered.select([\"name\", \"city\"])\nsorted_df = selected.orderBy([\"name\"])\n\n# This triggers execution\nsorted_df.show()  # Action!\n</code></pre> <p>Actions that trigger execution: - <code>show()</code> - Display rows - <code>count()</code> - Count rows - <code>collect()</code> - Future: return all data to Python</p>"},{"location":"getting-started/basic-operations/#complete-examples","title":"Complete Examples","text":""},{"location":"getting-started/basic-operations/#example-1-data-cleaning","title":"Example 1: Data Cleaning","text":"<pre><code># Remove invalid rows and select relevant columns\nclean_df = df.filter(\"age &gt; 0\") \\\n             .filter(\"age &lt; 120\") \\\n             .select([\"name\", \"age\", \"email\"]) \\\n             .orderBy([\"age\"])\n\nprint(f\"Valid records: {clean_df.count()}\")\nclean_df.show(10)\n</code></pre>"},{"location":"getting-started/basic-operations/#example-2-sales-analysis","title":"Example 2: Sales Analysis","text":"<pre><code># Analyze sales by region\nsales_summary = df.filter(\"amount &gt; 0\") \\\n                  .groupBy([\"region\"]) \\\n                  .agg([\n                      (\"amount\", \"sum\"),\n                      (\"amount\", \"avg\"),\n                      (\"transaction_id\", \"count\")\n                  ]) \\\n                  .orderBy([\"sum(amount)\"])\n\nprint(\"Sales Summary by Region:\")\nsales_summary.show()\n</code></pre>"},{"location":"getting-started/basic-operations/#example-3-user-segmentation","title":"Example 3: User Segmentation","text":"<pre><code># Find active users in top cities\nactive_users = df.filter(\"last_login_days &lt; 30\") \\\n                 .filter(\"age &gt;= 18\") \\\n                 .groupBy([\"city\"]) \\\n                 .count() \\\n                 .orderBy([\"count\"]) \\\n                 .limit(10)\n\nprint(\"Top 10 Cities by Active Users:\")\nactive_users.show()\n</code></pre>"},{"location":"getting-started/basic-operations/#best-practices","title":"Best Practices","text":""},{"location":"getting-started/basic-operations/#1-filter-early","title":"1. Filter Early","text":"<pre><code># Good\ndf.filter(\"age &gt; 18\").select([\"name\", \"city\"]).groupBy([\"city\"]).count()\n\n# Less efficient\ndf.groupBy([\"city\"]).count().filter(\"count &gt; 100\")\n</code></pre>"},{"location":"getting-started/basic-operations/#2-select-only-needed-columns","title":"2. Select Only Needed Columns","text":"<pre><code># Good\ndf.select([\"name\", \"age\"]).filter(\"age &gt; 18\")\n\n# Less efficient (processes all columns)\ndf.filter(\"age &gt; 18\")\n</code></pre>"},{"location":"getting-started/basic-operations/#3-use-limit-for-exploration","title":"3. Use Limit for Exploration","text":"<pre><code># Quick preview\ndf.limit(100).show()\n\n# Test on subset\ndf.limit(10000).filter(\"age &gt; 18\").groupBy([\"city\"]).count().show()\n</code></pre>"},{"location":"getting-started/basic-operations/#4-chain-related-operations","title":"4. Chain Related Operations","text":"<pre><code># Good - clear intent\nresult = df.filter(\"valid == true\") \\\n           .select([\"id\", \"value\"]) \\\n           .orderBy([\"value\"]) \\\n           .limit(100)\n\n# Avoid - unnecessary intermediate variables\nfiltered = df.filter(\"valid == true\")\nselected = filtered.select([\"id\", \"value\"])\nsorted_df = selected.orderBy([\"value\"])\nresult = sorted_df.limit(100)\n</code></pre>"},{"location":"getting-started/basic-operations/#next-steps","title":"Next Steps","text":"<ul> <li>DataFrame API Guide - Advanced DataFrame operations</li> <li>Performance Tips - Optimize your queries</li> <li>API Reference - Complete method documentation</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>This guide will help you install PyRust on your system.</p>"},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Python: 3.8 or higher</li> <li>Rust: 1.70 or higher (only for building from source)</li> <li>Operating System: Linux, macOS, or Windows</li> </ul>"},{"location":"getting-started/installation/#installation-methods","title":"Installation Methods","text":""},{"location":"getting-started/installation/#via-pip-recommended","title":"Via pip (Recommended)","text":"<p>Once PyRust is published to PyPI, installation is simple:</p> <pre><code>pip install pyrust\n</code></pre>"},{"location":"getting-started/installation/#from-github-release","title":"From GitHub Release","text":"<p>Download a pre-built wheel from the releases page:</p> <pre><code># Linux x86_64\npip install https://github.com/hadrien-chicault/PyRust/releases/download/v0.1.0/pyrust-0.1.0-cp312-cp312-linux_x86_64.whl\n\n# macOS x86_64\npip install https://github.com/hadrien-chicault/PyRust/releases/download/v0.1.0/pyrust-0.1.0-cp312-cp312-macosx_x86_64.whl\n\n# macOS ARM64 (Apple Silicon)\npip install https://github.com/hadrien-chicault/PyRust/releases/download/v0.1.0/pyrust-0.1.0-cp312-cp312-macosx_arm64.whl\n\n# Windows x86_64\npip install https://github.com/hadrien-chicault/PyRust/releases/download/v0.1.0/pyrust-0.1.0-cp312-cp312-win_amd64.whl\n</code></pre>"},{"location":"getting-started/installation/#from-source","title":"From Source","text":"<p>Building from source requires Rust and Maturin:</p> <pre><code># Clone the repository\ngit clone https://github.com/hadrien-chicault/PyRust.git\ncd PyRust\n\n# Install Rust (if not already installed)\ncurl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\n\n# Install maturin\npip install maturin\n\n# Build and install\nmaturin develop --release\n</code></pre>"},{"location":"getting-started/installation/#using-uv-fast-python-package-manager","title":"Using UV (Fast Python Package Manager)","text":"<p>UV is a fast Python package installer:</p> <pre><code># Install UV\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Install PyRust\nuv pip install pyrust\n\n# Or build from source\ngit clone https://github.com/hadrien-chicault/PyRust.git\ncd PyRust\nuv tool install maturin\nmaturin develop --release\n</code></pre>"},{"location":"getting-started/installation/#verify-installation","title":"Verify Installation","text":"<p>Test that PyRust is correctly installed:</p> <pre><code>from pyrust import SparkSession\n\n# Create a session\nspark = SparkSession.builder() \\\n    .appName(\"TestApp\") \\\n    .getOrCreate()\n\nprint(f\"PyRust installed successfully!\")\nprint(f\"App name: {spark.appName}\")\n</code></pre> <p>Expected output: <pre><code>PyRust installed successfully!\nApp name: TestApp\n</code></pre></p>"},{"location":"getting-started/installation/#optional-dependencies","title":"Optional Dependencies","text":""},{"location":"getting-started/installation/#for-development","title":"For Development","text":"<p>If you're contributing to PyRust, install development dependencies:</p> <pre><code>pip install pyrust[dev]\n</code></pre> <p>This includes: - <code>pytest</code> - Testing framework - <code>pytest-benchmark</code> - Performance benchmarking - <code>pandas</code> - For comparison tests - <code>maturin</code> - Build tool</p>"},{"location":"getting-started/installation/#for-data-analysis","title":"For Data Analysis","text":"<p>Additional libraries you might want:</p> <pre><code># Arrow for data interchange\npip install pyarrow\n\n# Pandas for comparison\npip install pandas\n\n# Jupyter for interactive analysis\npip install jupyter\n</code></pre>"},{"location":"getting-started/installation/#platform-specific-notes","title":"Platform-Specific Notes","text":""},{"location":"getting-started/installation/#linux","title":"Linux","text":"<p>On Linux, you may need to install additional system dependencies:</p> <pre><code># Ubuntu/Debian\nsudo apt-get update\nsudo apt-get install build-essential python3-dev\n\n# Fedora/RHEL\nsudo dnf install gcc python3-devel\n</code></pre>"},{"location":"getting-started/installation/#macos","title":"macOS","text":"<p>On macOS, ensure Xcode Command Line Tools are installed:</p> <pre><code>xcode-select --install\n</code></pre>"},{"location":"getting-started/installation/#windows","title":"Windows","text":"<p>On Windows, install the Visual C++ Build Tools:</p> <ol> <li>Download from Visual Studio Downloads</li> <li>Install \"Desktop development with C++\"</li> </ol>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/installation/#import-error","title":"Import Error","text":"<p>If you get <code>ImportError: No module named '_pyrust'</code>:</p> <pre><code># Reinstall with --force-reinstall\npip install --force-reinstall pyrust\n</code></pre>"},{"location":"getting-started/installation/#permission-denied","title":"Permission Denied","text":"<p>On Linux/macOS, if you get permission errors:</p> <pre><code># Install for current user only\npip install --user pyrust\n</code></pre>"},{"location":"getting-started/installation/#rust-version-too-old","title":"Rust Version Too Old","text":"<p>If building from source fails with Rust version errors:</p> <pre><code># Update Rust\nrustup update stable\n</code></pre>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<p>Now that PyRust is installed, check out:</p> <ul> <li>Quick Start Tutorial - Write your first PyRust program</li> <li>Basic Operations - Learn common data operations</li> <li>API Reference - Explore the full API</li> </ul>"},{"location":"getting-started/quickstart/","title":"Quick Start Tutorial","text":"<p>This tutorial will get you started with PyRust in 5 minutes.</p>"},{"location":"getting-started/quickstart/#your-first-pyrust-program","title":"Your First PyRust Program","text":"<p>Let's create a simple data analysis program:</p> <pre><code>from pyrust import SparkSession\n\n# 1. Create a SparkSession\nspark = SparkSession.builder() \\\n    .appName(\"MyFirstApp\") \\\n    .getOrCreate()\n\n# 2. Read a CSV file\ndf = spark.read.csv(\"data.csv\", header=True, infer_schema=True)\n\n# 3. Inspect the data\ndf.printSchema()\ndf.show(5)\n\n# 4. Perform transformations\nresult = df.select([\"name\", \"age\", \"city\"]) \\\n           .filter(\"age &gt; 18\") \\\n           .orderBy([\"age\"])\n\n# 5. Display results\nresult.show()\n</code></pre>"},{"location":"getting-started/quickstart/#step-by-step-breakdown","title":"Step-by-Step Breakdown","text":""},{"location":"getting-started/quickstart/#step-1-create-a-sparksession","title":"Step 1: Create a SparkSession","text":"<p>The <code>SparkSession</code> is your entry point to PyRust:</p> <pre><code>spark = SparkSession.builder() \\\n    .appName(\"MyFirstApp\") \\\n    .getOrCreate()\n</code></pre> <ul> <li><code>builder()</code> - Returns a builder for configuration</li> <li><code>appName()</code> - Sets your application name</li> <li><code>getOrCreate()</code> - Creates or retrieves the session</li> </ul>"},{"location":"getting-started/quickstart/#step-2-read-data","title":"Step 2: Read Data","text":"<p>Load data from various sources:</p> <pre><code># CSV with header\ndf = spark.read.csv(\"data.csv\", header=True, infer_schema=True)\n\n# Parquet (faster for large files)\ndf = spark.read.parquet(\"data.parquet\")\n</code></pre> <p>Supported formats: - CSV - Comma-separated values - Parquet - Columnar format (recommended for production)</p>"},{"location":"getting-started/quickstart/#step-3-inspect-data","title":"Step 3: Inspect Data","text":"<p>Understand your data structure:</p> <pre><code># Print schema (column types)\ndf.printSchema()\n# Output:\n# root\n#  |-- name: Utf8 (nullable = true)\n#  |-- age: Int64 (nullable = true)\n#  |-- city: Utf8 (nullable = true)\n\n# Show first rows\ndf.show(5)  # Show 5 rows\n\n# Count total rows\nprint(f\"Total rows: {df.count()}\")\n</code></pre>"},{"location":"getting-started/quickstart/#step-4-transform-data","title":"Step 4: Transform Data","text":"<p>Chain operations to transform your data:</p> <pre><code># Select specific columns\nsubset = df.select([\"name\", \"age\"])\n\n# Filter rows\nadults = df.filter(\"age &gt;= 18\")\n\n# Sort data\nsorted_df = df.orderBy([\"age\"])\n\n# Limit results\ntop_10 = df.limit(10)\n\n# Chain operations (lazy evaluation)\nresult = df.select([\"name\", \"age\"]) \\\n           .filter(\"age &gt; 18\") \\\n           .orderBy([\"age\"]) \\\n           .limit(100)\n</code></pre>"},{"location":"getting-started/quickstart/#step-5-aggregations","title":"Step 5: Aggregations","text":"<p>Compute statistics on grouped data:</p> <pre><code># Count by group\ncity_counts = df.groupBy([\"city\"]).count()\ncity_counts.show()\n\n# Multiple aggregations\nstats = df.groupBy([\"city\"]).agg([\n    (\"age\", \"avg\"),\n    (\"age\", \"min\"),\n    (\"age\", \"max\")\n])\nstats.show()\n</code></pre>"},{"location":"getting-started/quickstart/#complete-example","title":"Complete Example","text":"<p>Here's a complete data analysis workflow:</p> <pre><code>from pyrust import SparkSession\n\n# Initialize\nspark = SparkSession.builder().appName(\"Analysis\").getOrCreate()\n\n# Load sales data\nsales = spark.read.csv(\"sales.csv\", header=True, infer_schema=True)\n\n# Analyze high-value sales by region\nanalysis = sales.filter(\"amount &gt; 1000\") \\\n                .select([\"region\", \"amount\", \"product\"]) \\\n                .groupBy([\"region\"]) \\\n                .agg([\n                    (\"amount\", \"sum\"),\n                    (\"amount\", \"avg\"),\n                    (\"product\", \"count\")\n                ]) \\\n                .orderBy([\"sum(amount)\"])\n\n# Display results\nprint(\"High-Value Sales Analysis:\")\nanalysis.show()\n\n# Get summary statistics\nprint(f\"\\nTotal high-value transactions: {sales.filter('amount &gt; 1000').count()}\")\n</code></pre>"},{"location":"getting-started/quickstart/#performance-tips","title":"Performance Tips","text":""},{"location":"getting-started/quickstart/#1-filter-early","title":"1. Filter Early","text":"<p>Apply filters before other operations:</p> <pre><code># Good - filter first\ndf.filter(\"age &gt; 18\").select([\"name\", \"city\"]).groupBy([\"city\"]).count()\n\n# Less efficient - filter last\ndf.select([\"name\", \"city\"]).groupBy([\"city\"]).count().filter(\"count &gt; 100\")\n</code></pre>"},{"location":"getting-started/quickstart/#2-select-only-needed-columns","title":"2. Select Only Needed Columns","text":"<p>Reduce data size by selecting only required columns:</p> <pre><code># Good - select only needed columns\ndf.select([\"name\", \"age\"]).filter(\"age &gt; 18\")\n\n# Less efficient - process all columns\ndf.filter(\"age &gt; 18\")  # Still carries all columns\n</code></pre>"},{"location":"getting-started/quickstart/#3-use-parquet-for-large-files","title":"3. Use Parquet for Large Files","text":"<p>Parquet is much faster than CSV:</p> <pre><code># Save as Parquet for future use\ndf.write.parquet(\"data.parquet\")  # TODO: Not yet implemented\n\n# Read Parquet (10-100x faster than CSV)\ndf = spark.read.parquet(\"data.parquet\")\n</code></pre>"},{"location":"getting-started/quickstart/#4-limit-when-exploring","title":"4. Limit When Exploring","text":"<p>Use <code>limit()</code> for quick data exploration:</p> <pre><code># Fast preview\ndf.limit(1000).show()\n\n# Analyze small subset first\ndf.limit(10000).groupBy([\"category\"]).count().show()\n</code></pre>"},{"location":"getting-started/quickstart/#common-patterns","title":"Common Patterns","text":""},{"location":"getting-started/quickstart/#pattern-1-filter-and-aggregate","title":"Pattern 1: Filter and Aggregate","text":"<pre><code># Count adult users by city\nadult_by_city = df.filter(\"age &gt;= 18\") \\\n                  .groupBy([\"city\"]) \\\n                  .count() \\\n                  .orderBy([\"count\"])\nadult_by_city.show()\n</code></pre>"},{"location":"getting-started/quickstart/#pattern-2-top-n-analysis","title":"Pattern 2: Top N Analysis","text":"<pre><code># Top 10 highest sales\ntop_sales = df.orderBy([\"amount\"]) \\\n              .limit(10) \\\n              .select([\"customer\", \"amount\", \"date\"])\ntop_sales.show()\n</code></pre>"},{"location":"getting-started/quickstart/#pattern-3-multi-column-grouping","title":"Pattern 3: Multi-Column Grouping","text":"<pre><code># Sales by country and product\nsummary = df.groupBy([\"country\", \"product\"]) \\\n            .agg([(\"amount\", \"sum\"), (\"order_id\", \"count\")])\nsummary.show()\n</code></pre>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<p>You're now ready to use PyRust! Continue learning:</p> <ul> <li>Basic Operations - More detailed operation guides</li> <li>DataFrame API - Complete DataFrame guide</li> <li>Performance Tips - Optimize your queries</li> <li>API Reference - Full API documentation</li> </ul>"},{"location":"getting-started/quickstart/#sample-data","title":"Sample Data","text":"<p>Need sample data to practice? Create a test CSV file:</p> <pre><code>name,age,city,salary\nAlice,25,New York,75000\nBob,30,London,85000\nCharlie,35,New York,95000\nDiana,28,Paris,80000\nEve,42,London,110000\nFrank,19,Paris,45000\nGrace,55,New York,125000\nHenry,31,London,88000\n</code></pre> <p>Save this as <code>test.csv</code> and try the examples above!</p>"},{"location":"user-guide/column-operations/","title":"Column Operations","text":"<p>PyRust provides operations for manipulating DataFrame columns, including renaming columns and type casting. This guide covers column-level transformations.</p>"},{"location":"user-guide/column-operations/#renaming-columns","title":"Renaming Columns","text":""},{"location":"user-guide/column-operations/#withcolumnrenamed","title":"withColumnRenamed()","text":"<p>Rename a single column in the DataFrame:</p> <pre><code># Rename a single column\ndf = df.withColumnRenamed(\"old_name\", \"new_name\")\n\n# Chain multiple renames\ndf = df.withColumnRenamed(\"col1\", \"column_one\") \\\n       .withColumnRenamed(\"col2\", \"column_two\")\n</code></pre> <p>Alias: You can also use <code>with_column_renamed()</code> (snake_case) for compatibility.</p> <p>Parameters: - <code>existing</code>: Current column name (must exist) - <code>new</code>: New name for the column (must not already exist)</p> <p>Returns: A new DataFrame with the column renamed</p> <p>Behavior: - All data is preserved - Only affects the specified column - Other columns remain unchanged - Can be chained with other operations</p> <p>Examples:</p> <pre><code>from pyrust import SparkSession\n\nspark = SparkSession.builder.appName(\"ColumnOps\").getOrCreate()\n\n# Load data\ndf = spark.read.csv(\"users.csv\", header=True)\n\n# Make column names more descriptive\ndf = df.withColumnRenamed(\"name\", \"employee_name\") \\\n       .withColumnRenamed(\"age\", \"employee_age\")\n\n# Rename before aggregation\nresult = df.withColumnRenamed(\"city\", \"location\") \\\n           .groupBy(\"location\") \\\n           .count()\n</code></pre>"},{"location":"user-guide/column-operations/#common-patterns","title":"Common Patterns","text":""},{"location":"user-guide/column-operations/#standardizing-column-names","title":"Standardizing Column Names","text":"<pre><code># Convert to standard naming convention\nstandardized = (\n    df.withColumnRenamed(\"FirstName\", \"first_name\")\n      .withColumnRenamed(\"LastName\", \"last_name\")\n      .withColumnRenamed(\"EmailAddr\", \"email_address\")\n)\n</code></pre>"},{"location":"user-guide/column-operations/#preparing-for-joins","title":"Preparing for Joins","text":"<pre><code># Rename columns to avoid conflicts in joins\nusers = df.withColumnRenamed(\"id\", \"user_id\") \\\n          .withColumnRenamed(\"name\", \"user_name\")\n\norders = orders_df.withColumnRenamed(\"id\", \"order_id\")\n\n# Now join without column name conflicts\nresult = users.join(orders, on=[\"user_id\"])\n</code></pre>"},{"location":"user-guide/column-operations/#making-names-more-descriptive","title":"Making Names More Descriptive","text":"<pre><code># Add context to generic names\nimproved = (\n    df.withColumnRenamed(\"date\", \"purchase_date\")\n      .withColumnRenamed(\"amount\", \"total_amount\")\n      .withColumnRenamed(\"status\", \"order_status\")\n)\n</code></pre>"},{"location":"user-guide/column-operations/#fixing-typos-or-case","title":"Fixing Typos or Case","text":"<pre><code># Fix column name issues\nfixed = df.withColumnRenamed(\"adress\", \"address\")  # Fix typo\nfixed = df.withColumnRenamed(\"Name\", \"name\")        # Normalize case\n</code></pre>"},{"location":"user-guide/column-operations/#integration-with-other-operations","title":"Integration with Other Operations","text":""},{"location":"user-guide/column-operations/#with-filtering","title":"With Filtering","text":"<pre><code># Rename then filter\nresult = df.withColumnRenamed(\"age\", \"user_age\") \\\n           .filter(\"user_age &gt; 25\")\n</code></pre>"},{"location":"user-guide/column-operations/#with-selection","title":"With Selection","text":"<pre><code># Rename then select specific columns\nresult = df.withColumnRenamed(\"name\", \"full_name\") \\\n           .select(\"full_name\", \"email\")\n</code></pre>"},{"location":"user-guide/column-operations/#with-sorting","title":"With Sorting","text":"<pre><code># Rename then sort\nresult = df.withColumnRenamed(\"salary\", \"annual_salary\") \\\n           .orderBy(\"annual_salary\")\n</code></pre>"},{"location":"user-guide/column-operations/#with-aggregation","title":"With Aggregation","text":"<pre><code># Rename before grouping\nresult = df.withColumnRenamed(\"city\", \"location\") \\\n           .groupBy(\"location\") \\\n           .agg((\"salary\", \"avg\"))\n</code></pre>"},{"location":"user-guide/column-operations/#with-joins","title":"With Joins","text":"<pre><code># Rename before join to avoid ambiguity\ndf1 = df1.withColumnRenamed(\"name\", \"user_name\")\ndf2 = df2.withColumnRenamed(\"name\", \"product_name\")\n\nresult = df1.join(df2, on=[\"id\"])\n# Result has both user_name and product_name clearly distinguished\n</code></pre>"},{"location":"user-guide/column-operations/#error-handling","title":"Error Handling","text":""},{"location":"user-guide/column-operations/#column-doesnt-exist","title":"Column Doesn't Exist","text":"<pre><code>try:\n    df.withColumnRenamed(\"nonexistent\", \"new_name\")\nexcept RuntimeError:\n    print(\"Column 'nonexistent' does not exist\")\n</code></pre>"},{"location":"user-guide/column-operations/#target-name-already-exists","title":"Target Name Already Exists","text":"<pre><code>try:\n    # age already exists\n    df.withColumnRenamed(\"name\", \"age\")\nexcept RuntimeError:\n    print(\"Column 'age' already exists\")\n</code></pre>"},{"location":"user-guide/column-operations/#performance-tips","title":"Performance Tips","text":"<ol> <li> <p>Chain efficiently: Multiple renames can be chained together    <pre><code># Good - single pass through data\ndf.withColumnRenamed(\"a\", \"col_a\") \\\n  .withColumnRenamed(\"b\", \"col_b\") \\\n  .withColumnRenamed(\"c\", \"col_c\")\n</code></pre></p> </li> <li> <p>Rename early: If you only need renamed columns, rename before filtering    <pre><code># Better - rename affects fewer columns after select\ndf.select(\"name\", \"age\") \\\n  .withColumnRenamed(\"name\", \"full_name\")\n\n# vs renaming before select\ndf.withColumnRenamed(\"name\", \"full_name\") \\\n  .select(\"full_name\", \"age\")\n</code></pre></p> </li> <li> <p>Rename for clarity: Use descriptive names to make code more readable    <pre><code># Clear and self-documenting\ndf.withColumnRenamed(\"ts\", \"timestamp\") \\\n  .withColumnRenamed(\"qty\", \"quantity\") \\\n  .withColumnRenamed(\"amt\", \"amount\")\n</code></pre></p> </li> </ol>"},{"location":"user-guide/column-operations/#method-summary","title":"Method Summary","text":"Method Description Returns <code>withColumnRenamed(existing, new)</code> Rename a column New DataFrame <code>with_column_renamed(existing, new)</code> Snake_case alias New DataFrame"},{"location":"user-guide/column-operations/#api-aliases","title":"API Aliases","text":"<p>For compatibility, PyRust provides both naming conventions:</p> <pre><code>df.withColumnRenamed(\"old\", \"new\")  # camelCase (PySpark style)\ndf.with_column_renamed(\"old\", \"new\")  # snake_case (Python style)\n</code></pre>"},{"location":"user-guide/column-operations/#future-enhancements","title":"Future Enhancements","text":"<p>Future versions will support: - <code>withColumn()</code> - Add or replace columns with expressions - <code>cast()</code> - Change column data types - <code>alias()</code> - Column aliases in select expressions - <code>drop()</code> - Remove columns from DataFrame</p>"},{"location":"user-guide/column-operations/#examples","title":"Examples","text":"<p>See the complete examples in <code>examples/column_operations.py</code>:</p> <pre><code>python examples/column_operations.py\n</code></pre> <p>This example demonstrates: - Basic column renaming - Chaining multiple renames - Renaming with filters, joins, and aggregations - Error handling - Practical use cases</p>"},{"location":"user-guide/data-loading/","title":"Data Loading Guide","text":"<p>Learn how to efficiently load data into PyRust DataFrames.</p>"},{"location":"user-guide/data-loading/#supported-formats","title":"Supported Formats","text":"<p>PyRust currently supports: - CSV - Comma-separated values - Parquet - Columnar format (recommended)</p>"},{"location":"user-guide/data-loading/#csv-files","title":"CSV Files","text":""},{"location":"user-guide/data-loading/#basic-csv-loading","title":"Basic CSV Loading","text":"<pre><code># Simple read\ndf = spark.read.csv(\"data.csv\")\n\n# With header\ndf = spark.read.csv(\"data.csv\", header=True)\n\n# With schema inference\ndf = spark.read.csv(\"data.csv\", header=True, infer_schema=True)\n</code></pre>"},{"location":"user-guide/data-loading/#csv-options","title":"CSV Options","text":"Option Type Default Description <code>header</code> bool <code>True</code> First row contains column names <code>infer_schema</code> bool <code>True</code> Automatically detect column types"},{"location":"user-guide/data-loading/#schema-inference","title":"Schema Inference","text":"<p>When <code>infer_schema=True</code>, PyRust samples the first 1000 rows to detect types:</p> <pre><code>df = spark.read.csv(\"data.csv\", header=True, infer_schema=True)\ndf.printSchema()\n# Output:\n# root\n#  |-- name: Utf8\n#  |-- age: Int64\n#  |-- salary: Float64\n</code></pre> <p>Inference Rules: - Pure integers \u2192 <code>Int64</code> - Numbers with decimals \u2192 <code>Float64</code> - Text \u2192 <code>Utf8</code> (string) - Dates (future) \u2192 <code>Date</code> or <code>Timestamp</code></p>"},{"location":"user-guide/data-loading/#performance-tips-for-csv","title":"Performance Tips for CSV","text":"<ol> <li> <p>Use Schema Inference Carefully <pre><code># Fast: skip inference for known schema\ndf = spark.read.csv(\"data.csv\", header=True, infer_schema=False)\n</code></pre></p> </li> <li> <p>Convert to Parquet <pre><code># Read CSV once\ndf = spark.read.csv(\"large_file.csv\", header=True)\n\n# TODO: Write to Parquet (not yet implemented)\n# df.write.parquet(\"large_file.parquet\")\n\n# Future reads will be much faster\n# df = spark.read.parquet(\"large_file.parquet\")\n</code></pre></p> </li> </ol>"},{"location":"user-guide/data-loading/#parquet-files","title":"Parquet Files","text":""},{"location":"user-guide/data-loading/#basic-parquet-loading","title":"Basic Parquet Loading","text":"<pre><code># Read single file\ndf = spark.read.parquet(\"data.parquet\")\n\n# Read directory of Parquet files\ndf = spark.read.parquet(\"data_dir/\")\n</code></pre>"},{"location":"user-guide/data-loading/#why-parquet","title":"Why Parquet?","text":"<p>Advantages over CSV: - 10-100x faster for selective queries - 5-10x smaller file size - Schema preserved - no inference needed - Columnar format - only read needed columns - Built-in compression - automatic - Predicate pushdown - filters applied during read</p> <p>Comparison Example:</p> Metric CSV (1GB) Parquet (200MB) File Size 1.0 GB 0.2 GB (5x smaller) Full Read 12.5s 1.2s (10x faster) Select 2 cols 11.8s 0.3s (39x faster) Filter + Select 8.3s 0.2s (41x faster)"},{"location":"user-guide/data-loading/#parquet-partitioning","title":"Parquet Partitioning","text":"<p>Parquet files can be partitioned for even better performance:</p> <pre><code>data_dir/\n  year=2023/\n    month=01/\n      data.parquet\n    month=02/\n      data.parquet\n  year=2024/\n    month=01/\n      data.parquet\n</code></pre> <pre><code># Read all partitions\ndf = spark.read.parquet(\"data_dir/\")\n\n# Filter automatically uses partitions\n# Only reads data from 2024\ndf_2024 = df.filter(\"year == 2024\")\n</code></pre>"},{"location":"user-guide/data-loading/#file-path-patterns","title":"File Path Patterns","text":""},{"location":"user-guide/data-loading/#local-files","title":"Local Files","text":"<pre><code># Absolute path\ndf = spark.read.csv(\"/home/user/data.csv\")\n\n# Relative path\ndf = spark.read.csv(\"./data/input.csv\")\ndf = spark.read.csv(\"data.csv\")\n</code></pre>"},{"location":"user-guide/data-loading/#multiple-files-future","title":"Multiple Files (Future)","text":"<pre><code># Future: Read multiple files\n# df = spark.read.csv(\"data/*.csv\")\n# df = spark.read.csv([\"file1.csv\", \"file2.csv\"])\n</code></pre>"},{"location":"user-guide/data-loading/#data-sources-best-practices","title":"Data Sources Best Practices","text":""},{"location":"user-guide/data-loading/#1-choose-the-right-format","title":"1. Choose the Right Format","text":"Use Case Recommended Format Ad-hoc analysis CSV (easy to create/view) Production pipelines Parquet (fast, compressed) One-time import CSV (then convert to Parquet) Archival storage Parquet (compressed) Human-readable CSV Machine-only Parquet"},{"location":"user-guide/data-loading/#2-optimize-csv-reading","title":"2. Optimize CSV Reading","text":"<pre><code># For large CSVs, consider:\n# 1. Enable schema inference\ndf = spark.read.csv(\"large.csv\", header=True, infer_schema=True)\n\n# 2. Save to Parquet for future use\n# df.write.parquet(\"large.parquet\")  # TODO\n</code></pre>"},{"location":"user-guide/data-loading/#3-leverage-parquet-features","title":"3. Leverage Parquet Features","text":"<pre><code># Parquet automatically optimizes these operations:\n\n# Only reads 'name' and 'age' columns\ndf = spark.read.parquet(\"data.parquet\").select([\"name\", \"age\"])\n\n# Filter is pushed down to file read\ndf = spark.read.parquet(\"data.parquet\").filter(\"age &gt; 18\")\n\n# Combines both optimizations\ndf = spark.read.parquet(\"data.parquet\") \\\n       .select([\"name\", \"age\"]) \\\n       .filter(\"age &gt; 18\")\n</code></pre>"},{"location":"user-guide/data-loading/#common-issues","title":"Common Issues","text":""},{"location":"user-guide/data-loading/#issue-1-file-not-found","title":"Issue 1: File Not Found","text":"<pre><code># Error: file doesn't exist\ndf = spark.read.csv(\"missing.csv\")\n# RuntimeError: Failed to read CSV: ...\n\n# Solution: Check path\nimport os\nprint(os.path.exists(\"data.csv\"))  # True\ndf = spark.read.csv(\"data.csv\")\n</code></pre>"},{"location":"user-guide/data-loading/#issue-2-encoding-problems","title":"Issue 2: Encoding Problems","text":"<pre><code># CSV with special characters may fail\n# Future: add encoding parameter\n# df = spark.read.csv(\"data.csv\", encoding=\"utf-8\")\n</code></pre>"},{"location":"user-guide/data-loading/#issue-3-large-file-memory","title":"Issue 3: Large File Memory","text":"<pre><code># If file is too large, filter early\ndf = spark.read.csv(\"huge.csv\", header=True)\nsubset = df.limit(1000)  # Work with subset first\nsubset.show()\n</code></pre>"},{"location":"user-guide/data-loading/#next-steps","title":"Next Steps","text":"<ul> <li>DataFrame Guide - Working with loaded data</li> <li>Performance Tips - Optimize data loading</li> <li>API Reference - Complete API documentation</li> </ul>"},{"location":"user-guide/dataframe/","title":"DataFrame Guide","text":"<p>A comprehensive guide to working with PyRust DataFrames.</p>"},{"location":"user-guide/dataframe/#what-is-a-dataframe","title":"What is a DataFrame?","text":"<p>A DataFrame is a distributed collection of data organized into named columns. It's conceptually similar to: - A table in a relational database - A Pandas DataFrame (but distributed) - A spreadsheet with named columns</p>"},{"location":"user-guide/dataframe/#creating-dataframes","title":"Creating DataFrames","text":""},{"location":"user-guide/dataframe/#from-files","title":"From Files","text":"<pre><code># CSV\ndf = spark.read.csv(\"data.csv\", header=True, infer_schema=True)\n\n# Parquet\ndf = spark.read.parquet(\"data.parquet\")\n</code></pre>"},{"location":"user-guide/dataframe/#from-memory-future-feature","title":"From Memory (Future Feature)","text":"<pre><code># Not yet implemented\ndata = [[\"Alice\", 25], [\"Bob\", 30]]\ndf = spark.createDataFrame(data, [\"name\", \"age\"])\n</code></pre>"},{"location":"user-guide/dataframe/#dataframe-properties","title":"DataFrame Properties","text":""},{"location":"user-guide/dataframe/#schema","title":"Schema","text":"<p>Every DataFrame has a schema defining column names and types:</p> <pre><code># View schema\ndf.printSchema()\n\n# Output:\n# root\n#  |-- name: Utf8 (nullable = true)\n#  |-- age: Int64 (nullable = true)\n#  |-- salary: Float64 (nullable = true)\n</code></pre> <p>Supported Types: - <code>Utf8</code> - String/text - <code>Int64</code> - 64-bit integer - <code>Float64</code> - 64-bit float - <code>Boolean</code> - true/false - <code>Date</code>, <code>Timestamp</code> - Date/time values</p>"},{"location":"user-guide/dataframe/#row-count","title":"Row Count","text":"<pre><code>count = df.count()  # Triggers execution\nprint(f\"Total rows: {count}\")\n</code></pre>"},{"location":"user-guide/dataframe/#transformations","title":"Transformations","text":"<p>Transformations are lazy - they build a query plan but don't execute until an action is called.</p>"},{"location":"user-guide/dataframe/#select","title":"Select","text":"<p>Project specific columns:</p> <pre><code># Select columns\ndf.select([\"name\", \"age\"])\n\n# Reorder columns\ndf.select([\"age\", \"name\", \"city\"])\n\n# Select single column\ndf.select([\"name\"])\n</code></pre>"},{"location":"user-guide/dataframe/#filter-where","title":"Filter / Where","text":"<p>Keep rows matching a condition:</p> <pre><code># Numeric filters\ndf.filter(\"age &gt; 18\")\ndf.filter(\"salary &gt;= 50000\")\ndf.filter(\"score &lt; 100\")\n\n# String filters\ndf.filter(\"city == 'New York'\")\ndf.filter(\"status != 'inactive'\")\n\n# where() is an alias for filter()\ndf.where(\"age &gt; 18\")\n</code></pre> <p>Limitation: Currently only supports simple conditions (<code>column op value</code>). Complex conditions with AND/OR coming in future versions.</p>"},{"location":"user-guide/dataframe/#sort-orderby","title":"Sort / OrderBy","text":"<p>Sort rows by column values:</p> <pre><code># Sort by single column\ndf.orderBy([\"age\"])\n\n# Sort by multiple columns\ndf.orderBy([\"country\", \"city\", \"name\"])\n\n# sort() is an alias for orderBy()\ndf.sort([\"age\"])\n</code></pre> <p>Note: Currently only ascending order supported. Descending order coming in future versions.</p>"},{"location":"user-guide/dataframe/#limit","title":"Limit","text":"<p>Restrict number of rows:</p> <pre><code># First 10 rows\ndf.limit(10)\n\n# Top 5 after sorting\ndf.orderBy([\"sales\"]).limit(5)\n</code></pre>"},{"location":"user-guide/dataframe/#groupby","title":"GroupBy","text":"<p>Group rows for aggregation:</p> <pre><code># Count by group\ndf.groupBy([\"city\"]).count()\n\n# Multiple aggregations\ndf.groupBy([\"department\"]).agg([\n    (\"salary\", \"avg\"),\n    (\"salary\", \"max\"),\n    (\"age\", \"min\")\n])\n\n# Multi-level grouping\ndf.groupBy([\"country\", \"city\"]).count()\n</code></pre>"},{"location":"user-guide/dataframe/#actions","title":"Actions","text":"<p>Actions trigger execution and return results to Python.</p>"},{"location":"user-guide/dataframe/#show","title":"Show","text":"<p>Display rows in formatted table:</p> <pre><code># Default: 20 rows\ndf.show()\n\n# Custom number\ndf.show(5)\ndf.show(100)\n</code></pre>"},{"location":"user-guide/dataframe/#count","title":"Count","text":"<p>Return row count:</p> <pre><code>total = df.count()\nfiltered_count = df.filter(\"age &gt; 18\").count()\n</code></pre>"},{"location":"user-guide/dataframe/#collect-future","title":"Collect (Future)","text":"<p>Return all data to Python (not yet implemented):</p> <pre><code># Future feature\nrows = df.collect()  # Returns list of rows\n</code></pre>"},{"location":"user-guide/dataframe/#complex-workflows","title":"Complex Workflows","text":""},{"location":"user-guide/dataframe/#example-1-etl-pipeline","title":"Example 1: ETL Pipeline","text":"<pre><code># Extract\nraw_df = spark.read.csv(\"raw_data.csv\", header=True)\n\n# Transform\nclean_df = raw_df.filter(\"valid == true\") \\\n                 .select([\"id\", \"name\", \"amount\", \"date\"]) \\\n                 .filter(\"amount &gt; 0\") \\\n                 .orderBy([\"date\"])\n\n# Load (Future: write to Parquet)\nprint(f\"Processed {clean_df.count()} records\")\nclean_df.show(10)\n</code></pre>"},{"location":"user-guide/dataframe/#example-2-aggregation-report","title":"Example 2: Aggregation Report","text":"<pre><code># Load data\nsales = spark.read.parquet(\"sales.parquet\")\n\n# Multi-level aggregation\nreport = sales.filter(\"year == 2024\") \\\n              .groupBy([\"region\", \"product\"]) \\\n              .agg([\n                  (\"amount\", \"sum\"),\n                  (\"amount\", \"avg\"),\n                  (\"transaction_id\", \"count\")\n              ]) \\\n              .orderBy([\"region\", \"sum(amount)\"])\n\n# Display\nprint(\"2024 Sales Report:\")\nreport.show(50)\n</code></pre>"},{"location":"user-guide/dataframe/#example-3-data-quality-check","title":"Example 3: Data Quality Check","text":"<pre><code># Load data\ndf = spark.read.csv(\"user_data.csv\", header=True)\n\n# Check for data issues\nprint(\"=== Data Quality Report ===\\n\")\n\n# Total records\nprint(f\"Total records: {df.count()}\")\n\n# Age validation\ninvalid_age = df.filter(\"age &lt; 0\")\nprint(f\"Invalid ages: {invalid_age.count()}\")\n\n# Missing cities (assuming empty strings)\nmissing_city = df.filter(\"city == ''\")\nprint(f\"Missing cities: {missing_city.count()}\")\n\n# Show problematic records\nprint(\"\\nProblematic Records:\")\ndf.filter(\"age &lt; 0\").show(10)\n</code></pre>"},{"location":"user-guide/dataframe/#performance-optimization","title":"Performance Optimization","text":""},{"location":"user-guide/dataframe/#filter-pushdown","title":"Filter Pushdown","text":"<p>Apply filters early to reduce data volume:</p> <pre><code># Good - filter early\ndf.filter(\"year == 2024\") \\\n  .filter(\"valid == true\") \\\n  .select([\"name\", \"amount\"]) \\\n  .groupBy([\"name\"]) \\\n  .count()\n\n# Less efficient - filter late\ndf.groupBy([\"name\", \"year\"]) \\\n  .count() \\\n  .filter(\"year == 2024\")\n</code></pre>"},{"location":"user-guide/dataframe/#column-pruning","title":"Column Pruning","text":"<p>Select only needed columns:</p> <pre><code># Good - select early\ndf.select([\"id\", \"amount\"]) \\\n  .filter(\"amount &gt; 1000\") \\\n  .orderBy([\"amount\"])\n\n# Less efficient - carries all columns\ndf.filter(\"amount &gt; 1000\") \\\n  .orderBy([\"amount\"])\n</code></pre>"},{"location":"user-guide/dataframe/#limit-for-exploration","title":"Limit for Exploration","text":"<p>Use limit() when exploring large datasets:</p> <pre><code># Fast preview\ndf.limit(1000).show()\n\n# Test transformations on subset\ndf.limit(10000) \\\n  .filter(\"category == 'electronics'\") \\\n  .groupBy([\"brand\"]) \\\n  .count() \\\n  .show()\n</code></pre>"},{"location":"user-guide/dataframe/#common-patterns","title":"Common Patterns","text":""},{"location":"user-guide/dataframe/#top-n-query","title":"Top N Query","text":"<pre><code># Top 10 highest values\ntop_10 = df.orderBy([\"sales\"]) \\\n           .limit(10) \\\n           .select([\"product\", \"sales\"])\ntop_10.show()\n</code></pre>"},{"location":"user-guide/dataframe/#distinct-count","title":"Distinct Count","text":"<pre><code># Count unique cities\nunique_cities = df.select([\"city\"]) \\\n                  .distinct() \\  # TODO: Not yet implemented\n                  .count()\n</code></pre>"},{"location":"user-guide/dataframe/#summary-statistics","title":"Summary Statistics","text":"<pre><code># Statistics by group\nstats = df.groupBy([\"category\"]).agg([\n    (\"price\", \"min\"),\n    (\"price\", \"max\"),\n    (\"price\", \"avg\"),\n    (\"product_id\", \"count\")\n])\nstats.show()\n</code></pre>"},{"location":"user-guide/dataframe/#working-with-nulls","title":"Working with Nulls","text":""},{"location":"user-guide/dataframe/#handling-nulls","title":"Handling Nulls","text":"<pre><code># Filter out nulls (future feature)\n# df.filter(\"age IS NOT NULL\")\n\n# For now, use value checks\ndf.filter(\"age &gt; 0\")  # Assuming positive ages are valid\n</code></pre>"},{"location":"user-guide/dataframe/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/dataframe/#1-cache-intermediate-results-future","title":"1. Cache Intermediate Results (Future)","text":"<pre><code># Future feature\n# expensive_df = df.filter(...).select(...)\n# expensive_df.cache()  # Reuse in memory\n</code></pre>"},{"location":"user-guide/dataframe/#2-understand-lazy-evaluation","title":"2. Understand Lazy Evaluation","text":"<pre><code># These DON'T execute immediately\nfiltered = df.filter(\"age &gt; 18\")  # Lazy\nselected = filtered.select([\"name\"])  # Lazy\nsorted_df = selected.orderBy([\"name\"])  # Lazy\n\n# This triggers execution of entire chain\nsorted_df.show()  # Action!\n</code></pre>"},{"location":"user-guide/dataframe/#3-avoid-repeated-actions","title":"3. Avoid Repeated Actions","text":"<pre><code># Bad - counts twice (executes query twice)\ncount1 = df.filter(\"age &gt; 18\").count()\ncount2 = df.filter(\"age &gt; 18\").count()\n\n# Good - store result\nfiltered = df.filter(\"age &gt; 18\")\ncount = filtered.count()\n</code></pre>"},{"location":"user-guide/dataframe/#4-use-parquet-for-production","title":"4. Use Parquet for Production","text":"<pre><code># CSV: Good for ad-hoc analysis\ndf = spark.read.csv(\"data.csv\")\n\n# Parquet: Better for production (faster, smaller)\ndf = spark.read.parquet(\"data.parquet\")\n</code></pre>"},{"location":"user-guide/dataframe/#debugging","title":"Debugging","text":""},{"location":"user-guide/dataframe/#view-query-plan-future","title":"View Query Plan (Future)","text":"<pre><code># Future feature: df.explain()\n</code></pre>"},{"location":"user-guide/dataframe/#print-intermediate-results","title":"Print Intermediate Results","text":"<pre><code># Check data at each step\nstep1 = df.filter(\"age &gt; 18\")\nprint(f\"After filter: {step1.count()}\")\n\nstep2 = step1.select([\"name\", \"city\"])\nstep2.show(5)\n\nstep3 = step2.groupBy([\"city\"]).count()\nstep3.show()\n</code></pre>"},{"location":"user-guide/dataframe/#next-steps","title":"Next Steps","text":"<ul> <li>Data Loading Guide - More on reading data</li> <li>Performance Tips - Optimize your queries</li> <li>API Reference - Complete DataFrame API</li> </ul>"},{"location":"user-guide/joins/","title":"Joins","text":"<p>PyRust provides comprehensive support for joining DataFrames using various join strategies. This guide covers all join types and best practices.</p>"},{"location":"user-guide/joins/#overview","title":"Overview","text":"<p>Joins combine rows from two DataFrames based on a common column (or columns). PyRust supports all standard SQL join types.</p>"},{"location":"user-guide/joins/#join-types","title":"Join Types","text":""},{"location":"user-guide/joins/#inner-join-default","title":"Inner Join (default)","text":"<p>Returns only rows that have matching values in both DataFrames:</p> <pre><code># Inner join on single column\nresult = df1.join(df2, on=\"user_id\")\n\n# Explicit inner join\nresult = df1.join(df2, on=\"user_id\", how=\"inner\")\n\n# Join on multiple columns\nresult = df1.join(df2, on=[\"country\", \"city\"])\n</code></pre> <p>When to use: - You only want rows that exist in both DataFrames - Finding matching records between two datasets - Most common join type</p>"},{"location":"user-guide/joins/#left-join-left-outer","title":"Left Join (Left Outer)","text":"<p>Returns all rows from the left DataFrame, with matching rows from the right (nulls where no match):</p> <pre><code>result = df1.join(df2, on=\"user_id\", how=\"left\")\n</code></pre> <p>When to use: - Keep all records from the primary DataFrame - Add optional information from another DataFrame - Find which left records have no match (nulls in right columns)</p>"},{"location":"user-guide/joins/#right-join-right-outer","title":"Right Join (Right Outer)","text":"<p>Returns all rows from the right DataFrame, with matching rows from the left (nulls where no match):</p> <pre><code>result = df1.join(df2, on=\"user_id\", how=\"right\")\n</code></pre> <p>When to use: - Keep all records from the secondary DataFrame - Less common (can usually be rewritten as left join by swapping DataFrames)</p>"},{"location":"user-guide/joins/#full-outer-join","title":"Full Outer Join","text":"<p>Returns all rows from both DataFrames, with nulls where there's no match:</p> <pre><code>result = df1.join(df2, on=\"user_id\", how=\"outer\")\n\n# 'full' is an alias for 'outer'\nresult = df1.join(df2, on=\"user_id\", how=\"full\")\n</code></pre> <p>When to use: - Find all records from both DataFrames - Identify records unique to each DataFrame - Data reconciliation and comparison</p>"},{"location":"user-guide/joins/#semi-join","title":"Semi Join","text":"<p>Returns rows from the left DataFrame where there's a match in the right (only left columns):</p> <pre><code>result = df1.join(df2, on=\"user_id\", how=\"semi\")\n</code></pre> <p>When to use: - Filter left DataFrame based on existence in right - More efficient than inner join when you don't need right columns - \"WHERE EXISTS\" style queries</p>"},{"location":"user-guide/joins/#anti-join","title":"Anti Join","text":"<p>Returns rows from the left DataFrame where there's NO match in the right:</p> <pre><code>result = df1.join(df2, on=\"user_id\", how=\"anti\")\n</code></pre> <p>When to use: - Find records that don't have a match - Data validation (find missing records) - \"WHERE NOT EXISTS\" style queries</p>"},{"location":"user-guide/joins/#basic-examples","title":"Basic Examples","text":""},{"location":"user-guide/joins/#simple-join","title":"Simple Join","text":"<pre><code>from pyrust import SparkSession\n\nspark = SparkSession.builder.appName(\"JoinExample\").getOrCreate()\n\n# Load datasets\nusers = spark.read.csv(\"users.csv\", header=True)\norders = spark.read.csv(\"orders.csv\", header=True)\n\n# Join on user_id\nresult = users.join(orders, on=\"user_id\")\nresult.show()\n</code></pre>"},{"location":"user-guide/joins/#multi-column-join","title":"Multi-Column Join","text":"<pre><code># Join on multiple columns\ndf1 = spark.read.csv(\"sales_2023.csv\", header=True)\ndf2 = spark.read.csv(\"sales_2024.csv\", header=True)\n\n# Join on both country and city\nresult = df1.join(df2, on=[\"country\", \"city\"])\n</code></pre>"},{"location":"user-guide/joins/#string-vs-list","title":"String vs List","text":"<pre><code># Single column - can use string\nresult = df1.join(df2, on=\"id\")\n\n# Multiple columns - use list\nresult = df1.join(df2, on=[\"country\", \"city\"])\n</code></pre>"},{"location":"user-guide/joins/#join-patterns","title":"Join Patterns","text":""},{"location":"user-guide/joins/#enriching-data","title":"Enriching Data","text":"<pre><code># Add customer details to orders\norders = spark.read.csv(\"orders.csv\", header=True)\ncustomers = spark.read.csv(\"customers.csv\", header=True)\n\nenriched = orders.join(customers, on=\"customer_id\", how=\"left\")\nenriched.show()\n</code></pre>"},{"location":"user-guide/joins/#finding-missing-records","title":"Finding Missing Records","text":"<pre><code># Find orders without shipping info\norders = spark.read.csv(\"orders.csv\", header=True)\nshipments = spark.read.csv(\"shipments.csv\", header=True)\n\nmissing_shipments = orders.join(shipments, on=\"order_id\", how=\"anti\")\nprint(f\"Orders not yet shipped: {missing_shipments.count()}\")\n</code></pre>"},{"location":"user-guide/joins/#data-validation","title":"Data Validation","text":"<pre><code># Find records in both datasets (validation)\nexpected = spark.read.csv(\"expected.csv\", header=True)\nactual = spark.read.csv(\"actual.csv\", header=True)\n\n# Records in both\ncorrect = expected.join(actual, on=\"id\", how=\"inner\")\n\n# Records only in expected (missing from actual)\nmissing = expected.join(actual, on=\"id\", how=\"anti\")\n\n# Records only in actual (extra data)\nextra = actual.join(expected, on=\"id\", how=\"anti\")\n</code></pre>"},{"location":"user-guide/joins/#combining-with-filters","title":"Combining with Filters","text":"<pre><code># Join then filter\nresult = users.join(orders, on=\"user_id\") \\\n              .filter(\"order_amount &gt; 100\")\n\n# Filter before join (more efficient)\nhigh_value_orders = orders.filter(\"amount &gt; 100\")\nresult = users.join(high_value_orders, on=\"user_id\")\n</code></pre>"},{"location":"user-guide/joins/#chaining-joins","title":"Chaining Joins","text":"<pre><code># Join multiple DataFrames\nusers = spark.read.csv(\"users.csv\", header=True)\norders = spark.read.csv(\"orders.csv\", header=True)\nproducts = spark.read.csv(\"products.csv\", header=True)\n\nresult = users.join(orders, on=\"user_id\") \\\n              .join(products, on=\"product_id\")\n</code></pre>"},{"location":"user-guide/joins/#column-deduplication","title":"Column Deduplication","text":"<p>PyRust automatically deduplicates join key columns:</p> <pre><code>df1 = spark.read.csv(\"users.csv\", header=True)  # columns: id, name, age\ndf2 = spark.read.csv(\"details.csv\", header=True)  # columns: id, email, phone\n\n# Join on 'id' - the result will have 'id' only once\nresult = df1.join(df2, on=\"id\")\n# Result columns: id, name, age, email, phone\n</code></pre> <p>For joins, the join key appears only once in the result (from the left DataFrame).</p>"},{"location":"user-guide/joins/#handling-column-name-conflicts","title":"Handling Column Name Conflicts","text":"<p>If both DataFrames have columns with the same name (besides join keys), rename them first:</p> <pre><code># Both have 'status' column\norders = orders.withColumnRenamed(\"status\", \"order_status\")\nshipments = shipments.withColumnRenamed(\"status\", \"shipment_status\")\n\n# Now join without conflicts\nresult = orders.join(shipments, on=\"order_id\")\n</code></pre>"},{"location":"user-guide/joins/#performance-tips","title":"Performance Tips","text":""},{"location":"user-guide/joins/#1-filter-before-joining","title":"1. Filter Before Joining","text":"<pre><code># Good - reduce data before join\nfiltered_orders = orders.filter(\"amount &gt; 100\")\nresult = users.join(filtered_orders, on=\"user_id\")\n\n# Less efficient - filter after join\nresult = users.join(orders, on=\"user_id\").filter(\"amount &gt; 100\")\n</code></pre>"},{"location":"user-guide/joins/#2-use-semianti-for-filtering","title":"2. Use Semi/Anti for Filtering","text":"<pre><code># If you only need to filter (not add columns)\n# Use semi join instead of inner join\nactive_users = users.join(recent_orders, on=\"user_id\", how=\"semi\")\n\n# More efficient than\nactive_users = users.join(recent_orders, on=\"user_id\") \\\n                    .select(\"user_id\", \"name\", \"email\") \\\n                    .distinct()\n</code></pre>"},{"location":"user-guide/joins/#3-join-on-specific-columns","title":"3. Join on Specific Columns","text":"<pre><code># Better - join on indexed/sorted columns\nresult = df1.join(df2, on=\"id\")\n\n# Less efficient - joining on multiple columns\nresult = df1.join(df2, on=[\"name\", \"email\", \"phone\"])\n</code></pre>"},{"location":"user-guide/joins/#4-order-matters-for-right-join","title":"4. Order Matters for Right Join","text":"<pre><code># These are NOT equivalent\nresult1 = small_df.join(large_df, on=\"id\", how=\"left\")\nresult2 = large_df.join(small_df, on=\"id\", how=\"right\")\n\n# But result1 \u2248 result2 conceptually (column order differs)\n</code></pre>"},{"location":"user-guide/joins/#common-patterns","title":"Common Patterns","text":""},{"location":"user-guide/joins/#one-to-many-join","title":"One-to-Many Join","text":"<pre><code># One customer, many orders\ncustomers = spark.read.csv(\"customers.csv\", header=True)\norders = spark.read.csv(\"orders.csv\", header=True)\n\nresult = customers.join(orders, on=\"customer_id\")\n# Each customer row is repeated for each order\n</code></pre>"},{"location":"user-guide/joins/#many-to-many-join","title":"Many-to-Many Join","text":"<pre><code># Students to courses (via enrollment table)\nstudents = spark.read.csv(\"students.csv\", header=True)\ncourses = spark.read.csv(\"courses.csv\", header=True)\nenrollments = spark.read.csv(\"enrollments.csv\", header=True)\n\nresult = students.join(enrollments, on=\"student_id\") \\\n                 .join(courses, on=\"course_id\")\n</code></pre>"},{"location":"user-guide/joins/#self-join","title":"Self Join","text":"<pre><code># Find pairs of users from same city\nusers = spark.read.csv(\"users.csv\", header=True)\n\n# Rename columns to avoid conflicts\nusers1 = users.withColumnRenamed(\"name\", \"user1_name\")\nusers2 = users.withColumnRenamed(\"name\", \"user2_name\")\n\n# Join on city\npairs = users1.join(users2, on=\"city\")\n</code></pre>"},{"location":"user-guide/joins/#lookupdimension-table","title":"Lookup/Dimension Table","text":"<pre><code># Enrich fact table with dimension data\nfact_table = spark.read.csv(\"sales.csv\", header=True)\ndim_product = spark.read.csv(\"products.csv\", header=True)\ndim_customer = spark.read.csv(\"customers.csv\", header=True)\n\nenriched = fact_table.join(dim_product, on=\"product_id\", how=\"left\") \\\n                     .join(dim_customer, on=\"customer_id\", how=\"left\")\n</code></pre>"},{"location":"user-guide/joins/#aggregations-after-join","title":"Aggregations After Join","text":"<pre><code># Join then aggregate\nresult = orders.join(customers, on=\"customer_id\") \\\n               .groupBy(\"country\") \\\n               .agg((\"order_amount\", \"sum\"))\n\n# Show total sales by country\nresult.show()\n</code></pre>"},{"location":"user-guide/joins/#error-handling","title":"Error Handling","text":""},{"location":"user-guide/joins/#missing-join-keys","title":"Missing Join Keys","text":"<pre><code>try:\n    result = df1.join(df2, on=None)\nexcept RuntimeError:\n    print(\"Join keys are required\")\n</code></pre>"},{"location":"user-guide/joins/#invalid-join-type","title":"Invalid Join Type","text":"<pre><code>try:\n    result = df1.join(df2, on=\"id\", how=\"invalid\")\nexcept RuntimeError:\n    print(\"Invalid join type. Use: inner, left, right, outer, semi, anti\")\n</code></pre>"},{"location":"user-guide/joins/#comparison-with-pyspark","title":"Comparison with PySpark","text":"<p>PyRust join API is compatible with PySpark:</p> <pre><code># These work the same in PyRust and PySpark\ndf1.join(df2, on=\"id\")\ndf1.join(df2, on=\"id\", how=\"left\")\ndf1.join(df2, on=[\"col1\", \"col2\"])\n</code></pre> <p>Differences: - PyRust doesn't support join conditions (expressions) yet - only column names - No broadcast joins yet (optimization for small tables)</p>"},{"location":"user-guide/joins/#method-summary","title":"Method Summary","text":"Join Type Method Call Description Inner <code>df1.join(df2, on=\"id\")</code> Only matching rows Left <code>df1.join(df2, on=\"id\", how=\"left\")</code> All left + matches Right <code>df1.join(df2, on=\"id\", how=\"right\")</code> All right + matches Outer/Full <code>df1.join(df2, on=\"id\", how=\"outer\")</code> All rows from both Semi <code>df1.join(df2, on=\"id\", how=\"semi\")</code> Left where match exists Anti <code>df1.join(df2, on=\"id\", how=\"anti\")</code> Left where no match"},{"location":"user-guide/joins/#see-also","title":"See Also","text":"<ul> <li>Set Operations - Union, intersect, except</li> <li>SQL Queries - SQL-based joins</li> <li>Performance Tips - Join optimization strategies</li> </ul>"},{"location":"user-guide/performance/","title":"Performance Tips","text":"<p>Optimize your PyRust workflows for maximum performance.</p>"},{"location":"user-guide/performance/#pyrust-performance-profile","title":"PyRust Performance Profile","text":""},{"location":"user-guide/performance/#speed-advantages","title":"Speed Advantages","text":"<p>PyRust is typically 10-50x faster than PySpark:</p> Operation PySpark PyRust Speedup CSV Read 12.5s 1.2s 10.4x Filter + Select 8.3s 0.4s 20.8x GroupBy + Aggregate 15.2s 0.8s 19.0x Multi-stage Pipeline 45.6s 2.1s 21.7x"},{"location":"user-guide/performance/#why-so-fast","title":"Why So Fast?","text":"<ol> <li>Rust Performance - Compiled, not interpreted</li> <li>Arrow Columnar Format - Vectorized operations</li> <li>DataFusion Optimizer - Query plan optimization</li> <li>Zero-Copy - No Python/Rust boundary overhead</li> <li>Parallel Execution - Automatic parallelization</li> </ol>"},{"location":"user-guide/performance/#general-optimization-strategies","title":"General Optimization Strategies","text":""},{"location":"user-guide/performance/#1-choose-the-right-file-format","title":"1. Choose the Right File Format","text":""},{"location":"user-guide/performance/#parquet-vs-csv","title":"Parquet vs CSV","text":"<p>Use Parquet for: - Production pipelines - Large datasets (&gt;100MB) - Repeated queries - Column-selective queries</p> <p>Parquet advantages: <pre><code># Read Parquet: 10-100x faster\ndf = spark.read.parquet(\"data.parquet\")\n\n# Only reads needed columns (column pruning)\ndf.select([\"name\", \"age\"]).show()\n\n# Filters applied during read (predicate pushdown)\ndf.filter(\"age &gt; 18\").show()\n</code></pre></p> <p>Use CSV for: - Ad-hoc analysis - Human-readable data - One-time imports - Small files (&lt;10MB)</p>"},{"location":"user-guide/performance/#2-filter-early-and-often","title":"2. Filter Early and Often","text":"<p>Push filters as early as possible in your transformation chain:</p> <pre><code># \u2705 GOOD - Filter early\ndf.filter(\"year == 2024\") \\\n  .filter(\"status == 'active'\") \\\n  .select([\"name\", \"amount\"]) \\\n  .groupBy([\"name\"]) \\\n  .count()\n\n# \u274c BAD - Filter late\ndf.groupBy([\"name\", \"year\"]) \\\n  .count() \\\n  .filter(\"year == 2024\")\n</code></pre> <p>Impact: Filtering early can reduce execution time by 5-20x.</p>"},{"location":"user-guide/performance/#3-select-only-needed-columns","title":"3. Select Only Needed Columns","text":"<p>Column pruning reduces I/O and memory:</p> <pre><code># \u2705 GOOD - Select early\ndf.select([\"id\", \"amount\"]) \\\n  .filter(\"amount &gt; 1000\") \\\n  .groupBy([\"id\"]) \\\n  .count()\n\n# \u274c BAD - Carries unnecessary columns\ndf.filter(\"amount &gt; 1000\") \\\n  .groupBy([\"id\"]) \\\n  .count()\n</code></pre> <p>Impact: 2-5x faster with Parquet files.</p>"},{"location":"user-guide/performance/#4-minimize-sorting","title":"4. Minimize Sorting","text":"<p>Sorting is expensive - only sort when necessary:</p> <pre><code># \u2705 GOOD - Sort only final result\ndf.filter(\"active == true\") \\\n  .groupBy([\"category\"]) \\\n  .count() \\\n  .orderBy([\"count\"])  # Sort small result\n\n# \u274c BAD - Unnecessary sort\ndf.orderBy([\"name\"]) \\  # Sorts all data\n  .filter(\"active == true\") \\\n  .groupBy([\"category\"]) \\\n  .count()\n</code></pre>"},{"location":"user-guide/performance/#5-use-limit-for-exploration","title":"5. Use Limit for Exploration","text":"<p>When exploring data, use <code>limit()</code> liberally:</p> <pre><code># Quick preview\ndf.limit(1000).show()\n\n# Test transformations on subset\ndf.limit(10000) \\\n  .filter(\"category == 'electronics'\") \\\n  .groupBy([\"brand\"]) \\\n  .count() \\\n  .show()\n</code></pre>"},{"location":"user-guide/performance/#operation-specific-optimizations","title":"Operation-Specific Optimizations","text":""},{"location":"user-guide/performance/#reading-data","title":"Reading Data","text":"<pre><code># \u2705 GOOD - Parquet with schema\ndf = spark.read.parquet(\"data.parquet\")\n\n# \u26a0\ufe0f OK - CSV with schema inference\ndf = spark.read.csv(\"data.csv\", header=True, infer_schema=True)\n\n# \u274c SLOW - CSV without schema\ndf = spark.read.csv(\"data.csv\", header=False, infer_schema=False)\n</code></pre>"},{"location":"user-guide/performance/#filtering","title":"Filtering","text":"<pre><code># \u2705 GOOD - Simple conditions\ndf.filter(\"age &gt; 18\")\ndf.filter(\"salary &gt;= 50000\")\n\n# \u26a0\ufe0f Future - Complex conditions\n# df.filter(\"age &gt; 18 AND salary &gt;= 50000\")\n</code></pre>"},{"location":"user-guide/performance/#grouping","title":"Grouping","text":"<pre><code># \u2705 GOOD - Single aggregation\ndf.groupBy([\"city\"]).count()\n\n# \u2705 BETTER - Multiple aggregations in one pass\ndf.groupBy([\"city\"]).agg([\n    (\"age\", \"avg\"),\n    (\"salary\", \"avg\"),\n    (\"id\", \"count\")\n])\n\n# \u274c BAD - Separate aggregations\navg_age = df.groupBy([\"city\"]).agg([(\"age\", \"avg\")])\navg_salary = df.groupBy([\"city\"]).agg([(\"salary\", \"avg\")])\ncount_records = df.groupBy([\"city\"]).count()\n</code></pre>"},{"location":"user-guide/performance/#sorting","title":"Sorting","text":"<pre><code># \u2705 GOOD - Sort + Limit (Top N)\ndf.orderBy([\"sales\"]).limit(10)\n\n# \u274c BAD - Sort all data unnecessarily\ndf.orderBy([\"name\"])  # If order doesn't matter downstream\n</code></pre>"},{"location":"user-guide/performance/#memory-optimization","title":"Memory Optimization","text":""},{"location":"user-guide/performance/#understanding-memory-usage","title":"Understanding Memory Usage","text":"<p>PyRust uses Apache Arrow's columnar format, which is very memory-efficient:</p> <ul> <li>Columnar layout reduces memory fragmentation</li> <li>Compression reduces memory footprint</li> <li>Zero-copy operations avoid duplication</li> </ul>"},{"location":"user-guide/performance/#tips-for-large-datasets","title":"Tips for Large Datasets","text":"<ol> <li> <p>Stream Processing <pre><code># Process in chunks with limit\nfor offset in range(0, total, 100000):\n    chunk = df.limit(100000).offset(offset)  # Future: offset\n    process_chunk(chunk)\n</code></pre></p> </li> <li> <p>Select Early <pre><code># Don't: Load all columns\ndf = spark.read.parquet(\"huge.parquet\")\n\n# Do: Select needed columns\ndf = spark.read.parquet(\"huge.parquet\").select([\"id\", \"amount\"])\n</code></pre></p> </li> <li> <p>Filter Early <pre><code># Don't: Filter after loading\ndf = spark.read.parquet(\"huge.parquet\")\nfiltered = df.filter(\"year == 2024\")\n\n# Do: Filter during read (predicate pushdown)\ndf = spark.read.parquet(\"huge.parquet\")\n# Filter applied at read time automatically\nfiltered = df.filter(\"year == 2024\")\n</code></pre></p> </li> </ol>"},{"location":"user-guide/performance/#query-optimization-patterns","title":"Query Optimization Patterns","text":""},{"location":"user-guide/performance/#pattern-1-etl-pipeline","title":"Pattern 1: ETL Pipeline","text":"<pre><code># Optimized ETL\nclean_df = spark.read.parquet(\"raw.parquet\") \\  # Fast read\n             .filter(\"valid == true\") \\           # Early filter\n             .select([\"id\", \"amount\", \"date\"]) \\  # Column pruning\n             .filter(\"amount &gt; 0\") \\              # More filtering\n             .orderBy([\"date\"]) \\                 # Sort if needed\n             .limit(10000)                        # Limit output\n\nprint(f\"Processed: {clean_df.count()}\")\n</code></pre>"},{"location":"user-guide/performance/#pattern-2-aggregation-report","title":"Pattern 2: Aggregation Report","text":"<pre><code># Optimized aggregation\nreport = spark.read.parquet(\"sales.parquet\") \\\n              .filter(\"year == 2024\") \\              # Early filter\n              .select([\"region\", \"product\", \"amount\"]) \\  # Select needed\n              .groupBy([\"region\", \"product\"]) \\      # Group\n              .agg([(\"amount\", \"sum\"), (\"amount\", \"avg\")]) \\  # Multi-agg\n              .orderBy([\"sum(amount)\"])              # Sort result\n\nreport.show(50)\n</code></pre>"},{"location":"user-guide/performance/#pattern-3-data-quality","title":"Pattern 3: Data Quality","text":"<pre><code># Efficient validation\nvalidation = spark.read.parquet(\"data.parquet\") \\\n                  .select([\"id\", \"age\", \"email\"]) \\  # Select validators\n                  .filter(\"age &gt; 0\") \\                # Quick filter\n                  .filter(\"age &lt; 150\")                # Range check\n\nprint(f\"Valid records: {validation.count()}\")\n</code></pre>"},{"location":"user-guide/performance/#benchmarking-your-code","title":"Benchmarking Your Code","text":""},{"location":"user-guide/performance/#measure-performance","title":"Measure Performance","text":"<pre><code>import time\n\n# Time a query\nstart = time.time()\nresult = df.filter(\"age &gt; 18\").groupBy([\"city\"]).count()\nresult.show()\nelapsed = time.time() - start\nprint(f\"Query took: {elapsed:.2f}s\")\n</code></pre>"},{"location":"user-guide/performance/#compare-approaches","title":"Compare Approaches","text":"<pre><code># Approach 1: Filter late\nstart = time.time()\ndf.groupBy([\"city\"]).count().filter(\"count &gt; 100\").show()\ntime1 = time.time() - start\n\n# Approach 2: Filter early\nstart = time.time()\ndf.filter(\"active == true\").groupBy([\"city\"]).count().show()\ntime2 = time.time() - start\n\nprint(f\"Approach 1: {time1:.2f}s\")\nprint(f\"Approach 2: {time2:.2f}s ({time1/time2:.1f}x faster)\")\n</code></pre>"},{"location":"user-guide/performance/#performance-checklist","title":"Performance Checklist","text":"<p>Before running queries on large datasets:</p> <ul> <li> Using Parquet format? (vs CSV)</li> <li> Filtering as early as possible?</li> <li> Selecting only needed columns?</li> <li> Combining multiple aggregations in one <code>agg()</code> call?</li> <li> Avoiding unnecessary sorts?</li> <li> Using <code>limit()</code> for exploration?</li> <li> Testing on small subset first?</li> </ul>"},{"location":"user-guide/performance/#common-performance-pitfalls","title":"Common Performance Pitfalls","text":""},{"location":"user-guide/performance/#pitfall-1-late-filtering","title":"Pitfall 1: Late Filtering","text":"<pre><code># \u274c Processes all data before filtering\ndf.groupBy([\"category\"]).count().filter(\"count &gt; 1000\")\n\n# \u2705 Filters early\ndf.filter(\"active == true\").groupBy([\"category\"]).count()\n</code></pre>"},{"location":"user-guide/performance/#pitfall-2-multiple-aggregations","title":"Pitfall 2: Multiple Aggregations","text":"<pre><code># \u274c Three separate queries\navg1 = df.groupBy([\"dept\"]).agg([(\"salary\", \"avg\")])\nmax1 = df.groupBy([\"dept\"]).agg([(\"salary\", \"max\")])\ncount1 = df.groupBy([\"dept\"]).count()\n\n# \u2705 Single query\nstats = df.groupBy([\"dept\"]).agg([\n    (\"salary\", \"avg\"),\n    (\"salary\", \"max\"),\n    (\"id\", \"count\")\n])\n</code></pre>"},{"location":"user-guide/performance/#pitfall-3-unnecessary-sorts","title":"Pitfall 3: Unnecessary Sorts","text":"<pre><code># \u274c Sorts before filter\ndf.orderBy([\"name\"]).filter(\"age &gt; 18\").show()\n\n# \u2705 Filter first (might make sort unnecessary)\ndf.filter(\"age &gt; 18\").show()  # No sort needed for show()\n</code></pre>"},{"location":"user-guide/performance/#pitfall-4-reading-all-columns","title":"Pitfall 4: Reading All Columns","text":"<pre><code># \u274c Reads all columns from Parquet\ndf = spark.read.parquet(\"wide_table.parquet\")\ndf.select([\"id\", \"name\"]).show()\n\n# \u2705 Column pruning automatically applied\ndf = spark.read.parquet(\"wide_table.parquet\") \\\n          .select([\"id\", \"name\"])  # Only reads 2 columns\ndf.show()\n</code></pre>"},{"location":"user-guide/performance/#next-steps","title":"Next Steps","text":"<ul> <li>DataFrame Guide - Master DataFrame operations</li> <li>Transformations - Learn transformation patterns</li> <li>Architecture - Understand internals</li> </ul>"},{"location":"user-guide/set-operations/","title":"Data Operations","text":"<p>PyRust provides comprehensive data manipulation operations for working with DataFrames. This guide covers operations for removing duplicates, combining DataFrames, and performing set operations.</p>"},{"location":"user-guide/set-operations/#removing-duplicates","title":"Removing Duplicates","text":""},{"location":"user-guide/set-operations/#distinct","title":"distinct()","text":"<p>Remove all duplicate rows from a DataFrame:</p> <pre><code># Remove duplicate rows (considers all columns)\nunique_df = df.distinct()\n</code></pre> <p>The <code>distinct()</code> method: - Compares all columns when determining duplicates - Returns a new DataFrame with only unique rows - Order of rows is not guaranteed</p>"},{"location":"user-guide/set-operations/#dropduplicates","title":"dropDuplicates()","text":"<p>Remove duplicates based on specific columns:</p> <pre><code># Remove duplicates based on specific columns\ndf.dropDuplicates(['name'])           # By single column\ndf.dropDuplicates(['name', 'city'])   # By multiple columns\ndf.dropDuplicates()                   # All columns (same as distinct)\n</code></pre> <p>Alias: You can also use <code>drop_duplicates()</code> (snake_case) for compatibility.</p> <p>Parameters: - <code>subset</code>: List of column names to consider for deduplication - If <code>None</code> or empty, all columns are used</p> <p>Examples:</p> <pre><code>from pyrust import SparkSession\n\nspark = SparkSession.builder.appName(\"DedupeExample\").getOrCreate()\n\n# Sample data with duplicates\ndf = spark.read.csv(\"users.csv\", header=True)\n\n# Keep only one row per unique name\nunique_names = df.dropDuplicates(['name'])\n\n# Keep only one row per unique (name, city) combination\nunique_pairs = df.dropDuplicates(['name', 'city'])\n\n# Remove all duplicate rows\ncompletely_unique = df.dropDuplicates()\n</code></pre>"},{"location":"user-guide/set-operations/#combining-dataframes","title":"Combining DataFrames","text":""},{"location":"user-guide/set-operations/#union-unionall","title":"union() / unionAll()","text":"<p>Combine two DataFrames vertically (stack rows):</p> <pre><code># Combine DataFrames (keeps all rows including duplicates)\ncombined = df1.union(df2)\n\n# unionAll() is an alias for union()\ncombined = df1.unionAll(df2)\n</code></pre> <p>Requirements: - Both DataFrames must have the same schema (column names and types) - Column order must match</p> <p>Behavior: - All rows from both DataFrames are included - Duplicates are kept (equivalent to SQL <code>UNION ALL</code>) - To remove duplicates after union, use <code>.distinct()</code></p> <p>Examples:</p> <pre><code># Combine two datasets\ndf1 = spark.read.csv(\"data_2023.csv\", header=True)\ndf2 = spark.read.csv(\"data_2024.csv\", header=True)\n\n# Combine keeping all rows\nall_data = df1.union(df2)\nprint(f\"Total rows: {all_data.count()}\")\n\n# Combine and remove duplicates\nunique_data = df1.union(df2).distinct()\nprint(f\"Unique rows: {unique_data.count()}\")\n</code></pre>"},{"location":"user-guide/set-operations/#set-operations","title":"Set Operations","text":""},{"location":"user-guide/set-operations/#intersect","title":"intersect()","text":"<p>Find rows that exist in both DataFrames:</p> <pre><code># Get rows common to both DataFrames\ncommon_rows = df1.intersect(df2)\n</code></pre> <p>Behavior: - Returns rows that appear in both DataFrames - Similar to SQL <code>INTERSECT</code> - May keep some duplicates (use <code>.distinct()</code> for truly unique results)</p> <p>Examples:</p> <pre><code># Find common users between two datasets\nusers_2023 = spark.read.csv(\"users_2023.csv\", header=True)\nusers_2024 = spark.read.csv(\"users_2024.csv\", header=True)\n\n# Users in both years\nreturning_users = users_2023.intersect(users_2024)\nreturning_users.show()\n\n# Ensure unique results\nunique_returning = users_2023.intersect(users_2024).distinct()\n</code></pre>"},{"location":"user-guide/set-operations/#exceptall-subtract","title":"exceptAll() / subtract()","text":"<p>Find rows in the first DataFrame that don't exist in the second:</p> <pre><code># Get rows in df1 but not in df2\ndifference = df1.exceptAll(df2)\n\n# subtract() is an alias\ndifference = df1.subtract(df2)\n</code></pre> <p>Behavior: - Returns rows from the first DataFrame that don't appear in the second - Equivalent to SQL <code>EXCEPT</code> - Duplicates are removed from the result</p> <p>Examples:</p> <pre><code># Find users who left\nall_users = spark.read.csv(\"all_users.csv\", header=True)\nactive_users = spark.read.csv(\"active_users.csv\", header=True)\n\n# Users who are no longer active\ninactive_users = all_users.exceptAll(active_users)\ninactive_users.show()\n\n# Also works with subtract()\ninactive_users = all_users.subtract(active_users)\n</code></pre>"},{"location":"user-guide/set-operations/#chaining-operations","title":"Chaining Operations","text":"<p>All operations can be chained together:</p> <pre><code># Complex chain of operations\nresult = (\n    df1.union(df2)           # Combine DataFrames\n    .distinct()               # Remove duplicates\n    .filter(\"age &gt; 25\")       # Filter rows\n    .orderBy(\"name\")          # Sort\n    .limit(100)               # Limit results\n)\n\n# Another example: data comparison\nhigh_earners = df.filter(\"salary &gt; 100000\")\nlong_tenure = df.filter(\"years_employed &gt; 5\")\n\n# People with high salary but short tenure\nnew_high_earners = high_earners.exceptAll(long_tenure)\n\n# People with both high salary and long tenure\nsenior_high_earners = high_earners.intersect(long_tenure).distinct()\n</code></pre>"},{"location":"user-guide/set-operations/#performance-tips","title":"Performance Tips","text":"<ol> <li> <p>Use distinct() early: Apply deduplication before expensive operations    <pre><code># Good\ndf.distinct().filter(\"complex_condition\").join(other)\n\n# Less efficient\ndf.filter(\"complex_condition\").join(other).distinct()\n</code></pre></p> </li> <li> <p>Use dropDuplicates() with specific columns: More efficient than distinct()    <pre><code># More efficient if you only care about name uniqueness\ndf.dropDuplicates(['name'])\n\n# Less efficient - checks all columns\ndf.distinct()\n</code></pre></p> </li> <li> <p>Union with distinct only when needed: union() is cheap, distinct() is expensive    <pre><code># If you know there are no duplicates\ndf1.union(df2)  # Fast\n\n# Only if you need unique rows\ndf1.union(df2).distinct()  # Slower\n</code></pre></p> </li> <li> <p>Order matters for except(): <code>df1.exceptAll(df2)</code> \u2260 <code>df2.exceptAll(df1)</code> <pre><code># Rows in A but not in B\na_only = df_a.exceptAll(df_b)\n\n# Rows in B but not in A\nb_only = df_b.exceptAll(df_a)\n</code></pre></p> </li> </ol>"},{"location":"user-guide/set-operations/#common-patterns","title":"Common Patterns","text":""},{"location":"user-guide/set-operations/#deduplication","title":"Deduplication","text":"<pre><code># Keep first occurrence based on timestamp\ndf.orderBy(\"timestamp\").dropDuplicates(['user_id'])\n\n# Remove exact duplicate rows\ndf.distinct()\n\n# Remove duplicates by key columns only\ndf.dropDuplicates(['id', 'date'])\n</code></pre>"},{"location":"user-guide/set-operations/#combining-multiple-sources","title":"Combining Multiple Sources","text":"<pre><code># Combine data from multiple files\ndf_list = [\n    spark.read.csv(\"data_jan.csv\", header=True),\n    spark.read.csv(\"data_feb.csv\", header=True),\n    spark.read.csv(\"data_mar.csv\", header=True),\n]\n\n# Union all\nfrom functools import reduce\ncombined = reduce(lambda a, b: a.union(b), df_list)\n\n# Remove any duplicates across sources\nfinal = combined.distinct()\n</code></pre>"},{"location":"user-guide/set-operations/#set-analysis","title":"Set Analysis","text":"<pre><code># Venn diagram analysis\nset_a = df.filter(\"category = 'A'\").select(\"user_id\")\nset_b = df.filter(\"category = 'B'\").select(\"user_id\")\n\n# Only in A\na_only = set_a.exceptAll(set_b)\n\n# Only in B\nb_only = set_b.exceptAll(set_a)\n\n# In both A and B\nboth = set_a.intersect(set_b).distinct()\n\n# In A or B (or both)\neither = set_a.union(set_b).distinct()\n</code></pre>"},{"location":"user-guide/set-operations/#data-quality-checks","title":"Data Quality Checks","text":"<pre><code># Find duplicate IDs (should be unique)\ndf.groupBy(\"id\").count().filter(\"count &gt; 1\")\n\n# Compare two datasets for differences\nexpected = spark.read.csv(\"expected.csv\", header=True)\nactual = spark.read.csv(\"actual.csv\", header=True)\n\n# Rows in expected but not in actual (missing data)\nmissing = expected.exceptAll(actual)\n\n# Rows in actual but not in expected (extra data)\nextra = actual.exceptAll(expected)\n\n# Rows in both (correct data)\ncorrect = expected.intersect(actual)\n</code></pre>"},{"location":"user-guide/set-operations/#examples","title":"Examples","text":"<p>See the complete examples in <code>examples/data_operations.py</code>:</p> <pre><code>python examples/data_operations.py\n</code></pre> <p>This example demonstrates: - Removing duplicates with distinct() and dropDuplicates() - Combining DataFrames with union() - Finding common rows with intersect() - Finding differences with exceptAll() - Chaining multiple operations - Practical use cases</p>"},{"location":"user-guide/set-operations/#method-summary","title":"Method Summary","text":"Method Description Removes Duplicates <code>distinct()</code> Remove duplicate rows (all columns) Yes <code>dropDuplicates(cols)</code> Remove duplicates by columns Yes <code>union(other)</code> Combine DataFrames vertically No <code>unionAll(other)</code> Alias for union() No <code>intersect(other)</code> Rows in both DataFrames Partial* <code>exceptAll(other)</code> Rows in first but not second Yes <code>subtract(other)</code> Alias for exceptAll() Yes <p>*Note: <code>intersect()</code> may keep some duplicates. Use <code>.distinct()</code> after for fully unique results.</p>"},{"location":"user-guide/set-operations/#api-aliases","title":"API Aliases","text":"<p>For compatibility, several methods have snake_case aliases:</p> <pre><code>df.dropDuplicates(['name'])  # camelCase\ndf.drop_duplicates(['name'])  # snake_case (alias)\n\ndf.unionAll(other)    # camelCase\n# (snake_case alias not provided as union() is already simple)\n\ndf.exceptAll(other)   # camelCase\ndf.subtract(other)    # alternative name\n</code></pre>"},{"location":"user-guide/sql-queries/","title":"SQL Queries","text":"<p>PyRust provides full SQL query support through DataFusion's SQL engine. You can execute standard SQL queries on your DataFrames using temporary views.</p>"},{"location":"user-guide/sql-queries/#quick-start","title":"Quick Start","text":"<pre><code>from pyrust import SparkSession\n\n# Create a SparkSession\nspark = SparkSession.builder.appName(\"SQLExample\").getOrCreate()\n\n# Load data\ndf = spark.read.csv(\"users.csv\", header=True)\n\n# Register as temporary view\ndf.createOrReplaceTempView(\"users\")\n\n# Execute SQL query\nresult = spark.sql(\"SELECT name, age FROM users WHERE age &gt; 25\")\nresult.show()\n</code></pre>"},{"location":"user-guide/sql-queries/#creating-temporary-views","title":"Creating Temporary Views","text":"<p>Before you can query a DataFrame with SQL, you need to register it as a temporary view:</p> <pre><code># Register DataFrame as a view\ndf.createOrReplaceTempView(\"users\")\n\n# Now you can reference it in SQL queries\nresult = spark.sql(\"SELECT * FROM users\")\n</code></pre>"},{"location":"user-guide/sql-queries/#replacing-views","title":"Replacing Views","text":"<p>The <code>createOrReplaceTempView()</code> method will replace any existing view with the same name:</p> <pre><code># Create initial view\ndf.createOrReplaceTempView(\"my_data\")\n\n# Later, replace it with a filtered version\nfiltered_df = df.filter(\"age &gt; 30\")\nfiltered_df.createOrReplaceTempView(\"my_data\")  # Replaces the old view\n</code></pre>"},{"location":"user-guide/sql-queries/#basic-sql-queries","title":"Basic SQL Queries","text":""},{"location":"user-guide/sql-queries/#select","title":"SELECT","text":"<p>Select specific columns:</p> <pre><code>result = spark.sql(\"SELECT name, age, city FROM users\")\n</code></pre> <p>Select all columns:</p> <pre><code>result = spark.sql(\"SELECT * FROM users\")\n</code></pre>"},{"location":"user-guide/sql-queries/#where-clause","title":"WHERE Clause","text":"<p>Filter rows based on conditions:</p> <pre><code>result = spark.sql(\"SELECT * FROM users WHERE age &gt; 25\")\nresult = spark.sql(\"SELECT * FROM users WHERE city = 'New York'\")\nresult = spark.sql(\"SELECT * FROM users WHERE age &gt; 25 AND salary &gt; 70000\")\n</code></pre>"},{"location":"user-guide/sql-queries/#order-by","title":"ORDER BY","text":"<p>Sort results:</p> <pre><code># Ascending order (default)\nresult = spark.sql(\"SELECT * FROM users ORDER BY age\")\n\n# Descending order\nresult = spark.sql(\"SELECT * FROM users ORDER BY salary DESC\")\n\n# Multiple columns\nresult = spark.sql(\"SELECT * FROM users ORDER BY city, age DESC\")\n</code></pre>"},{"location":"user-guide/sql-queries/#limit","title":"LIMIT","text":"<p>Limit the number of rows:</p> <pre><code>result = spark.sql(\"SELECT * FROM users LIMIT 10\")\n</code></pre>"},{"location":"user-guide/sql-queries/#aggregations","title":"Aggregations","text":""},{"location":"user-guide/sql-queries/#count","title":"COUNT","text":"<p>Count rows:</p> <pre><code>result = spark.sql(\"SELECT COUNT(*) as total FROM users\")\n</code></pre>"},{"location":"user-guide/sql-queries/#group-by","title":"GROUP BY","text":"<p>Group and aggregate:</p> <pre><code>result = spark.sql(\"\"\"\n    SELECT city, COUNT(*) as count\n    FROM users\n    GROUP BY city\n\"\"\")\n</code></pre>"},{"location":"user-guide/sql-queries/#multiple-aggregations","title":"Multiple Aggregations","text":"<p>Use multiple aggregate functions:</p> <pre><code>result = spark.sql(\"\"\"\n    SELECT\n        city,\n        COUNT(*) as total,\n        AVG(age) as avg_age,\n        MIN(salary) as min_salary,\n        MAX(salary) as max_salary,\n        SUM(salary) as total_salary\n    FROM users\n    GROUP BY city\n\"\"\")\n</code></pre>"},{"location":"user-guide/sql-queries/#having-clause","title":"HAVING Clause","text":"<p>Filter groups after aggregation:</p> <pre><code>result = spark.sql(\"\"\"\n    SELECT city, COUNT(*) as count\n    FROM users\n    GROUP BY city\n    HAVING COUNT(*) &gt; 5\n\"\"\")\n</code></pre>"},{"location":"user-guide/sql-queries/#joins","title":"Joins","text":"<p>You can perform joins using SQL syntax:</p>"},{"location":"user-guide/sql-queries/#inner-join","title":"INNER JOIN","text":"<pre><code># Register both DataFrames\nemployees.createOrReplaceTempView(\"employees\")\ndepartments.createOrReplaceTempView(\"departments\")\n\n# Join them\nresult = spark.sql(\"\"\"\n    SELECT e.name, e.age, d.dept_name\n    FROM employees e\n    INNER JOIN departments d ON e.dept_id = d.dept_id\n\"\"\")\n</code></pre>"},{"location":"user-guide/sql-queries/#left-join","title":"LEFT JOIN","text":"<pre><code>result = spark.sql(\"\"\"\n    SELECT e.name, d.dept_name\n    FROM employees e\n    LEFT JOIN departments d ON e.dept_id = d.dept_id\n\"\"\")\n</code></pre>"},{"location":"user-guide/sql-queries/#other-join-types","title":"Other Join Types","text":"<p>PyRust supports all standard SQL join types: - <code>INNER JOIN</code> - <code>LEFT JOIN</code> (or <code>LEFT OUTER JOIN</code>) - <code>RIGHT JOIN</code> (or <code>RIGHT OUTER JOIN</code>) - <code>FULL OUTER JOIN</code></p>"},{"location":"user-guide/sql-queries/#subqueries","title":"Subqueries","text":""},{"location":"user-guide/sql-queries/#subquery-in-where","title":"Subquery in WHERE","text":"<p>Filter based on a subquery result:</p> <pre><code>result = spark.sql(\"\"\"\n    SELECT name, age\n    FROM users\n    WHERE age &gt; (SELECT AVG(age) FROM users)\n\"\"\")\n</code></pre>"},{"location":"user-guide/sql-queries/#subquery-in-from","title":"Subquery in FROM","text":"<p>Use a subquery as a data source:</p> <pre><code>result = spark.sql(\"\"\"\n    SELECT city, avg_salary\n    FROM (\n        SELECT city, AVG(salary) as avg_salary\n        FROM users\n        GROUP BY city\n    ) AS city_stats\n    WHERE avg_salary &gt; 70000\n\"\"\")\n</code></pre>"},{"location":"user-guide/sql-queries/#sql-functions","title":"SQL Functions","text":""},{"location":"user-guide/sql-queries/#string-functions","title":"String Functions","text":"<pre><code>result = spark.sql(\"\"\"\n    SELECT\n        name,\n        UPPER(name) as upper_name,\n        LOWER(city) as lower_city\n    FROM users\n\"\"\")\n</code></pre> <p>Common string functions: - <code>UPPER(col)</code> - Convert to uppercase - <code>LOWER(col)</code> - Convert to lowercase - <code>LENGTH(col)</code> - Get string length - <code>TRIM(col)</code> - Remove whitespace</p>"},{"location":"user-guide/sql-queries/#math-functions","title":"Math Functions","text":"<pre><code>result = spark.sql(\"\"\"\n    SELECT\n        age,\n        age * 2 as double_age,\n        age + 10 as age_plus_10\n    FROM users\n\"\"\")\n</code></pre>"},{"location":"user-guide/sql-queries/#case-expressions","title":"CASE Expressions","text":"<p>Conditional logic in SQL:</p> <pre><code>result = spark.sql(\"\"\"\n    SELECT\n        name,\n        age,\n        CASE\n            WHEN age &lt; 25 THEN 'Young'\n            WHEN age &lt; 35 THEN 'Middle'\n            ELSE 'Senior'\n        END as age_category\n    FROM users\n\"\"\")\n</code></pre>"},{"location":"user-guide/sql-queries/#mixing-sql-and-dataframe-operations","title":"Mixing SQL and DataFrame Operations","text":"<p>You can seamlessly mix SQL queries with DataFrame operations:</p>"},{"location":"user-guide/sql-queries/#sql-dataframe-operations","title":"SQL \u2192 DataFrame Operations","text":"<pre><code># Start with SQL\nresult = spark.sql(\"SELECT * FROM users WHERE age &gt; 25\")\n\n# Continue with DataFrame operations\nresult = result.filter(\"salary &gt; 70000\") \\\n               .orderBy(\"salary\") \\\n               .limit(10)\n</code></pre>"},{"location":"user-guide/sql-queries/#dataframe-operations-sql","title":"DataFrame Operations \u2192 SQL","text":"<pre><code># Start with DataFrame operations\nfiltered = df.filter(\"age &gt; 25\")\n\n# Register and query with SQL\nfiltered.createOrReplaceTempView(\"filtered_users\")\nresult = spark.sql(\"SELECT city, COUNT(*) FROM filtered_users GROUP BY city\")\n</code></pre>"},{"location":"user-guide/sql-queries/#error-handling","title":"Error Handling","text":"<p>PyRust raises <code>RuntimeError</code> for SQL errors:</p> <pre><code>try:\n    result = spark.sql(\"SELECT * FROM nonexistent_table\")\nexcept RuntimeError as e:\n    print(f\"SQL error: {e}\")\n</code></pre> <p>Common errors: - Table not found: Make sure you've called <code>createOrReplaceTempView()</code> first - Column not found: Check column names in your query - Syntax error: Verify your SQL syntax is correct</p>"},{"location":"user-guide/sql-queries/#performance-tips","title":"Performance Tips","text":"<ol> <li> <p>Filter early: Use WHERE clauses to reduce data before joins    <pre><code># Good\nspark.sql(\"SELECT * FROM users WHERE age &gt; 25\")\n\n# Less efficient for large datasets\nspark.sql(\"SELECT * FROM users\").filter(\"age &gt; 25\")\n</code></pre></p> </li> <li> <p>Use column selection: Select only needed columns    <pre><code># Good\nspark.sql(\"SELECT name, age FROM users\")\n\n# Unnecessary if you only need two columns\nspark.sql(\"SELECT * FROM users\")\n</code></pre></p> </li> <li> <p>Aggregate before joining: Reduce data size before expensive operations    <pre><code>spark.sql(\"\"\"\n    SELECT u.city, s.avg_salary\n    FROM users u\n    JOIN (\n        SELECT city, AVG(salary) as avg_salary\n        FROM salaries\n        GROUP BY city\n    ) s ON u.city = s.city\n\"\"\")\n</code></pre></p> </li> </ol>"},{"location":"user-guide/sql-queries/#examples","title":"Examples","text":"<p>See the complete SQL examples in <code>examples/sql_queries.py</code>:</p> <pre><code>python examples/sql_queries.py\n</code></pre> <p>This example demonstrates: - Basic queries (SELECT, WHERE, ORDER BY) - Aggregations (GROUP BY, HAVING) - CASE expressions - Subqueries - String and math functions - Mixing SQL with DataFrame operations - Replacing temporary views</p>"},{"location":"user-guide/sql-queries/#supported-sql-features","title":"Supported SQL Features","text":"<p>PyRust leverages DataFusion's SQL engine, which supports:</p> <ul> <li>\u2705 SELECT, WHERE, ORDER BY, LIMIT</li> <li>\u2705 GROUP BY, HAVING</li> <li>\u2705 Aggregations: COUNT, SUM, AVG, MIN, MAX</li> <li>\u2705 Joins: INNER, LEFT, RIGHT, FULL OUTER</li> <li>\u2705 Subqueries in SELECT, FROM, and WHERE</li> <li>\u2705 CASE expressions</li> <li>\u2705 String functions: UPPER, LOWER, TRIM, LENGTH</li> <li>\u2705 Math operators: +, -, *, /</li> <li>\u2705 Comparison operators: =, !=, &lt;, &gt;, &lt;=, &gt;=</li> <li>\u2705 Logical operators: AND, OR, NOT</li> <li>\u2705 Common SQL functions</li> </ul> <p>For advanced SQL features, refer to DataFusion's SQL documentation.</p>"},{"location":"user-guide/transformations/","title":"Transformations Guide","text":"<p>Master PyRust's transformation operations for powerful data manipulation.</p>"},{"location":"user-guide/transformations/#understanding-transformations","title":"Understanding Transformations","text":""},{"location":"user-guide/transformations/#lazy-evaluation","title":"Lazy Evaluation","text":"<p>Transformations don't execute immediately - they build a query plan:</p> <pre><code># None of these execute yet\ndf1 = df.filter(\"age &gt; 18\")        # Lazy\ndf2 = df1.select([\"name\", \"city\"]) # Lazy\ndf3 = df2.orderBy([\"name\"])        # Lazy\n\n# This triggers execution\ndf3.show()  # Action! Executes entire chain\n</code></pre> <p>Benefits: - Query optimization - Efficient execution planning - Reduced memory usage</p>"},{"location":"user-guide/transformations/#transformations-vs-actions","title":"Transformations vs Actions","text":"Transformations (Lazy) Actions (Eager) <code>select()</code> <code>show()</code> <code>filter()</code> / <code>where_()</code> <code>count()</code> <code>orderBy()</code> / <code>sort()</code> <code>collect()</code> (future) <code>limit()</code> <code>groupBy()</code>"},{"location":"user-guide/transformations/#column-operations","title":"Column Operations","text":""},{"location":"user-guide/transformations/#selecting-columns","title":"Selecting Columns","text":"<pre><code># Select specific columns\ndf.select([\"name\", \"age\"])\n\n# Reorder columns\ndf.select([\"age\", \"name\", \"city\"])\n\n# Select all except (future feature)\n# df.select(df.columns.except([\"id\"]))\n</code></pre>"},{"location":"user-guide/transformations/#renaming-columns-future","title":"Renaming Columns (Future)","text":"<pre><code># Future: Column renaming\n# df.withColumnRenamed(\"old_name\", \"new_name\")\n</code></pre>"},{"location":"user-guide/transformations/#adding-columns-future","title":"Adding Columns (Future)","text":"<pre><code># Future: Computed columns\n# df.withColumn(\"age_plus_10\", col(\"age\") + 10)\n</code></pre>"},{"location":"user-guide/transformations/#row-operations","title":"Row Operations","text":""},{"location":"user-guide/transformations/#filtering","title":"Filtering","text":""},{"location":"user-guide/transformations/#simple-conditions","title":"Simple Conditions","text":"<pre><code># Numeric comparisons\ndf.filter(\"age &gt; 18\")\ndf.filter(\"salary &gt;= 50000\")\ndf.filter(\"score &lt; 100\")\n\n# String equality\ndf.filter(\"city == 'New York'\")\ndf.filter(\"status == 'active'\")\n\n# String inequality\ndf.filter(\"country != 'USA'\")\n</code></pre>"},{"location":"user-guide/transformations/#multiple-filters","title":"Multiple Filters","text":"<pre><code># Chain filters (implicit AND)\ndf.filter(\"age &gt; 18\") \\\n  .filter(\"age &lt; 65\") \\\n  .filter(\"salary &gt; 30000\")\n\n# Future: Combined conditions\n# df.filter(\"age &gt; 18 AND salary &gt; 30000\")\n# df.filter(\"city == 'NYC' OR city == 'LA'\")\n</code></pre>"},{"location":"user-guide/transformations/#sorting","title":"Sorting","text":"<pre><code># Sort ascending\ndf.orderBy([\"age\"])\n\n# Multiple columns (priority: left to right)\ndf.orderBy([\"country\", \"city\", \"name\"])\n\n# Future: Descending order\n# df.orderBy(col(\"age\").desc())\n</code></pre>"},{"location":"user-guide/transformations/#limiting","title":"Limiting","text":"<pre><code># First N rows\ndf.limit(10)\n\n# Top N (combine with sort)\ndf.orderBy([\"sales\"]).limit(5)\n</code></pre>"},{"location":"user-guide/transformations/#deduplication-future","title":"Deduplication (Future)","text":"<pre><code># Future: Remove duplicates\n# df.distinct()\n# df.dropDuplicates([\"email\"])\n</code></pre>"},{"location":"user-guide/transformations/#aggregations","title":"Aggregations","text":""},{"location":"user-guide/transformations/#simple-count","title":"Simple Count","text":"<pre><code># Count by group\ndf.groupBy([\"city\"]).count()\n\n# Multiple group columns\ndf.groupBy([\"country\", \"city\"]).count()\n</code></pre>"},{"location":"user-guide/transformations/#multiple-aggregations","title":"Multiple Aggregations","text":"<pre><code># Various aggregate functions\ndf.groupBy([\"department\"]).agg([\n    (\"salary\", \"sum\"),\n    (\"salary\", \"avg\"),\n    (\"salary\", \"min\"),\n    (\"salary\", \"max\"),\n    (\"employee_id\", \"count\")\n])\n</code></pre>"},{"location":"user-guide/transformations/#aggregation-functions","title":"Aggregation Functions","text":"Function Description Example Output Column <code>count</code> Count rows <code>count(column)</code> <code>sum</code> Sum values <code>sum(column)</code> <code>avg</code> Average <code>avg(column)</code> <code>mean</code> Average (alias) <code>avg(column)</code> <code>min</code> Minimum <code>min(column)</code> <code>max</code> Maximum <code>max(column)</code>"},{"location":"user-guide/transformations/#transformation-patterns","title":"Transformation Patterns","text":""},{"location":"user-guide/transformations/#pattern-1-filter-select-sort","title":"Pattern 1: Filter-Select-Sort","text":"<pre><code># Extract specific data\nresult = df.filter(\"status == 'active'\") \\\n           .select([\"name\", \"email\", \"signup_date\"]) \\\n           .orderBy([\"signup_date\"])\nresult.show()\n</code></pre>"},{"location":"user-guide/transformations/#pattern-2-aggregate-filter","title":"Pattern 2: Aggregate-Filter","text":"<pre><code># Group and filter groups\nsummary = df.groupBy([\"product\"]) \\\n            .agg([(\"quantity\", \"sum\")]) \\\n            .filter(\"sum(quantity) &gt; 1000\")\nsummary.show()\n</code></pre>"},{"location":"user-guide/transformations/#pattern-3-multiple-groupings","title":"Pattern 3: Multiple Groupings","text":"<pre><code># Hierarchical aggregation\nby_region = df.groupBy([\"region\"]).agg([(\"sales\", \"sum\")])\nby_city = df.groupBy([\"region\", \"city\"]).agg([(\"sales\", \"sum\")])\nby_store = df.groupBy([\"region\", \"city\", \"store\"]).agg([(\"sales\", \"sum\")])\n</code></pre>"},{"location":"user-guide/transformations/#pattern-4-top-n-per-group","title":"Pattern 4: Top N per Group","text":"<pre><code># Top 5 highest sales\ntop_sales = df.orderBy([\"amount\"]) \\\n              .limit(5)\n\n# Future: Top N per category\n# window = Window.partitionBy(\"category\").orderBy(col(\"sales\").desc())\n# df.withColumn(\"rank\", row_number().over(window)) \\\n#   .filter(\"rank &lt;= 5\")\n</code></pre>"},{"location":"user-guide/transformations/#optimization-strategies","title":"Optimization Strategies","text":""},{"location":"user-guide/transformations/#1-filter-pushdown","title":"1. Filter Pushdown","text":"<p>Push filters as early as possible:</p> <pre><code># Good - filter early\ndf.filter(\"year == 2024\") \\\n  .select([\"name\", \"amount\"]) \\\n  .groupBy([\"name\"]) \\\n  .agg([(\"amount\", \"sum\")])\n\n# Less efficient - filter late\ndf.groupBy([\"name\", \"year\"]) \\\n  .agg([(\"amount\", \"sum\")]) \\\n  .filter(\"year == 2024\")\n</code></pre>"},{"location":"user-guide/transformations/#2-column-pruning","title":"2. Column Pruning","text":"<p>Select only needed columns early:</p> <pre><code># Good - select early\ndf.select([\"id\", \"amount\", \"date\"]) \\\n  .filter(\"amount &gt; 1000\") \\\n  .groupBy([\"date\"]) \\\n  .agg([(\"amount\", \"sum\")])\n\n# Less efficient - all columns carried through\ndf.filter(\"amount &gt; 1000\") \\\n  .groupBy([\"date\"]) \\\n  .agg([(\"amount\", \"sum\")])\n</code></pre>"},{"location":"user-guide/transformations/#3-minimize-sorts","title":"3. Minimize Sorts","text":"<p>Sorting is expensive - only sort when necessary:</p> <pre><code># Good - sort only final result\ndf.filter(\"age &gt; 18\") \\\n  .groupBy([\"city\"]) \\\n  .count() \\\n  .orderBy([\"count\"])\n\n# Less efficient - unnecessary intermediate sort\ndf.orderBy([\"name\"]) \\\n  .filter(\"age &gt; 18\") \\\n  .groupBy([\"city\"]) \\\n  .count()\n</code></pre>"},{"location":"user-guide/transformations/#4-limit-early-for-exploration","title":"4. Limit Early for Exploration","text":"<pre><code># Quick exploration\ndf.limit(1000) \\\n  .filter(\"category == 'electronics'\") \\\n  .groupBy([\"brand\"]) \\\n  .count() \\\n  .show()\n</code></pre>"},{"location":"user-guide/transformations/#advanced-patterns","title":"Advanced Patterns","text":""},{"location":"user-guide/transformations/#conditional-aggregation-future","title":"Conditional Aggregation (Future)","text":"<pre><code># Future: Conditional sums\n# df.groupBy([\"product\"]).agg(\n#     sum(when(col(\"status\") == \"sold\", col(\"amount\"))).alias(\"sold_amount\"),\n#     sum(when(col(\"status\") == \"returned\", col(\"amount\"))).alias(\"returned_amount\")\n# )\n</code></pre>"},{"location":"user-guide/transformations/#window-functions-future","title":"Window Functions (Future)","text":"<pre><code># Future: Running totals, rankings\n# window = Window.orderBy(\"date\")\n# df.withColumn(\"running_total\", sum(\"amount\").over(window))\n</code></pre>"},{"location":"user-guide/transformations/#joins-future","title":"Joins (Future)","text":"<pre><code># Future: Join DataFrames\n# df1.join(df2, on=\"user_id\", how=\"inner\")\n# df1.join(df2, df1.id == df2.user_id, how=\"left\")\n</code></pre>"},{"location":"user-guide/transformations/#best-practices","title":"Best Practices","text":"<ol> <li>Filter Early, Often</li> <li>Apply filters before expensive operations</li> <li> <p>Reduces data volume throughout pipeline</p> </li> <li> <p>Select Only Needed Columns</p> </li> <li>Reduces memory usage and I/O</li> <li> <p>Especially important with wide tables</p> </li> <li> <p>Understand Lazy Evaluation</p> </li> <li>Transformations build a plan</li> <li>Actions trigger execution</li> <li> <p>Multiple actions re-execute the plan</p> </li> <li> <p>Use Appropriate Aggregations</p> </li> <li><code>count()</code> for counting</li> <li><code>sum()</code> for totals</li> <li><code>avg()</code> for means</li> <li> <p>Combine multiple in single <code>agg()</code> call</p> </li> <li> <p>Test on Subsets</p> </li> <li>Use <code>limit()</code> for quick validation</li> <li>Test transformations on small data first</li> </ol>"},{"location":"user-guide/transformations/#next-steps","title":"Next Steps","text":"<ul> <li>Performance Guide - Optimize transformations</li> <li>DataFrame API - Complete DataFrame guide</li> <li>API Reference - Method documentation</li> </ul>"}]}